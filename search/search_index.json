{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<ul> <li>Getting Started</li> </ul>"},{"location":"#user-documentation","title":"User Documentation","text":"<ul> <li>For the <code>daphne</code> command-line interface API: see <code>bin/daphne --help</code></li> <li>Running DAPHNE in a Local Environment</li> <li>Running DAPHNE on the Distributed Runtime</li> <li>DAPHNE Packaging, Distributed Deployment, and Management</li> <li>DaphneLib: DAPHNE's Python API</li> <li>DaphneLib API Reference</li> <li>Porting Numpy to DaphneLib</li> <li>DaphneDSL Language Reference</li> <li>DaphneDSL Built-in Functions</li> <li>Using SQL in DaphneDSL</li> <li>A Few Early Example Algorithms in DaphneDSL</li> <li>FileMetaData Format (reading and writing data)</li> <li>Profiling DAPHNE using PAPI</li> <li>Custom Extensions to DAPHNE</li> </ul>"},{"location":"#developer-documentation","title":"Developer Documentation","text":"<ul> <li>Creating release artifacts</li> <li>Maintaining GPG signing keys</li> </ul>"},{"location":"#how-tos-and-guidelines","title":"How-tos and Guidelines","text":"<ul> <li>Handling a Pull Request</li> <li>Implementing a Built-in Kernel</li> <li>Binary Data Format</li> <li>DAPHNE Configuration: Getting Information from the User</li> <li>Extending DAPHNE with more scheduling knobs</li> <li>Extending the DAPHNE Distributed Runtime</li> <li>Building DAPHNE with the build.sh script</li> <li>Logging in DAPHNE</li> <li>Profiling in DAPHNE</li> <li>Testing</li> </ul>"},{"location":"#source-code-documentation","title":"Source Code Documentation","text":"<p>The source code is partly documented in doxygen style. Until the generation of a proper documentation has been set up, you can simply have a look at the documentation in the individual source files.</p>"},{"location":"BinaryFormat/","title":"Binary Data Format","text":"<p>DAPHNE defines its own binary representation for the serialization of in-memory data objects (matrices/frames). This representation is intended to be used by default whenever we need to transfer or persistently store these in-memory objects, e.g., for</p> <ul> <li>the data transfer in the distributed runtime</li> <li>a custom binary file format</li> <li>the eviction of in-memory data to secondary storage</li> </ul> <p>Disclaimer: The current specification is a first draft and will likely be refined as we proceed. At the moment, we focus on the case of a single block per data object.</p> <p>Endianess: For now, we assume little endian.</p> <p>Images: In the images below, all addresses and sizes are specified in bytes (<code>[B]</code>).</p>"},{"location":"BinaryFormat/#binary-representation-of-a-whole-data-object","title":"Binary Representation of a Whole Data Object","text":"<p>The binary representation of a data object (matrix/frame) starts with a header containing general and data type-specific information. The data object is partitioned into rectangular blocks (in the extreme case, this can mean a single block). All blocks are represented individually (see binary representation of a single block below) and stored along with their position in the data object.</p> <pre><code>+--------+------+\n| header | body |\n+--------+------+\n</code></pre>"},{"location":"BinaryFormat/#header","title":"Header","text":"<p>The header consists of the following information:</p> <ul> <li>DAPHNE binary format version number (<code>1</code> for now) (uint8)</li> <li>data type <code>dt</code> (uint8)</li> <li>number of rows <code>#r</code> (uint64)</li> <li>number of columns <code>#c</code> (uint64)</li> </ul> <p>We currently support the following data types:</p> code data type <code>0</code> (reserved) <code>1</code> <code>DenseMatrix</code> <code>2</code> <code>CSRMatrix</code> <code>3</code> <code>Frame</code> <p>We currently support the following value types:</p> code C++ value type 0 (reserved) 1 <code>uint8_t</code> 2 <code>uint16_t</code> 3 <code>uint32_t</code> 4 <code>uint64_t</code> 5 <code>int8_t</code> 6 <code>int16_t</code> 7 <code>int32_t</code> 8 <code>int64_t</code> 9 <code>float</code> 10 <code>double</code> <p>Depending on the data type, there are more information in the header:</p> <p>For <code>DenseMatrix</code> and <code>CSRMatrix</code>:</p> <ul> <li>value type <code>vt</code> (uint8)</li> </ul> <pre><code>addr[B]  0 0 1  1 2  9 10 17 18 18\n        +---+----+----+-----+-----+\n        | 1 | dt | #r |  #c |  vt |\n        +---+----+----+-----+-----+\nsize[B]   1    1    8    8     1\n</code></pre> <p>For <code>Frame</code>:</p> <ul> <li>value type <code>vt</code> (uint8), for each column</li> <li>length of the label <code>len</code> (uint16) and label <code>lbl</code> (character string), for each column</li> </ul> <pre><code>addr[B]  0 0 1  1 2  9 10 17 18               18+#c-1 18+#c                                         *\n        +---+----+----+-----+-------+     +----------+--------+--------+     +-----------+-----------+\n        | 1 | dt | #r |  #c | vt[0] | ... | vt[#c-1] | len[0] | lbl[0] | ... | len[#c-1] | lbl[#c-1] |\n        +---+----+----+-----+-------+     +----------+--------+--------+     +-----------+-----------+\nsize[B]   1    1    8    8      1               1         2     len[0]             2       len[#c-1]\n</code></pre>"},{"location":"BinaryFormat/#body","title":"Body","text":"<p>The body consists of a sequence of:</p> <ul> <li>a pair of</li> <li>row index <code>rx</code> (uint64)</li> <li>column index <code>cx</code> (uint64)</li> <li>a binary block representation</li> </ul> <p>For the special case of a single block, this looks as follows:</p> <pre><code>addr[B]  0 7 8 15 16       *\n        +---+----+----------+\n        | 0 |  0 | block[0] |\n        +---+----+----------+\n          8    8       *\nsize[B] \n</code></pre>"},{"location":"BinaryFormat/#binary-representation-of-a-single-block","title":"Binary Representation of a Single Block","text":"<p>A single data block is a rectangular partition of a data object. In the extreme case, a single block can span the entire data object in both dimensions (one block per data object).</p> <p>General block header</p> <ul> <li>number of rows <code>#r</code> (uint32)</li> <li>number of columns <code>#c</code> (uint32)</li> <li>block type <code>bt</code> (uint8)</li> <li>block type-specific information (see below)</li> </ul> <pre><code>addr[B]  0  3 4  7 8  8 9                        *\n        +----+----+----+--------------------------+\n        | #r | #c | bt | block type-specific info |\n        +----+----+----+--------------------------+\nsize[B]    4    4    1               *\n</code></pre>"},{"location":"BinaryFormat/#block-types","title":"Block types","text":"<p>We define different block types to allow for a space-efficient representation depending on the data. When serializing a data object, the block types are not required to match the in-memory representation (e.g., the blocks of a <code>DenseMatrix</code> could use the sparse binary representation). Moreover, different blocks may be represented as different block types (e.g., some blocks might use the dense binary representation and others the sparse one). We currently support the following block types:</p> code block type <code>0</code> empty <code>1</code> dense <code>2</code> sparse (CSR) <code>3</code> ultra-sparse (COO) <p>Most block types store their value type as part of the block type-specific information. Note that the value type used for the binary representation is not required to match the value type of the in-memory object (e.g., <code>DenseMatrix&lt;uint64_t&gt;</code> may be represented as a dense block with value type <code>uint8_t</code>, if the value range permits). Furthermore, each block may be represented using its individual value type.</p>"},{"location":"BinaryFormat/#empty-block","title":"Empty block","text":"<p>This block type is used to represent blocks that contain only zeros of the respective value type very space-efficiently.</p> <p>Block type-specific information: none</p> <pre><code>addr[B]  0  3 4  7 8 8\n        +----+----+---+\n        | #r | #c | 0 |\n        +----+----+---+\nsize[B]    4    4   1\n</code></pre>"},{"location":"BinaryFormat/#dense-block","title":"Dense block","text":"<p>Block type-specific information:</p> <ul> <li>value type <code>vt</code> (uint8)</li> <li>values <code>v</code> in row-major (value type <code>vt</code>)</li> </ul> <p>Below, <code>S</code> denotes the size (in bytes) of a single value of type <code>vt</code>.</p> <pre><code>addr[B]  0  3 4  7 8 8 9  9 10                             10+#r*#c*S\n        +----+----+---+----+---------+---------+     +---------------+\n        | #r | #c | 1 | vt | v[0, 0] | v[0, 1] | ... | v[#r-1, #c-1] |\n        +----+----+---+----+---------+---------+     +---------------+\nsize[B]    4    4   1    1      S         S                  S\n</code></pre>"},{"location":"BinaryFormat/#sparse-block-compressed-sparse-row-csr","title":"Sparse block (compressed sparse row, CSR)","text":"<p>Block type-specific information:</p> <ul> <li>value type <code>vt</code>  (uint8)</li> <li>number of non-zeros in the block <code>#nzb</code> (uint64)</li> <li>for each row</li> <li>number of non-zeros in the row <code>#nzr</code> (uint32)</li> <li>for each non-zero in the row<ul> <li>column index <code>cx</code> (uint32)</li> <li>value <code>v</code> (value type <code>vt</code>)</li> </ul> </li> </ul> <p>Note that both a row and the entire block might contain no non-zeros.</p> <p>Below, <code>S</code> denotes the size (in bytes) of a single value of type <code>vt</code>.</p> <pre><code>                                                                 18 + 4*#r +       \naddr[B]  0  3 4  7 8 8 9  9 10  17 18                             #nzb*(4+S)\n        +----+----+---+----+------+--------+     +--------+     +-----------+\n        | #r | #c | 2 | vt | #nzb | row[0] | ... | row[i] | ... | row[#r-1] |\n        +----+----+---+----+------+--------+     +--------+     +-----------+\nsize[B]    4    4   1    1     8              4+#nzr[i]*(4+S)\n\n                ______________________________________|_______________________\n               /                                                              \\\n\n               +---------+----------+     +----------+     +------------------+\n               | #nzr[i] | nz[i, 0] | ... | nz[i, j] | ... | nz[i, #nzr[i]-1] |\n               +---------+----------+     +----------+     +------------------+\n                    4         4+S              4+S                  4+S \n\n                                  ______________|____________\n                                 /                           \\\n\n                                 +----------+----------------+\n                                 | cx[i, j] | v[i, cx[i, j]] |\n                                 +----------+----------------+\n                                       4             S\n</code></pre>"},{"location":"BinaryFormat/#ultra-sparse-block-coordinate-coo","title":"Ultra-sparse block (coordinate, COO)","text":"<p>Ultra-sparse blocks contain almost no non-zeros, so we want to keep the overhead of the meta data low. Thus, we distinguish blocks with a single column (where we don't need to store the column index) and blocks with more than one column.</p>"},{"location":"BinaryFormat/#blocks-with-a-single-column","title":"Blocks with a single column","text":"<p>Block type-specific information:</p> <ul> <li>value type <code>vt</code> (uint8)</li> <li>number of non-zeros in the block <code>#nzb</code> (uint32)</li> <li>for each non-zero</li> <li>row index <code>rx</code> (uint32)</li> <li>value <code>v</code> (value type <code>vt</code>)</li> </ul> <p>Below, <code>S</code> denotes the size (in bytes) of a single value of type <code>vt</code>.</p> <pre><code>addr[B]  0  3 4  7 8 8 9  9 10  13 14                         14+#nzb*(4+S)\n        +----+----+---+----+------+-------+     +-------+     +------------+\n        | #r | #c | 3 | vt | #nzb | nz[0] | ... | nz[i] | ... | nz[#nzb-1] |\n        +----+----+---+----+------+-------+     +-------+     +------------+\nsize[B]    4    4   1    1     4     4+S           4+S              4+S\n\n                                          __________|__________\n                                         /                     \\\n\n                                         +-------+-------------+\n                                         | rx[i] | v[rx[i], 0] |\n                                         +-------+-------------+\n                                             4          S\n</code></pre>"},{"location":"BinaryFormat/#blocks-with-more-than-one-column","title":"Blocks with more than one column","text":"<p>Block type-specific information:</p> <ul> <li>value type <code>vt</code> (uint8)</li> <li>number of non-zeros in the block <code>#nzb</code> (uint32)</li> <li>for each non-zero</li> <li>row index <code>rx</code> (uint32)</li> <li>column index <code>cx</code> (uint32)</li> <li>value <code>v</code> (value type <code>vt</code>)</li> </ul> <p>Below, <code>S</code> denotes the size (in bytes) of a single value of type <code>vt</code>.</p> <pre><code>addr[B]  0  3 4  7 8 8 9  9 10  13 14                         14+#nzb*(8+S)\n        +----+----+---+----+------+-------+     +-------+     +------------+\n        | #r | #c | 3 | vt | #nzb | nz[0] | ... | nz[i] | ... | nz[#nzb-1] |\n        +----+----+---+----+------+-------+     +-------+     +------------+\nsize[B]    4    4   1    1     4     8+S           8+S              8+S\n\n                                    ________________|________________\n                                   /                                 \\\n\n                                   +-------+-------+-----------------+\n                                   | rx[i] | cx[i] | v[rx[i], cx[i]] |\n                                   +-------+-------+-----------------+\n                                       4       4            S\n</code></pre>"},{"location":"BuildEnvironment/","title":"Building in unsupported environments","text":"<p>DAPHNE can also be built in environments other than the one endorsed by the DAPHNE project team. To help with that, there are scripts to build your own toolchain in the <code>buildenv</code> directory. These scripts can be used to build the compiler and necessary libraries to build DAPHNE in unsupported environments  (RHEL UBI, CentOS, ...) Besides the build scripts, there is a docker file to create a UBI image to build for Redhat 8.</p> <p>Usage: * Create Docker image with build-ubi8.sh * Run the Docker image with run-ubi8.sh * Run build-all.sh * Run source env.sh inside or outside the Docker image to set PATH and linker variables to use the created environment  (you need to cd into the directory containing env.sh as this uses $PWD)</p> <p>Beware that this procedure needs ~50GB of free disk space. Also, the provided CUDA SDK expects a recent driver (550+)  version. That will most likely be an issue on large installations - exchange the relevant version and file names for  another CUDA version in env.sh to build another version.</p>"},{"location":"Codegen/","title":"Code Generation with MLIR","text":"<p>This document describes the process of directly generating code with the MLIR framework.</p>"},{"location":"Codegen/#motivation","title":"Motivation","text":"<p>DAPHNE provides a kernel for (almost) every DaphneIR operation which reside in <code>src/runtime/local/kernels/</code>. These are precompiled as a shared library and linked during compile-time. Even though these kernels can be highly optimized and thus achieve great runtime characteristics, they may not provide a desired level of extensibility for custom value types. They may also be lacking information only available at compile-time that could enable further optimizations. Additionally, through the process of progressively lowering the input IR, the code generation pipeline may enable more optimization possibilities such as operator or loop fusion.</p> <p>As an alternative way to implement our operators we provide the code generation pipeline which progressively lowers the DaphneIR available after parsing the DaphneDSL script to operations in either the same dialect or operations from other dialects. With that, we can optionally replace certain kernels by generating code directly, and also perform a hybrid compilation approach where we mix kernel calls with code generation in order to exploit advantages of both, precompiled kernel libraries and code generation. Code generation passes are found in <code>src/compiler/lowering/</code>.</p>"},{"location":"Codegen/#guidelines","title":"Guidelines","text":"<p>Currently, the code generation pipeline is enabled with the CLI flag <code>--mlir-codegen</code>. This adds the following passes that perform transformations and lowerings:</p> <ul> <li>DenseMatrixOptPass</li> <li>MatMulOpLoweringPass</li> <li>AggAllLoweringPass</li> <li>MapOpLoweringPass</li> <li>InlinerPass</li> <li>LowerEwOpPass</li> <li>ConvertMathToLLVMPass</li> <li>ModOpLoweringPass</li> <li>Canonicalizer</li> <li>CSE</li> <li>LoopFusion</li> <li>AffineScalarReplacement</li> <li>LowerAffinePass</li> </ul> <p>These passes are added in the <code>DaphneIrExecutor::buildCodegenPipeline</code> function. The <code>--mlir-hybrid-codegen</code> flag disables the <code>MatMulOpLoweringPass</code> since the kernel implementation vastly outperforms the generated code of this pass.</p>"},{"location":"Codegen/#runtime-interoperability","title":"Runtime Interoperability","text":"<p>Runtime interoperability with the <code>DenseMatrix</code> object is achieved with two kernels in <code>src/runtime/local/kernels/ConvertDenseMatrixToMemRef.h</code> and <code>src/runtime/local/kernels/ConvertMemRefToDenseMatrix.h</code> and the corresponding DaphneOps <code>Daphne_ConvertMemRefToDenseMatrix</code> and <code>Daphne_ConvertDenseMatrixToMemRef</code>. These kernels define how a MemRef is passed to a kernel and how a kernel can return a <code>StridedMemRefType</code>.</p>"},{"location":"Codegen/#debugging","title":"Debugging","text":"<p>In order to enable our debug <code>PrintIRPass</code> pass, one has to add <code>--explain mlir_code_gen</code> when running <code>daphne</code>. Additionally, it is recommended to use the <code>daphne-opt</code> tool to test passes in isolation. One just has to provide the input IR for a pass to <code>daphne-opt</code> and the correct flag to run the pass (or multiple passes) on the IR. <code>daphne-opt</code> provides all the functionality of the <code>mlir-opt</code> tool.</p> <p><code>daphne-opt --lower-ew --debug-only=dialect-conversion ew.mlir</code> performs the <code>LowerEwOpPass</code> on the input file <code>ew.mlir</code> while providing dialect conversion debug information.</p>"},{"location":"Codegen/#testing","title":"Testing","text":"<p>To test the generated code, there currently are two different approaches.</p> <p>End-to-end tests can be found under <code>test/api/cli/codegen/</code> and are part of the existing Catch2 test-suite with the its own tag, <code>TAG_CODEGEN</code>.</p> <p>Additionally, there are tests that check the generated IR by running the <code>llvm-lit</code>, <code>daphne-opt</code>, and <code>FileCheck</code> utilities. These tests reside under <code>test/compiler/lowering/</code>. They are <code>.mlir</code> files containing the input IR of a certain pass, or pass pipeline, and the <code>llvm-lit</code> directive at the top of the file (<code>RUN:</code>). In that line we specify how <code>llvm-lit</code> executes the test, e.g., <code>// RUN: daphne-opt --lower-ew %s | FileCheck %s</code>, means that <code>daphne-opt</code> is called with the <code>--lower-ew</code> flag and the current file as input, the output of that, in addition to the file itself, is piped to <code>FileCheck</code>. <code>FileCheck</code> uses the comments in the <code>.mlir</code> file to check for certain conditions, e.g., <code>// CHECK-NOT: daphne.ewAdd</code> looks through the IR and fails if <code>daphne.ewAdd</code> can be found. These <code>llvm-lit</code> tests are all run by the <code>codegen</code> testcase in <code>test/codegen/Codegen.cpp</code>.</p> <p>All codegen tests can be executed by running <code>bin/run_tests '[codegen]'</code>.</p>"},{"location":"Config/","title":"Configuration - Getting Information from the User","text":"<p>The behavior of the DAPHNE system can be influenced by the user by means of a cascading configuration mechanism. There is a set of options that can be passed from the user to the system. These options are collected in the <code>DaphneUserConfig</code> (src/api/cli/DaphneUserConfig.h). The cascade consists of the following steps:</p> <ul> <li>The defaults of all options are hard-coded directly in the <code>DaphneUserConfig</code>.</li> <li>At program start-up, a configuration file is loaded, which overrides the defaults (WIP, #141).</li> <li>After that, command-line arguments further override the configuration (src/api/cli/daphne.cpp).</li> <li>(In the future, DaphneDSL will also offer means to change the configuration at run-time.)</li> </ul> <p>The <code>DaphneUserConfig</code> is available to all parts of the code, including:</p> <ul> <li>The DAPHNE compiler: The <code>DaphneUserConfig</code> is passed to the <code>DaphneIrExecutor</code> and from there to all passes that need it.</li> <li>The DAPHNE runtime: The <code>DaphneUserConfig</code> is part of the <code>DaphneContext</code>, which is passed to all kernels.</li> </ul> <p>Hence, information provided by the user can be used to influence both, the compiler and the runtime. The use of environment variables to pass information into the system is discouraged.</p>"},{"location":"Config/#how-to-extend-the-configuration","title":"How to extend the configuration?","text":"<p>If you need to add additional information from the user, you must take roughly the following steps:</p> <ol> <li>Create a new member in <code>DaphneUserConfig</code> and hard-code a reasonable default.</li> <li>Add a command-line argument to the system's CLI API in src/api/cli/daphne.cpp. We use LLVM's CommandLine 2.0 library for parsing CLI arguments. Make sure to update the corresponding member the <code>DaphneUserConfig</code> with the parsed argument.</li> <li>For compiler passes: If necessary, pass on the <code>DaphneUserConfig</code> to the compiler pass you are working on in src/compiler/execution/DaphneIrExecutor.cpp. For kernels: All kernels automatically get the <code>DaphneUserConfig</code> via the <code>DaphneContext</code> (their last parameter), so no action is required from your side.</li> <li>Access the new member of the <code>DaphneUserConfig</code> in your code.</li> </ol>"},{"location":"Deploy/","title":"Deploying","text":"<p>DAPHNE Packaging, Distributed Deployment, and Management</p>"},{"location":"Deploy/#overview","title":"Overview","text":"<p>This file explains the deployment of the Daphne system, on HPC with SLURM or manually through SSH, and highlights the excerpts from descriptions of functionalities in deploy/ directory (mostly deploy-distributed-on-slurm.sh):</p> <ul> <li>compilation of the Singularity image,</li> <li>compilation of Daphne (and the Daphne DistributedWorker) within the Singularity image,</li> <li>packaging compiled Daphne,</li> <li>packaging compiled Daphne with user payload as a payload package,</li> <li>uploading the payload package to an HPC platform,</li> <li>starting and managing DAPHNE workers on HPC platforms using SLURM,</li> <li>executing DAPHNE on HPC using SLURM,</li> <li>collection of logs from daphne execution, and</li> <li>cleanup of worker environments and payload deployment.</li> </ul>"},{"location":"Deploy/#background","title":"Background","text":"<p>Daphne's distributed system consists of a single coordinator and multiple DistributedWorkers (you can read more about Distributed DAPHNE here).  For now, in order to execute Daphne in a distributed fashion, we need to deploy DistributedWorkers manually. Coordinator gets the worker's addresses through an environmental variable.</p> <p><code>deployDistributed.sh</code> manually connects to machines with SSH and starts up DistributedWorker processes. On the other hand the deploy-distributed-on-slurm.sh packages and starts Daphne on a target HPC platform, and is tailored to the communication required with Slurm and the target HPC platform.</p>"},{"location":"Deploy/#deploying-without-slurm-support","title":"Deploying without Slurm support","text":"<p><code>deployDistributed.sh</code> can be used to manually connect to a list of machines and remotely start up workers, get status of running workers or terminate distributed worker processes. This script depends only on an SSH client/server and does not require any use of a resource management tool (e.g. SLURM). With this script you can:</p> <ul> <li>build and deploy DistributedWorkers to remote machines</li> <li>start workers</li> <li>check status of running workers</li> <li>kill workers</li> </ul> <p>Workers' own IPs and ports to listen to, can be specified inside the script, or with <code>--peers [IP[:PORT]],[IP[:PORT]],...</code>. Default port for all workers is 50000, but this can also be specified inside the script or with <code>-p,--port PORT</code>. If running on same machine (e.g. localhost), different ports must be specified.</p> <p>With <code>--deploy</code> the script builds <code>DistributedWorker</code> executable (<code>./build.sh --target DistributedWorker</code>), compresses <code>build</code>, <code>lib</code> and <code>bin</code> folders and uses <code>scp</code> and <code>ssh</code> to send and decompress at remote machines, inside the directory specified by <code>--pathToBuild</code> (default <code>~/DaphneDistributedWorker/</code>). If running workers on localhost, <code>PATH_TO_BUILD</code> can be set <code>/path/to/daphne</code> and provided <code>DistributedWorker</code> is built, <code>--deploy</code> is not nessecary.</p> <p>Ssh username must be specified inside the script. For now the script assumes all remote machines can be accessed with the same <code>username</code>, <code>id_rsa</code> key and ssh port (default 22).</p> <p>Usage example:</p> <pre><code># deploy distributed\n$ ./deployDistributed.sh --help\n$ ./deployDistributed.sh --deploy --pathToBuild /path/to/dir --peers localhost:5000,localhost:5001\n$ ./deployDistributed.sh -r # (Uses default peers and path/to/build/ to start workers)\n</code></pre>"},{"location":"Deploy/#deploying-with-slurm-support","title":"Deploying with Slurm support","text":"<p>Building the Daphne system (to be later deployed on distributed nodes) can be done with a Singularity container. The Singularity container can be built on the utilized HPC. deployDistributed.sh sends executables on each node, assuming there are different storages for each node. This might cause unnecessary overwrites if the workers use same mounted user storage (e.g. HPC environments with distributed storages). Instead deploy-distributed-on-slurm.sh should be used for such cases. The latter also automatically generates the environmental variable <code>PEERS</code> from Slurm.</p>"},{"location":"Deploy/#how-to-use-deploy-distributed-on-slurmsh-for-daphne-packaging-distributed-deployment-and-management-using-slurm","title":"How to use <code>deploy-distributed-on-slurm.sh</code> for DAPHNE Packaging, Distributed Deployment, and Management using Slurm","text":"<p>This explains how to set up the Distributed Workers on a HPC platform, and it also briefly comments on what to do afterwards (how to run, manage, stop, and clean it). Commands, with their parameters and arguments, are hence described below for deployment with deploy-distributed-on-slurm.sh.</p> <pre><code>Usage: deploy-distributed-on-slurm.sh &lt;options&gt; &lt;command&gt;\n\nStart the DAPHNE distributed deployment on remote machines using Slurm.\n\nThese are the options (short and long formats available):\n  -h, --help              Print this help message and exit.\n  -i SSH_IDENTITY_FILE    Specify OpenSSH identity file (default: private key in ~/.ssh/id_rsa.pub).\n  -u, --user SSH_USERNAME Specify OpenSSH username (default: $USER).\n  -l, --login SSH_LOGIN_NODE_HOSTNAME     Specify OpenSSH login name hostname (default: localhost).\n  -d, --pathToBuild       A path to deploy or where the build is already deployed (default ~/DaphneDistributedWorker can be specified in the script).\n  -n, --numcores          Specify number of workers (cores) to use to deploy DAPHNE workers (default: 128).\n  -p, --port              Specify DAPHNE deployed port range begin (default: 50000).\n  --args ARGS_CS          Specify arguments of a DaphneDSL SCRIPT in a comma-separated format.\n  -S, --ssh-arg=S         Specify additional arguments S for ssh client (default command: $SSH_COMMAND).\n  -C, --scp-arg=C         Specify additional arguments C for scp client (default command: $SCP_COMMAND).\n  -R, --srun-arg=R        Specify additional arguments R for srun client.\n  -G, --singularity-arg=G Specify additional arguments G for singularity client.\n\nThese are the commands that can be executed:\n  singularity             Compile the Singularity SIF image for DAPHNE (and transfer it to the target platform).\n  build                   Compile DAPHNE codes (daphne, DistributedWorker) using the Singularity image for DAPHNE.\n                          It should only be invoked from the code base root directory.\n                          It could also be invoked on a target platform after a transfer.\n  package                 Create the package image with *.daphne scripts and a compressed build/ directory.\n  transfer                Transfers (uploads) a package to the target platform.\n  start                   Run workers on remote machines through login node (deploys this script and runs workers).\n  workers                 Run workers on current login node through Slurm.\n  status                  Get distributed workers' status.\n  wait                    Waits until all workers are up.\n  stop                    Stops all distributed workers.\n  run [SCRIPT [ARGs]]     Run one request on the deployed platform by processing one DaphneDSL SCRIPT file (default: /dev/stdin)\n                          using optional arguments (ARGs in script format).\n  clean                   Cleans (deletes) the package on the target platform.\n  deploy                  Deploys everything in one sweep: singularity=&gt;build=&gt;package=&gt;transfer=&gt;start=&gt;wait=&gt;run=&gt;clean.\n\n\nThe default connection to the target platform (HPC) login node is through OpenSSH, configured by default in ~/.ssh (see: man ssh_config).\n\nThe default ports for worker peers begin at 50000 (PORTRANGE_BEGIN) and the list of PEERS is generated as:\nPEERS = ( WORKER1_IP:PORTRANGE_BEGIN, WORKER1_IP:PORTRANGE_BEGIN+1, ..., WORKER2_IP:PORTRANGE_BEGIN, WORKER2_IP:PORTRANGE_BEGIN+1, ... )\n\nLogs can be found at [pathToBuild]/logs.\n</code></pre>"},{"location":"Deploy/#short-examples","title":"Short Examples","text":"<p>The following list presents few examples about how to use the deploy-distributed-on-slurm.sh command.</p> <p>These comprise more hands-on documentation about deployment, including tutorial-like explanation examples about how to package, distributively deploy, manage, and execute workloads using DAPHNE.</p> <ol> <li> <p>Builds the Singularity image and uses it to compile the build directory codes, then packages it.</p> <pre><code>./deploy-distributed-on-slurm.sh singularity &amp;&amp; ./deploy-distributed-on-slurm.sh build &amp;&amp; ./deploy-distributed-on-slurm.sh package\n</code></pre> </li> <li> <p>Transfers a package to the target platform through OpenSSH, using login node HPC, user hpc, and identify key hpc.pub.</p> <pre><code>./deploy-distributed-on-slurm.sh --login HPC --user hpc -i ~/.ssh/hpc.pub transfer\n</code></pre> </li> <li> <p>Using login node HPC, accesses the target platform and starts workers on remote machines.</p> <pre><code>./deploy-distributed-on-slurm.sh -l HPC start\n</code></pre> </li> <li> <p>Runs one request (script called example-time.daphne) on the deployment using 1024 cores, login node HPC, and default OpenSSH configuration.</p> <pre><code>./deploy-distributed-on-slurm.sh -l HPC -n 1024 run example-time.daphne\n</code></pre> </li> <li> <p>Executes one request (DaphneDSL script input from standard input) at a running deployed platform, using default singularity/srun configurations.</p> <pre><code>./deploy-distributed-on-slurm.sh run\n</code></pre> </li> <li> <p>Deploys once at the target platform through OpenSSH using default login node (localhost), then cleans.</p> <pre><code>./deploy-distributed-on-slurm.sh deploy -n 10\n</code></pre> </li> <li> <p>Starts workers at a running deployed platform using custom srun arguments (2 hours dual-core with 10G memory).</p> <pre><code>./deploy-distributed-on-slurm.sh workers -R=\"-t 120 --mem-per-cpu=10G --cpu-bind=cores --cpus-per-task=2\"\n</code></pre> </li> <li> <p>Executes a request with custom srun arguments (30 minutes single-core).</p> <pre><code>./deploy-distributed-on-slurm.sh run -R=\"--time=30 --cpu-bind=cores --nodes=1 --ntasks-per-node=1 --cpus-per-task=1\"\n</code></pre> </li> <li> <p>Example request job from a pipe.</p> <pre><code>cat ../scripts/examples/hello-world.daph | ./deploy-distributed-on-slurm.sh run\n</code></pre> </li> </ol>"},{"location":"Deploy/#scenario-usage-example","title":"Scenario Usage Example","text":"<p>Here is a scenario usage as a longer example demo.</p> <ol> <li> <p>Fetch the code from the latest GitHub code repository.</p> <pre><code>function compile() {\n  git clone --recursive git@github.com:daphne-eu/daphne.git 2&gt;&amp;1 | tee daphne-$(date +%F-%T).log\n  cd daphne/deploy\n  ./deploy-distributed-on-slurm.sh singularity # creates the Singularity container image\n  ./deploy-distributed-on-slurm.sh build   # Builds the daphne codes using the container\n}\ncompile\n</code></pre> </li> <li> <p>Package the built targets (binaries) to packet file <code>daphne-package.tgz</code>.</p> <pre><code>./deploy-distributed-on-slurm.sh package\n</code></pre> </li> <li> <p>Transfer the packet file <code>daphne-package.tgz</code> to <code>HPC</code> (Slurm) with OpenSSH key <code>~/.ssh/hpc.pub</code> and unpack it.</p> <pre><code>./deploy-distributed-on-slurm.sh --login HPC --user $USER -i ~/.ssh/hpc.pub transfer \n</code></pre> <p>E.g., for EuroHPC Vega, use the instance, if your username matches the one at Vega and the key is <code>~/.ssh/hpc.pub</code>:</p> <pre><code>./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub transfer\n</code></pre> </li> <li> <p>Start the workers from the local computer by logging into the HPC login node:</p> <pre><code>./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub start\n</code></pre> </li> <li> <p>Starting a main target on the HPC (Slurm) and connecting it with the started workers, to execute payload from the stream.</p> <pre><code>cat ../scripts/examples/hello-world.daph | ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub run \n</code></pre> </li> <li> <p>Starting a main target on the HPC (Slurm) and connecting it with the started workers, to execute payload from a file.</p> <pre><code>./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub run example-time.daphne\n</code></pre> </li> <li> <p>Stopping all workers on the HPC (Slurm).</p> <pre><code>./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub stop\n</code></pre> </li> <li> <p>Cleaning the uploaded targets from the HPC login node.</p> <pre><code>./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub clean\n</code></pre> </li> </ol>"},{"location":"DistributedRuntime/","title":"Distributed Runtime","text":"<p>Running DAPHNE on the Distributed Runtime</p>"},{"location":"DistributedRuntime/#background","title":"Background","text":"<p>Daphne supports execution in a distributed fashion. Utilizing the Daphne Distributed Runtime does not require any changes to the DaphneDSL script. Similar to the local vectorized engine (here, section 4), the compiler automatically fuses operations and creates pipelines for the distributed runtime, which then uses multiple distributed nodes (workers) that work on their local data, while a main node, the coordinator, is responsible for transferring the data and code to be executed. As mentioned above, changes at DaphneDSL code are not needed, however the user is required to start the workers, either manually or using an HPC tool as SLURM (scripts that start the workers locally or remotely, natively or not, can be found here).</p>"},{"location":"DistributedRuntime/#scope","title":"Scope","text":"<p>This document focuses on:</p> <ul> <li>how to start distributed workers</li> <li>executing Daphne scripts on the distributed runtime</li> <li>DAPHNE's distributed runtime has two different backends. This page explains how things work with the gRPC backend. A brief introduction to the other backend using OpenMPI can be viewed in this document.</li> </ul>"},{"location":"DistributedRuntime/#build-the-daphne-prototype","title":"Build the Daphne prototype","text":"<p>First you need to build the Daphne prototype. This doc assumes that you already built Daphne and can run it locally. If you need help building or running Daphne see here.</p>"},{"location":"DistributedRuntime/#building-the-distributed-worker","title":"Building the Distributed Worker","text":"<p>The Daphne distributed worker is a different executable which can be build using the build-script and providing the <code>--target</code> argument:</p> <pre><code>./build.sh --target DistributedWorker\n</code></pre>"},{"location":"DistributedRuntime/#start-distributed-workers","title":"Start distributed workers","text":"<p>Before executing Daphne on the distributed runtime, worker nodes must first be up and running. You can start a distributed worker within the Daphne prototype directory as follows:</p> <pre><code># IP:PORT is the IP and PORT the worker server will be listening too\n./bin/DistributedWorker IP:PORT \n</code></pre> <p>There are scripts that automate this task and can help running multiple workers at once locally or even utilizing tools (like SLURM) in HPC environments.</p> <p>Each worker can be left running and reused for multiple scripts and pipeline executions (however, for now they might run into memory issues, see Limitations section below).</p> <p>Each worker can be terminated by sending a <code>SIGINT</code> (Ctrl+C) or by using the scripts mentioned above.</p>"},{"location":"DistributedRuntime/#set-up-environmental-variables","title":"Set up environmental variables","text":"<p>After setting up the workers, before we run Daphne we need to specify which IPs and ports the workers are listening too. For now we use an environmental variable called <code>DISTRIBUTED_WORKERS</code> where we list IPs and ports of the workers separated by a comma.</p> <pre><code># Example for 2 workers.\n# Worker1 listens to localhost:5000\n# Worker2 listens to localhost:5001\nexport DISTRIBUTED_WORKERS=localhost:5000,localhost:5001\n</code></pre>"},{"location":"DistributedRuntime/#run-daphne-using-the-distributed-runtime","title":"Run DAPHNE using the distributed runtime","text":"<p>Now that we have all workers up and running and the environmental variable is set we can run Daphne. We enable the distributed runtime by specifying the flag argument <code>--distributed</code>.</p> <p>(*) Note that we execute Daphne from the same bash shell we've set up the environmental variable  <code>DISTRIBUTED_WORKERS</code>.</p> <pre><code>./bin/daphne --distributed ./example.script\n</code></pre> <p>For now only asynchronous-gRPC is implemented as a distributed backend and selection is hardcoded here.</p>"},{"location":"DistributedRuntime/#example","title":"Example","text":"<p>On one terminal with start up a Distributed Worker:</p> <pre><code>$./bin/DistributedWorker localhost:5000\nStarted Distributed Worker on `localhost:5000`\n</code></pre> <p>On another terminal we set the environment variable and execute script <code>distributed.daph</code>:</p> <pre><code>$ export DISTRIBUTED_WORKERS=localhost:5000\n$ ./bin/daphne --distributed ./scripts/example/distributed.daph\n</code></pre>"},{"location":"DistributedRuntime/#current-limitations","title":"Current limitations","text":"<p>Distributed Runtime is still under development and currently there are various limitations. Most of these limitations will be fixed in the future.</p> <ul> <li>Distributed runtime for now heavily depends on the vectorized engine of Daphne and how pipelines are created and multiple operations are fused together (more here - section 4). This causes some limitations related to pipeline creation (e.g. not supporting pipelines with different result outputs or pipelines with no outputs).</li> <li>For now distributed runtime only supports <code>DenseMatrix</code> types and value types <code>double</code> - <code>DenseMatrix&lt;double&gt;</code> (issue #194).</li> <li>A Daphne pipeline input might exist multiple times in the input array. For now this is not supported. In the future similar pipelines will simply omit multiple pipeline inputs and each one will be provided only once.</li> <li>Garbage collection at worker (node) level is not implemented yet. This means that after some time the workers can fill up their memory completely, requiring a restart.</li> </ul>"},{"location":"DistributedRuntime/#what-next","title":"What Next?","text":"<p>You might want to have a look at</p> <ul> <li>the distributed runtime development guideline</li> <li>the contribution guidelines</li> <li>the open distributed related issues</li> </ul>"},{"location":"Extensions/","title":"Custom Extensions to DAPHNE","text":"<p>DAPHNE will be extensible in various respects. Users will be able to add their own kernels, data types, value types, compiler passes, runtime schedulers, etc. without changing the DAPHNE source code itself.</p> <p>So far, DAPHNE has initial support for adding custom kernels.</p>"},{"location":"Extensions/#custom-kernel-extensions","title":"Custom Kernel Extensions","text":"<p>Users can add their own custom kernels (physical operators) to DAPHNE following a three-step approach:</p> <ol> <li>The extension is implemented as a stand-alone code base.</li> <li>The extension is compiled as a shared library.</li> <li>The extension is used in a DaphneDSL script or via DAPHNE's Python API.</li> </ol> <p>Since this feature is still in an early stage, we only mention the most important points here rather than providing a full reference of what's supported.</p> <p>Furthermore, we include a running example of adding two custom kernels for the summation of a dense matrix of single-precision floating-point values.  We are interested in two variants: one sequential implementation for the CPU and one implementation that uses SIMD instructions from Intel's AVX (256-bit vector registers) on the CPU. All files shown below can be found in <code>/scripts/examples/extensions/myKernels/</code>.</p>"},{"location":"Extensions/#step-1-implementing-a-kernel-extension","title":"Step 1: Implementing a Kernel Extension","text":"<p>A kernel extension consists at least of the following:</p> <ul> <li>A C++ source file, which includes some essential DAPHNE headers and defines one or multiple kernel functions.   The kernel functions have to follow a certain interface (*) and have <code>extern \"C\"</code> linkage.   Within the kernel functions, extension developers have a lot of freedom.   Nevertheless, we also plan to provide some best practices and helpers to make extension development more productive.</li> <li>A kernel catalog JSON file, which provides some essential information on the kernels provided in the extension, such that DAPHNE knows how to use them.   This information includes: the mnemonic of the DaphneIR operation (*), the name of the kernel function, the list of result/argument types, the backend (e.g. CPU or a specific hardware accelerator), and the path to the shared library of the extension (relative to this JSON file).</li> <li>To build the extension, it is recommendable (but not required) to include a Makefile or similar as well.</li> </ul> <p>(*) We will add a concrete list of DaphneIR operations for which custom kernels can be added later. This list will be understandable by DAPHNE users, and will contain the operations' mnemonics, arguments, results, as well as expected C++ kernel function interfaces. In the meantime, developers familiar with DAPHNE internals can already find references of the DaphneIR operations in <code>src/ir/daphneir/DaphneOps.td</code> and a reference of the kernel interfaces in <code>build/runtime/local/kernels/kernels.cpp</code> (generated during the DAPHNE build).</p> <p>Running example:</p> <p>C++ source file <code>myKernels.cpp</code>: <pre><code>#include &lt;runtime/local/datastructures/DenseMatrix.h&gt;\n\n#include &lt;immintrin.h&gt; // for the SIMD-enabled kernel\n#include &lt;iostream&gt;\n#include &lt;stdexcept&gt;\n\nclass DaphneContext;\n\nextern \"C\" {\n    // Custom sequential sum-kernel.\n    void mySumSeq(\n        float * res,\n        const DenseMatrix&lt;float&gt; * arg,\n        DaphneContext * ctx\n    ) {\n        std::cerr &lt;&lt; \"hello from mySumSeq()\" &lt;&lt; std::endl;\n        const float * valuesArg = arg-&gt;getValues();\n        *res = 0;\n        for(size_t r = 0; r &lt; arg-&gt;getNumRows(); r++) {\n            for(size_t c = 0; c &lt; arg-&gt;getNumCols(); c++)\n                *res += valuesArg[c];\n            valuesArg += arg-&gt;getRowSkip();\n        }\n    }\n\n    // Custom SIMD-enabled sum-kernel.\n    void mySumSIMD(\n        float * res,\n        const DenseMatrix&lt;float&gt; * arg,\n        DaphneContext * ctx\n    ) {\n        std::cerr &lt;&lt; \"hello from mySumSIMD()\" &lt;&lt; std::endl;\n\n        // Validation.\n        const size_t numCells = arg-&gt;getNumRows() * arg-&gt;getNumCols();\n        if(numCells % 8)\n            throw std::runtime_error(\n                \"for simplicity, the number of cells must be \"\n                \"a multiple of 8\"\n            );\n        if(arg-&gt;getNumCols() != arg-&gt;getRowSkip())\n            throw std::runtime_error(\n                \"for simplicity, the argument must not be \"\n                \"a column segment of another matrix\"\n            );\n\n        // SIMD accumulation (8x f32).\n        const float * valuesArg = arg-&gt;getValues();\n        __m256 acc = _mm256_setzero_ps();\n        for(size_t i = 0; i &lt; numCells / 8; i++) {\n            acc = _mm256_add_ps(acc, _mm256_loadu_ps(valuesArg));\n            valuesArg += 8;\n        }\n\n        // Summation of accumulator elements.\n        *res =\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[0] +\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[1] +\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[2] +\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[3] +\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[4] +\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[5] +\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[6] +\n            (reinterpret_cast&lt;float*&gt;(&amp;acc))[7];\n    }\n}\n</code></pre></p> <p>Kernel catalog file <code>myKernels.json</code>: <pre><code>[\n  {\n    \"opMnemonic\": \"sumAll\",\n    \"kernelFuncName\": \"mySumSeq\",\n    \"resTypes\": [\"float\"],\n    \"argTypes\": [\"DenseMatrix&lt;float&gt;\"],\n    \"backend\": \"CPP\",\n    \"libPath\": \"libMyKernels.so\"\n  },\n  {\n    \"opMnemonic\": \"sumAll\",\n    \"kernelFuncName\": \"mySumSIMD\",\n    \"resTypes\": [\"float\"],\n    \"argTypes\": [\"DenseMatrix&lt;float&gt;\"],\n    \"backend\": \"CPP\",\n    \"libPath\": \"libMyKernels.so\"\n  }\n]\n</code></pre></p> <p><code>Makefile</code>: <pre><code>libMyKernels.so: myKernels.o\n    g++ -shared myKernels.o -o libMyKernels.so\n\nmyKernels.o: myKernels.cpp\n    g++ -c -fPIC myKernels.cpp -I../../../../src/ -std=c++17 -O3 -mavx2 -o myKernels.o\n\nclean:\n    rm -rf myKernels.o libMyKernels.so\n</code></pre></p>"},{"location":"Extensions/#step-2-building-a-kernel-extension","title":"Step 2: Building a Kernel Extension","text":"<p>The kernel extension must be built as a shared library. Additional details will follow.</p> <p>Running example:</p> <p>Given the Makefile above, the extension is built by simply running <code>make</code> in the extension's directory, which produces the shared library <code>libMyKernels.so</code>:</p> <pre><code>make\n</code></pre>"},{"location":"Extensions/#step-3-using-a-kernel-extension","title":"Step 3: Using a Kernel Extension","text":"<p>The kernels in a kernel extension can be used either automatically by DAPHNE or manually by the user. Automatic use is currently restricted to the selection of the kernel based on result/argument data/value types, but in the future we plan to support custom cost models as well. Besides that, the manual employment of custom kernels is very useful for experimentation, e.g., to see the impact of the kernel in the context of a larger integrated data analysis pipeline. To this end, DaphneDSL compiler hints tell DAPHNE to use a specific kernel, even though DAPHNE's optimizing compiler may not choose the kernel, otherwise.</p> <p>Running example:</p> <p>A minimal example using a summation on a matrix of single-precision floating-point values could look as follows:</p> <p><code>demo.daphne</code>: <pre><code># Create a matrix of random f32 values in [0, 1] (400 MiB).\nX = rand(10^4, 10^4, as.f32(0), as.f32(1), 1, 12345);\n# Calculate the sum over the matrix.\ns = sum(X);\n# Print the sum.\nprint(s);\n</code></pre></p> <p>We execute this script from the DAPHNE root directory by: <pre><code>bin/daphne scripts/examples/extensions/myKernels/demo.daphne\n</code></pre></p> <p>In order to manually use our custom sequential <code>sum</code>-kernel, we add the DAPHNE compiler hint <code>::mySumSeq</code> to the script:</p> <p><code>demoSeq.daphne</code>: <pre><code>X = rand(10^4, 10^4, as.f32(0), as.f32(1), 1, 12345);\ns = sum::mySumSeq(X);\nprint(s);\n</code></pre></p> <p>We execute this script with the following command, whereby the argument <code>--kernel-ext</code> specified the kernel catalog JSON file of the extension to use: <pre><code>bin/daphne --kernel-ext scripts/examples/extensions/myKernels/myKernels.json scripts/examples/extensions/myKernels/demoSeq.daphne\n</code></pre></p> <p>Alternatively, we can try our custom SIMD-enabled <code>sum</code>-kernel by adapting the compiler hint accordingly:</p> <p><code>demoSIMD.daphne</code>: <pre><code>X = rand(10^4, 10^4, as.f32(0), as.f32(1), 1, 12345);\ns = sum::mySumSIMD(X);\nprint(s);\n</code></pre></p> <p>We execute this script by: <pre><code>bin/daphne --kernel-ext scripts/examples/extensions/myKernels/myKernels.json scripts/examples/extensions/myKernels/demoSIMD.daphne\n</code></pre></p>"},{"location":"FPGAconfiguration/","title":"FPGA Configuration","text":"<p>FPGA configuration for usage in DAPHNE</p>"},{"location":"FPGAconfiguration/#system-requirments","title":"System requirments","text":"<p>Daphne build script for FPGA kernels support requires additional QUARTUSDIR system variable definition. Example command is presented in fpga-build-env.sh or in the following command:</p> <p><code>export QUARTUSDIR=/opt/intel/intelFPGA_pro/21.4</code></p> <p>To build the Daphne with the FPGA support <code>-fpgaopencl</code> flag has to be used:</p> <p><code>./build.sh --fpgaopenc</code></p> <p>To run developed or precompiled, included in Daphne repository FPGA OpenCL kernels an installedand configured  FPGA device is required. Our example kernels have been tested using Intel(R) PAC D5005 card</p> <p>DAPHNE contains some example linear algebra kernels developed using T2SP framework. Example precompiled FPGA kernels can be usedon  DAPHNE DSL description level. To prepare the system for the precompiled FPGA kernels some FPGA and OpenCL system variables are required. The easiest way to set up required varables is to use the init_opencl.sh script from installed Intel(R) Quartus sowtware or from the Intel(R) OpenCL RTE or Intel(R) OpenCL SDK packages.</p> <p>Example script usage: <code>source /opt/intel/intelFPGA_pro/21.4/hld/init_opencl.sh</code></p> <p>For additional details please look into the Intel guide or SDK for openCL.</p>"},{"location":"FPGAconfiguration/#precompiled-fpga-kernels","title":"Precompiled FPGA Kernels","text":"<p>To use a precompiled FPGA kernel a FPGA image is required (*.aocx). FPGA device has to programmed with particular image which contains required kernel implementation. Example FPGA programming command using example FPGA image:</p> <p><code>aocl program acl0 src/runtime/local/kernels/FPGAOPENCL/bitstreams/sgemm.aocx</code></p> <p>Additionally the BITSTREAM variable has to be defind in the system. Please look into the following example:</p> <p><code>export BITSTREAM=src/runtime/local/kernels/FPGAOPENCL/bitstreams/sgemm.aocx</code></p> <p>When another FPGA image contains implementation for another required computational kernel then FPGA device has to be reprogrammed and BITSTREAM variable value has to be changed.</p>"},{"location":"FileMetaDataFormat/","title":"Read and Write Data","text":"<p>Reading and writing (meta) data in Daphne.</p> <p>When loading data with <code>read()</code> in a DaphneDSL script, the system expects a file with the same file name in the same directory as the data file with an additional extension <code>.meta</code>. This file contains a description of meta data stored in JSON format.</p> <p>There are two slightly varying ways of specifying meta data depending on whether there is a schema for the columns (e.g., a data frame - the corresponding C++ type is the Frame class) or not (this data can currently (as of version 0.1) be loaded as <code>DenseMatrix&lt;VT&gt;</code> or <code>CSRMatrix&lt;VT&gt;</code> where <code>VT</code> is the value type template parameter).</p> <p>If data is written from a DaphneDSL script via <code>write()</code>, the meta data file will be written to the corresponding <code>filename.meta</code>.</p>"},{"location":"FileMetaDataFormat/#currently-supported-json-fields","title":"Currently supported JSON fields","text":"Name Expected Data Allowed values numRows Integer # number of rows numCols Integer # number of columns valueType String <code>si8, si32, si64, // signed integers (intX_t)</code><code>ui8, ui32, ui64, // unsigned integers (uintx_t)</code><code>f32, f64, // floating point (float, double)</code>Contained within schema this may be an empty string. In this case all columns of a data frame will have the same valueType defined outside of the schema data field numNonZeros Integer # number of non-zeros (optional) schema JSON nested elements of \"label\" and \"valueType\" fields label String column name/header (optional, may be empty string \"\")"},{"location":"FileMetaDataFormat/#matrix-example","title":"Matrix Example","text":"<p>The example below describes a 2 by 4 dense matrix of double precision values.</p>"},{"location":"FileMetaDataFormat/#matrix-csv","title":"Matrix CSV","text":"<pre><code>-0.1,-0.2,0.1,0.2\n3.14,5.41,6.22216,5\n</code></pre>"},{"location":"FileMetaDataFormat/#matrix-metadata","title":"Matrix Metadata","text":"<pre><code>{\n    \"numRows\": 2,\n    \"numCols\": 4,\n    \"valueType\": \"f64\",\n    \"numNonZeros\": 0\n}\n</code></pre>"},{"location":"FileMetaDataFormat/#data-frame-example","title":"Data Frame Example","text":"<p>The example below describes a 2 by 2 Frame with signed integers in the first columns named foo and double precision  values in the second column named bar.</p>"},{"location":"FileMetaDataFormat/#data-frame-csv","title":"Data Frame CSV","text":"<pre><code>1,0.5\n2,1.0\n</code></pre>"},{"location":"FileMetaDataFormat/#data-frame-metadata","title":"Data Frame Metadata","text":"<pre><code>{\n  \"numRows\": 2,\n  \"numCols\": 2,\n  \"schema\": [\n    {\n      \"label\": \"foo\",\n      \"valueType\": \"si64\"\n    },\n    {\n      \"label\": \"bar\",\n      \"valueType\": \"f64\"\n    }\n  ]\n}\n</code></pre>"},{"location":"FileMetaDataFormat/#data-frame-meta-data-with-default-valuetype","title":"Data Frame Meta Data with Default ValueType","text":"<pre><code>{\n    \"numRows\": 5,\n    \"numCols\": 3,\n    \"valueType\": \"f32\",\n    \"schema\": [\n        {\n            \"label\": \"a\",\n            \"valueType\": \"\"\n        },\n        {\n            \"label\": \"bc\",\n            \"valueType\": \"\"\n        },\n        {\n            \"label\": \"def\",\n            \"valueType\": \"\"\n        }\n    ]    \n}\n</code></pre>"},{"location":"FileMetaDataFormat/#data-frame-meta-data-with-empty-labels","title":"Data Frame Meta Data with Empty Labels","text":"<pre><code> {\n     \"numRows\": 5,\n     \"numCols\": 3,\n     \"schema\": [\n         {\n             \"label\": \"\",\n             \"valueType\": \"si64\"\n         },\n         {\n             \"label\": \"\",\n             \"valueType\": \"f64\"\n         },\n         {\n             \"label\": \"\",\n             \"valueType\": \"ui32\"\n         }\n     ]\n }\n</code></pre>"},{"location":"GPG-signing-keys/","title":"GPG signing keys","text":""},{"location":"GPG-signing-keys/#providing-information-about-gpg-signing-keys","title":"Providing information about GPG signing keys","text":"<p>In general see the GitHub documentation about generating and maintaining your GPG keys for signing commits and releases. There are also tons of how-tos and tutorials out there about GPG and its best practices (which might change over time).</p> <p>Here are a few pointers how to deal with your GPG keys in the context of DAPHNE. Keys need to be: 1. entered in your GitHub profile (see GitHub documentation). 2. kept up to date (remove and add again in GitHub after prolonging the expiry date). 3. kept up to date in the <code>KEYS.txt</code> file in the DAPHNE repo root directory. 4. updated on third party key servers you might have used.</p> <p>Keyserver recommendation: As of February 2024 we recommend keyserver.ubuntu.com, keys.openpgp.org and keys.mailvelope.com. The latter two are recommended as they perform email verification. </p> <ul> <li>To export your key information to <code>KEYS.txt</code> use these two commands (replace <code>&lt;your-email&gt;</code> with the email address associated with your GPG key:</li> </ul> <pre><code>gpg --keyid-format=0xshort --list-key --with-fingerprint &lt;your-email&gt; | tee -a KEYS.txt\ngpg --armor --export &lt;your-email&gt; | tee -a KEYS.txt\n</code></pre> <ul> <li>If you are updating your key information in <code>KEYS.txt</code>, open that file with a text editor to remove the old key after appending the new one.</li> </ul>"},{"location":"GettingStarted/","title":"Getting Started","text":"<p>This document provides quick-start instructions for DAPHNE users and DAPHNE developers, which should work in most cases. Furthermore, it contains background information to help with custom setups.</p> <ul> <li>Quickstart for Users</li> <li>Quickstart for Developers</li> <li>Additional Details for Custom Setups</li> </ul>"},{"location":"GettingStarted/#quickstart-for-users","title":"Quickstart for Users","text":"<p>Follow these instructions if you want to use DAPHNE to define and run your own integrated data analysis pipelines. These simple steps should suffice to get started for most users. If required, you can find more details for custom setups later in this document.</p>"},{"location":"GettingStarted/#1-get-daphne","title":"1. Get DAPHNE","text":"<p>Option 1: Use a binary release</p> <p>Download and extract the file <code>daphne-&lt;flavor&gt;-&lt;version&gt;-bin.tgz</code> from the release page. It is recommended to use the latest version and to choose the flavor based on your platform and needs (e.g., <code>X86-64</code>, <code>cuda-X86-64</code>, or <code>ARMV8</code>). Using DAPHNE with CUDA requires Nvidia Pascal hardware or newer.</p> <p>Option 2: Build from sources</p> <p>To build from the latest sources with the most up-to-date state of DAPHNE, please follow the Quickstart for Developers below. If you want to use a source release, also follow those instructions, but instead of cloning the DAPHNE repository, download the source code of the latest release (file <code>&lt;version&gt;.zip</code>) from the release page. After the build, you can simply use the system without making changes to the source code.</p>"},{"location":"GettingStarted/#2-run-daphne","title":"2. Run DAPHNE","text":"<p>Ubuntu. The DAPHNE release binaries should run natively on recent Ubuntu systems (20.04 or later) and perhaps on other GNU/Linux distributions. However, if there are any issues running DAPHNE natively, please try the pre-built DAPHNE container image.</p> <p>Windows. The container image can also be used in Windows and WSL. Installing WSL and Docker should be straightforward using the documentation provided by Microsoft. On an installed WSL container, launching DAPHNE via Docker should work the same way as in a native installation.</p> <p>The following commands should be executed in a bash (or a compatible shell) from the extracted binary release directory (e.g., <code>daphne-X86-64-v&lt;version&gt;-bin</code>).</p>"},{"location":"GettingStarted/#option-1-run-daphne-in-the-container","title":"Option 1: Run DAPHNE in the Container","text":"<p>Download the container image <pre><code>docker pull daphneeu/daphne:latest_X86-64_BASE\n</code></pre> Hint: In case of Docker permission errors, consider prepending <code>sudo</code> to the command.</p> <p>Hint: You may want to choose another image tag based on your platform and needs, e.g., <code>latest_X86-64_CUDA</code> (for GPU support) or <code>latest_ARMV8_BASE</code> (for ARM support).</p> <p>Run a DaphneDSL hello-world script in the container, which should print the following (besides some numbers): <pre><code>containers/quickstart.sh scripts/examples/hello-world.daph\n</code></pre> <pre><code>Hello World!\n</code></pre></p>"},{"location":"GettingStarted/#option-2-run-daphne-natively","title":"Option 2: Run DAPHNE Natively","text":"<p>Run a DaphneDSL hello-world script natively, which should print the following (besides some numbers):</p> <p><pre><code>./run-daphne.sh scripts/examples/hello-world.daph\n</code></pre> <pre><code>Hello World!\n</code></pre></p> <p>Hint: Run with <code>--cuda</code> to activate CUDA ops, see also <code>./run-daphne.sh --help</code>.</p> <p>Run a DaphneLib example natively, which should print something like the following (the concrete numbers are random):</p> <p><pre><code>./run-python.sh scripts/examples/daphnelib/shift-and-scale.py\n</code></pre> <pre><code>[[ 0.60676254 -0.88097088 -0.33785961]\n [-0.48030447 -1.04911355  0.97716037]\n [ 1.44938037  1.31980374 -0.2036432 ]\n [-0.060317   -0.46788873 -1.59526103]\n [-1.51552144  1.07816942  1.15960347]]\n</code></pre></p> <p>Hint: DaphneLib requires numpy and pandas, consider installing them in a virtual environment.</p>"},{"location":"GettingStarted/#next-steps","title":"Next Steps","text":"<ul> <li>Browse the user documentation, especially the parts on DaphneDSL and DaphneLib.</li> <li>Have a look at some examples.</li> <li>Start using DAPHNE for your own integrated data analysis pipelines.</li> </ul>"},{"location":"GettingStarted/#quickstart-for-developers","title":"Quickstart for Developers","text":"<p>Follow these instructions if you want to make modifications to DAPHNE in terms of the source code, tests, examples, tooling, or documentation. These simple steps should suffice to get started for most developers. If required, you can find more details for custom setups later in this document.</p>"},{"location":"GettingStarted/#1-clone-the-daphne-source-code-repository","title":"1. Clone the DAPHNE Source Code Repository","text":"<p>Clone the source code of the main DAPHNE repository (command below) or your own fork (adapt the command below as necessary).</p> <pre><code>git clone https://github.com/daphne-eu/daphne.git\n</code></pre>"},{"location":"GettingStarted/#2-download-the-daphne-development-container-image","title":"2. Download the DAPHNE Development Container Image","text":"<p>The development container image already contains all necessary dependencies of a DAPHNE development environment as well as a useful initialization of environment variables etc., such that you don't need to worry about these things and can have a productive start.</p> <p>Get the container image</p> <p><pre><code>docker pull daphneeu/daphne-dev:latest_X86-64_BASE\n</code></pre> Hint: In case of Docker permission errors, try prepending <code>sudo</code> to the command.</p> <p>Hint: You may want to choose another image tag based on your platform and needs, e.g., <code>latest_X86-64_CUDA</code> (for GPU support) or <code>latest_ARMV8_BASE</code> (for ARM support).</p> <p>Enter the container, which should finally print something like the following (where <code>xyz</code> is your user name on your system, and the password and IP address may vary):</p> <p><pre><code>cd daphne\n./containers/run-docker-example.sh\n</code></pre> <pre><code>Use xyz with password Docker!0147 for SSH login\nDocker Container IP address(es):\n172.17.0.2\nxyz@daphne-container:/daphne$\n</code></pre></p> <p>Hint: In case of Docker permission errors, consider setting <code>USE_SUDO=sudo</code> in <code>run-docker-example.sh</code> near line 26.</p> <p>Hint: If you pulled a container image of another tag above (e.g., <code>latest_X86-64_CUDA</code>), set <code>DOCKER_TAG=</code> accordingly in <code>run-docker-example.sh</code> near line 22.</p> <p>Hint: You can leave the container at any time by executing <code>exit</code> or by typing <code>[Ctrl]</code>+<code>[D]</code>.</p>"},{"location":"GettingStarted/#3-build-daphne-and-run-the-test-suite","title":"3. Build DAPHNE and Run the Test Suite","text":"<p>Everything related to building and running DAPHNE should be done inside the DAPHNE development container, as described above.</p> <p>Build DAPHNE, which should finally print something like the following: <pre><code>./build.sh --no-deps --target all\n</code></pre> <pre><code>[DAPHNE]..Successfully built Daphne://all\n</code></pre></p> <p>Run the test suite, which should finally print something like the following (the concrete numbers of assertions and test cases may have changed since the time of this writing): <pre><code>./test.sh -nb -d yes\n</code></pre> <pre><code>===============================================================================\nAll tests passed (307532 assertions in 1186 test cases)\n</code></pre></p> <p>Run a hello-world script, which should print the following: <pre><code>bin/daphne scripts/examples/hello-world.daph\n</code></pre> <pre><code>Hello World!\n</code></pre></p>"},{"location":"GettingStarted/#next-steps_1","title":"Next Steps","text":"<ul> <li>Browse the user and developer documentation.</li> <li>Get familiar with the contribution guidelines.</li> <li>Start working in DAPHNE (for inspiration, see the list of open issues, including \"good first issues\")</li> </ul>"},{"location":"GettingStarted/#additional-details-for-custom-setups","title":"Additional Details for Custom Setups","text":"<p>The Quickstart for Users and Quickstart for Developers above should be enough for most users and developers to get started. Nevertheless, there are cases when more details are required, e.g., when DAPHNE shall be run natively, without one of the pre-built containers.</p>"},{"location":"GettingStarted/#system-requirements","title":"System Requirements","text":"<p>Please ensure that your development system meets the following requirements before trying to build DAPHNE. Note that the DAPHNE container images already ship with all these dependencies installed. There is a convenience script to install all necessary dependencies to compile DAPHNE on Ubuntu distributions. However, in order to use optional features, such as CUDA or FPGA support, the respective requirements listed below still need to be installed separately.</p> <p>(*) You can view the version numbers as an orientation rather than a strict requirement. Newer versions should work as well, older versions might work as well.</p>"},{"location":"GettingStarted/#operating-system","title":"Operating system","text":"OS distribution/version known to work (*) Comment GNU/Linux Manjaro Last checked in April 2024 GNU/Linux Ubuntu 20.04 - 24.04 All versions in that range work. 20.04 needs CMake installed from Snap. GNU/Linux Ubuntu 18.04 Used with Intel PAC D5005 FPGA, custom toolchain needed GNU/Linux RHEL 8 Custom toolchain needed MS Windows 10 Build 19041, 11 Works on Ubuntu WSL2, using the provided Docker images is recommended"},{"location":"GettingStarted/#windows","title":"Windows","text":"<p>Installing WSL and Docker should be straightforward using the documentation provided by Microsoft. On an installed WSL container, launching DAPHNE via Docker (see below) should work the same way as in a native installation.</p>"},{"location":"GettingStarted/#software","title":"Software","text":"tool/lib version known to work (*) comment clang 10.0.0 cmake 3.20 On Ubuntu 20.04, install by <code>sudo snap install cmake --classic</code> to fulfill the version requirement; <code>apt</code> provides only version 3.16.3. GCC/G++ 9.3.0 Last checked version: 13.2 (compiles with warnings) gfortran 9.3.0 git 2.25.1 java (e.g. openjdk) 11 (1.7 should be fine) jq json commandline processor used in docker image generation scripts. libpfm4-dev 4.10 This dependency is needed for profiling support [DAPHNE-#479] gRPC 1.38.0 libssl-dev 1.1.1 Dependency introduced while optimizing grpc build (which used to build ssl unnecessarily) lld 10.0.0 llvm-10-tools 10, 15, 18 <code>apt</code> provides up to <code>llvm-10-tools</code> for Ubuntu 20.04 whereas 22.04 / 24.04 require a newer version such as <code>llvm-15-tools</code>. ninja 1.10.0 numpy 1.19.5 pandas 0.25.3 pkg-config 0.29.1 python3 3.8.5 unzip 6.0 uuid-dev wget Used to fetch additional dependencies and other artefacts. zlib1g-dev Needed to satisfy cmake dependencies. *** *** *** CUDA SDK 11.7.1 Optional for CUDA ops Intel FPGA SDK or OneAPI FPGA Add-On 2022.x Optional for FPGAOPENCL ops OneAPI SDK 2022.x Optional for OneAPI ops tensorflow 2.13.1 Optional for data exchange between DaphneLib and TensorFlow torch 2.3.0+cu121 Optional for data exchange between DaphneLib and PyTorch"},{"location":"GettingStarted/#hardware","title":"Hardware","text":"<ul> <li>about 7.5 GB of free disk space to build from source (mostly due to dependencies)</li> <li>Optional:</li> <li>NVidia GPU for CUDA ops (tested on Pascal and newer architectures); 8GB for CUDA SDK</li> <li>Intel GPU for OneAPI ops (tested on Coffeelake graphics); 23 GB for OneAPI</li> <li>Intel FPGA for FPGAOPENCL ops (tested on PAC D5005 accelerator); 23 GB for OneAPI</li> </ul>"},{"location":"GettingStarted/#obtaining-the-source-code","title":"Obtaining the Source Code","text":"<p>The DAPHNE system is based on MLIR, which is a part of the LLVM monorepo. The LLVM monorepo is included in this repository as a submodule. Thus, clone this repository as follows to also clone the submodule:</p> <pre><code>git clone --recursive https://github.com/daphne-eu/daphne.git\n</code></pre> <p>Upstream changes to this repository might contain changes to the submodule (we might have upgraded to a newer version of MLIR/LLVM). Thus, please pull as follows:</p> <pre><code># in git &gt;= 2.14\ngit pull --recurse-submodules\n\n# in git &lt; 2.14\ngit pull &amp;&amp; git submodule update --init --recursive\n\n# or use this little convenience script\n./pull.sh\n</code></pre>"},{"location":"GettingStarted/#building-the-daphne-system","title":"Building the DAPHNE system","text":"<p>Simply build the system using the build-script without any arguments:</p> <pre><code>./build.sh\n</code></pre> <p>When you do this the first time, or when there were updates to the LLVM submodule, this will also download and build the third-party material, which might increase the build time significantly. Subsequent builds, e.g., when you changed something in this repository, will be much faster.</p> <p>If the build fails in between (e.g., due to missing packages), multiple build directories (e.g., daphne, antlr, llvm) require cleanup. To only remove build output use the following two commands:</p> <pre><code>./build.sh --clean\n./build.sh --cleanDeps\n</code></pre> <p>If you want to remove downloaded and extracted artifacts, use this:</p> <pre><code>./build.sh --cleanCache\n</code></pre> <p>For convenience, you can call the following to remove them all.</p> <pre><code>./build.sh --cleanAll\n</code></pre> <p>See this page for more information.</p>"},{"location":"GettingStarted/#setting-up-the-environment","title":"Setting up the environment","text":"<p>As DAPHNE uses shared libraries, these need to be found by the operating system's loader to link them at runtime. Since most DAPHNE setups will not end up in one of the standard directories (e.g., <code>/usr/local/lib</code>), environment variables are a convenient way to set everything up without interfering with system installations (where you might not even have administrative privileges to do so).</p> <pre><code># from your cloned DAPHNE repo or your otherwise extracted sources/binaries: \nexport DAPHNE_ROOT=$PWD \nexport LD_LIBRARY_PATH=$DAPHNE_ROOT/lib:$DAPHNE_ROOT/thirdparty/installed/lib:$LD_LIBRARY_PATH\n# optionally, you can add the location of the DAPHNE executable to your PATH:\nexport PATH=$DAPHNE_ROOT/bin:$PATH\n</code></pre> <p>If you're running/compiling DAPHNE from a container you'll most probably *not* need to set these environment variables (unless you have reason to customize your setup - then it is assumed that you know what you are doing).</p>"},{"location":"GettingStarted/#running-the-tests","title":"Running the Tests","text":"<pre><code>./test.sh\n</code></pre> <p>We use catch2 as the unit test framework. You can use all command line arguments of catch2 with <code>test.sh</code>.</p>"},{"location":"GettingStarted/#running-daphne","title":"Running DAPHNE","text":"<p>Write a little DaphneDSL script or use <code>scripts/examples/hello-world.daph</code>...</p> <pre><code>print(\"Hello World!\");\n</code></pre> <p>... and execute it as follows: <code>bin/daphne scripts/examples/hello-world.daph</code> (This command works if <code>daphne</code> is run after building from source. Omit <code>bin/</code> in the path to the DAPHNE binary if executed from the binary distribution).</p> <p>Optionally flags like <code>--cuda</code> can be added after the daphne command and before the script file to activate support for accelerated ops (see software requirements above and build instructions). For further flags that can be set at runtime to activate additional functionality, run <code>daphne --help</code>.</p>"},{"location":"GettingStarted/#building-and-running-with-containers","title":"Building and Running with Containers","text":"<p>To avoid installing dependencies and to circumvent conflicts with existing installed libraries, one may use containers.</p> <ul> <li>You need to install Docker or Singularity: Docker version 20.10.2 or higher | Singularity version 3.7.0-1.el7 or higher are sufficient</li> <li>You can use the provided docker files and scripts to create and run DAPHNE.</li> </ul> <p>A full description on containers is available in the containers subdirectory.</p> <p>The following recreates all images provided by daphneeu</p> <pre><code>cd containers\n./build-containers.sh\n</code></pre> <p>Running in an interactive container can be done with this run script, which takes care of mounting your current directory and handling permissions:</p> <pre><code># please customize this script first\n./containers/run-docker-example.sh\n</code></pre> <p>For more about building and running with containers, refer (once again) to the directory <code>containers/</code> and its README.md. For documentation about using containers in conjunction with our cluster deployment scripts, refer to Deploy.md.</p>"},{"location":"GettingStarted/#exploring-the-source-code","title":"Exploring the Source Code","text":"<p>As an entry point for exploring the source code, you might want to have a look at the code behind the <code>daphne</code> executable, which can be found in <code>src/api/cli/daphne.cpp</code>.</p> <p>On the top-level, there are the following directories:</p> <ul> <li><code>bin</code>: after compilation, generated binaries will be placed here (e.g., daphne)</li> <li><code>build</code>: temporary build output</li> <li><code>containers</code>: scripts and configuration files to get/build/run with Docker or Singularity containers</li> <li><code>deploy</code>: shell scripts to ease deployment in SLURM clusters</li> <li><code>doc</code>: documentation written in markdown (e.g., what you are reading at the moment)</li> <li><code>lib</code>: after compilation, generated library files will be placed here (e.g., libAllKernels.so, libCUDAKernels.so, ...)</li> <li><code>scripts</code>: a collection of algorithms and examples written in DAPHNE's own domain specific language (DaphneDSL)</li> <li><code>src</code>: the actual source code, subdivided into the individual components of the system</li> <li><code>test</code>: test cases</li> <li><code>thirdparty</code>: required external software</li> </ul>"},{"location":"HDFS-Usage/","title":"HDFS Usage","text":"<p>About employing HDFS as a distributed file system.</p> <p>This document shows how a DAPHNE user can execute DAPHNE scripts using HDFS as a file system, which is optimized for performance on big data through distributed computing. This document assumes that DAPHNE was build with the <code>--hdfs</code> options, if this is not the case please rebuild DAPHNE with the <code>--hdfs</code> option <code>./build.sh --hdfs</code></p> <p>The DAPHNE build script uses HAWQ (libhdfs3).</p>"},{"location":"HDFS-Usage/#configuring-daphne-for-hdfs","title":"Configuring DAPHNE for HDFS","text":"<p>In order for DAPHNE to utilize the HDFS file system certain command line arguments need to be passed (or included in the config file).</p> <ul> <li><code>--enable-hdfs</code>: A flag to enable hdfs.</li> <li><code>--hdfs-address=&lt;IP:PORT&gt;</code>: The IP and port HDFS listens to.</li> <li><code>--hdfs-username=&lt;username&gt;</code>: The username used to connect to HDFS.</li> </ul>"},{"location":"HDFS-Usage/#reading-from-hdfs","title":"Reading from HDFS","text":"<p>In order to read a file from the HDFS some pre processing must be done. Assuming the file is named <code>FILE_NAME</code>, a user needs to:</p> <ol> <li>Upload the file into HDFS. DAPHNE expects the file to be located inside a directory with some specific naming conventions.    The path can by any path under HDFS, however the file must be named with the following convention:</li> </ol> <pre><code>/path/to/hdfs/file/FILE_NAME.FILE_TYPE/FILE_NAME.FILE_TYPE_segment_1\n</code></pre> <p><code>FILE_TYPE</code> is either <code>.csv</code> or <code>.dbdf</code> (DAPHNE binary data format) followed by <code>.hdfs</code>, e.g. <code>myfile.csv.hdfs</code>.</p> <p>The suffix <code>_segment_1</code> is necessary, since we support multiple writers at once (see more below), the writers need to write into different files (different segments). In this case where the user pre-uploads the file, it needs to be in the same format, but just one segment.</p> <p>Each segment must also have it's own .meta file within the HDFS. This is a JSON file containg information about the size of the segment as well as the type. For example <code>myfile.csv.hdfs_segment_1.meta</code>:</p> <pre><code>{\n    \"numCols\": 10,\n    \"numRows\": 10,\n    \"valueType\": \"f64\"\n}\n</code></pre> <ol> <li>We also need to create a .meta file containing information about the file, within the local file system (from where DAPHNE is invoked).     Similar to any other file which will be read by DAPHNE, we need to create a .meta file, which is in JSON format, containing information about where the     file is, information about the rows/cols etc. The file should be named: <code>FILE_NAME.FILE_TYPE.meta</code>, e.g.     <code>myfile.csv.hdfs.meta</code>. The meta file should contain all the regular information any DAPHNE meta file contains, but in addition it also contains information about whether this is an HDFS file and where it is located within HDFS:</li> </ol> <pre><code>{\n    \"hdfs\": {\n        \"HDFSFilename\": \"/path/to/hdfs/file/FILE_NAME.FILE_TYPE\",\n        \"isHDFS\": true\n    },\n    \"numCols\": 10,\n    \"numRows\": 10,\n    \"valueType\": \"f64\"\n}\n</code></pre>"},{"location":"HDFS-Usage/#example","title":"Example:","text":"<p>Let's say we have a dataset called <code>training_data.csv</code> which we want to upload to HDFS and use it with DAPHNE.</p> <ol> <li>Upload file under path <code>datasets</code> and create the segment .meta file. HDFS should look like this:</li> </ol> <pre><code>$ hdfs dfs -ls /\n/datasets/training_data.csv.hdfs/training_data.csv.hdfs_segment_1\n/datasets/training_data.csv.hdfs/training_data.csv.hdfs_segment_1.meta\n\n$ hdfs dfs -cat /datasets/training_data.csv.hdfs/training_data.csv.hdfs_segment_1.meta\n{\"numCols\":10,\"numRows\":10,\"valueType\":\"f64\"}\n</code></pre> <ol> <li>Create the local file .meta file:</li> </ol> <pre><code>$ cat ./training_data.csv.hdfs.meta\n{\"hdfs\":{\"HDFSFilename\":\"/datasets/training_data.csv.hdfs\",\"isHDFS\":true},\"numCols\":10,\"numRows\":10,\"valueType\":\"f64\"}\n</code></pre> <ol> <li>DAPHNE script:</li> </ol> <pre><code>X = readMatrix(\"training_data.csv.hdfs\");\nprint(X);\n</code></pre> <ol> <li>Run DAPHNE</li> </ol> <pre><code>./bin/daphne --enable-hdfs --hdfs-ip=&lt;IP:PORT&gt; --hdfs-username=ubuntu code.daph\n</code></pre>"},{"location":"HDFS-Usage/#writing-to-hdfs","title":"Writing to HDFS","text":"<p>In order to write to HDFS we just need to use the <code>writeMatrix</code> function like we would for any other file type and specify the hdfs suffix. For example:</p> <ol> <li>Code</li> </ol> <pre><code>X = rand(10, 10, 0.0, 1.0, 1.0, 1);\nwriteMatrix(X, \"randomSet.csv.hdfs\");\n</code></pre> <ol> <li>Call daphne</li> </ol> <pre><code>./bin/daphne --enable-hdfs --hdfs-ip=&lt;IP:PORT&gt; --hdfs-username=ubuntu code.daph\n</code></pre> <p>This will create the following files inside HDFS:</p> <pre><code>$ hdfs dfs -ls /\n/randomSet.csv.hdfs/randomSet.csv.hdfs_segment_1\n/randomSet.csv.hdfs/randomSet.csv.hdfs_segment_1.meta\n\n$ hdfs dfs -cat /randomSet.csv.hdfs/randomSet.csv.hdfs_segment_1.meta\n{\"numCols\":10,\"numRows\":10,\"valueType\":\"f64\"}\n</code></pre> <p>And also the .meta file within the local file system named <code>randomSet.csv.hdfs.meta</code>:</p> <pre><code>{\n    \"hdfs\": {\n        \"HDFSFilename\": \"/randomSet.csv.hdfs\",\n        \"isHDFS\": true\n    },\n    \"numCols\": 10,\n    \"numRows\": 10,\n    \"valueType\": \"f64\"\n}\n</code></pre>"},{"location":"HDFS-Usage/#limitations","title":"Limitations:","text":"<p>For now writing to a specific directory, through DAPHNE, within HDFS is not supported. DAPHNE will always try to write under the root HDFS directory <code>/&lt;name&gt;.&lt;type&gt;.hdfs</code>.</p>"},{"location":"HDFS-Usage/#distributed-runtime","title":"Distributed Runtime","text":"<p>Both read and write operations are supported by the distributed runtime.</p>"},{"location":"HDFS-Usage/#read","title":"Read","text":"<p>Exactly the same preprocessing must be done, creating one file inside the HDFS with the appropriate naming conventions. Users can then run DAPHNE using the distributed runtime and depending on the generated pipeline, DAPHNE's distributed workers will read their corresponding part of the data speeding up IO significantly. For example:</p> <ol> <li>DAPHNE script:</li> </ol> <pre><code>X = readMatrix(\"training_data.csv.hdfs\");\nprint(X+X);\n</code></pre> <ol> <li>Run DAPHNE</li> </ol> <pre><code>$ export DISTRIBUTED_WORKERS=worker-1:&lt;PORT&gt;:worker-2:&lt;PORT&gt;\n$ ./bin/daphne --distributed --dist_backend=sync-gRPC --enable-hdfs --hdfs-ip=&lt;IP:PORT&gt; --hdfs-username=ubuntu code.daph\n</code></pre>"},{"location":"HDFS-Usage/#write","title":"Write","text":"<p>Similar to read, nothing really changes, users just need to call DAPHNE using the distributed runtime flags. Notice that since we have multiple workers/writers, more than one segements are generated inside HDFS:</p> <ol> <li>Code</li> </ol> <pre><code>X = rand(10, 10, 0.0, 1.0, 1.0, 1);\nwriteMatrix(X, \"randomSet.csv.hdfs\");\n</code></pre> <ol> <li>Call daphne</li> </ol> <pre><code>$ export DISTRIBUTED_WORKERS=worker-1:&lt;PORT&gt;:worker-2:&lt;PORT&gt;\n$ ./bin/daphne --distributed --dist_backend=sync-gRPC --enable-hdfs --hdfs-ip=&lt;IP:PORT&gt; --hdfs-username=ubuntu code.daph\n</code></pre> <p>Assuming 2 distributed workers:</p> <pre><code>$ hdfs dfs -ls /\n/randomSet.csv.hdfs/randomSet.csv.hdfs_segment_1        # First part of the matrix\n/randomSet.csv.hdfs/randomSet.csv.hdfs_segment_1.meta\n/randomSet.csv.hdfs/randomSet.csv.hdfs_segment_2        # Second part of the matrix\n/randomSet.csv.hdfs/randomSet.csv.hdfs_segment_2.meta\n\n$ hdfs dfs -cat /randomSet.csv.hdfs/randomSet.csv.hdfs_segment_1.meta\n{\"numCols\":10,\"numRows\":5,\"valueType\":\"f64\"}\n$ hdfs dfs -cat /randomSet.csv.hdfs/randomSet.csv.hdfs_segment_2.meta\n{\"numCols\":10,\"numRows\":5,\"valueType\":\"f64\"}\n</code></pre> <p>And also the .meta file within the local file system named <code>randomSet.csv.hdfs.meta</code>.</p>"},{"location":"HDFS-Usage/#notes","title":"Notes","text":"<p>It does not matter how many segments are generated or exist. DAPHNE is designed to read the segments according to the current state (distributed or not and how many distributed workers are being used).</p> <p>For example if we use 4 distributed workers to write a matrix, DAPHNE will generate 4 different segments. DAPHNE can later read the same matrix either in local execution (no distributed runtime) or using a different number of workers, not depending on the amount of segments generated earlier.</p>"},{"location":"MPI-Usage/","title":"MPI Usage","text":"<p>About employing MPI as a distributed runtime backend.</p> <p>The DAPHNE runtime system is designed with the goal of supporting various distributed runtime that relies on various technologies, e.g. MPI and RPC.</p> <p>This document shows how a DAPHNE user can execute DAPHNE scripts on a distributed computing environment with the MPI backend implementation of the DAPHNE runtime system. This document assumes that the DAPHNE was build with the <code>--mpi</code> options, if this is not the case please rebuild DAPHNE with the <code>--mpi</code> option <code>./build.sh --mpi</code></p> <p>The DAPHNE build script uses Open MPI. The DAPHNE build script does not configure the Open MPI installation with the SLURM support option. For users who want to add the SLURM, please visit the Open MPI documentation (adding <code>--with-slurm</code> to the build command of the Open MPI libbrary) and edit the DAPHNE build script. Also, users who wants to use other MPI implementations e.g., Intel MPI may edit the corresponding part in the DAPHNE build script.</p>"},{"location":"MPI-Usage/#when-daphne-is-installed-natively-wo-container","title":"When DAPHNE is Installed Natively (w/o Container)","text":"<ol> <li> <p>Ensure that your system knows about the installed MPI     -- The <code>PATH</code> and <code>LD_LIBRARY_PATH</code>environment variable has to be updated as follows  </p> <pre><code>export PATH=$PATH:&lt;DAPHNE_INSTALLATION&gt;/thirdparty/installed/bin/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;DAPHNE_INSTALLATION&gt;//thirdparty/installed/lib/ \n</code></pre> <p>Please do not forget to replace <code>&lt;DAPHNE_INSTALLATION&gt;</code> with the actual path</p> </li> <li> <p>Run basic example @ <code>/examples/matrix_addition_for_mpi.daph</code> as follows</p> <pre><code>mpirun -np 10 ./bin/daphne --distributed --dist_backend=MPI scripts/examples/matrix_addition_for_mpi.daph\n</code></pre> </li> </ol> <p>The command above executes 10 processes locally on one machine.</p> <p>In order to run on a distributed system, you need to provide the machine names or the machinefile which contains the machine names. For instance assuming that <code>my_hostfile</code> is a text file that contains machine names</p> <pre><code>mpirun -np 10 --hostfile my_hostfile  ./bin/daphne --distributed --dist_backend=MPI scripts/examples/matrix_addition_for_mpi.daph\n</code></pre> <p>The command above starts 10 processes distributed on following the hosts in the my_hostfile. For more options, please check the Open MPI documentation.</p> <p>From a DAPHNE runtime point of view, the <code>--distributed</code> option tells the DAPHNE runtime system to utilize the distributed backend, while the <code>--dist_backend=MPI</code> indicate the type of the backend implementation.</p>"},{"location":"MPI-Usage/#when-daphne-is-installed-with-containers-eg-singularity","title":"When DAPHNE is Installed with Containers (e.g. singularity)","text":"<p>The main difference is that the mpirun command is called at the level of the container as follows</p> <pre><code>mpirun -np 10 singularity exec &lt;singularity-image&gt; daphne/bin/daphne --distributed   --dist_backend=MPI --vec --num-threads=2 daphne/scripts/examples/matrix_addition_for_mpi.daph\n</code></pre> <p>Please do not forget to replace <code>&lt;singularity-image&gt;</code> with the actual singularity image.</p>"},{"location":"Profiling/","title":"Profiling DAPHNE using PAPI","text":"<p>You can profile your DAPHNE script by using the <code>--enable-profiling</code> CLI switch. Note that profiling is only available if DAPHNE was built without the <code>--no-papi</code> flag.</p> <p>DAPHNE supports profiling via the PAPI profiling library, specifically the high-level (HL) PAPI API.</p> <p>When run with profiling enabled, the DAPHNE compiler will generate code that automatically starts and stops profiling (via PAPI) at the start and end of the DAPHNE script.</p> <p>You can configure which events to profile via the <code>PAPI_EVENTS</code> environmental variable, e.g.:</p> <pre><code>$ PAPI_EVENTS=\"perf::CYCLES,perf::INSTRUCTIONS,perf::CACHE-REFERENCES,perf::CACHE MISSES,perf::BRANCHES,perf::BRANCH-MI SSES\" PAPI_REPORT=1 ./daphne --enable-profiling script.daph\n</code></pre> <p>For more details about the supported events as well as other PAPI-HL configuration options you can check the PAPI HL API documentation. You can also get a list of the supported events on your machine via the <code>papi_native_avail</code> PAPI utility (included in the <code>papi-tools</code> package on Debian-based systems).</p>"},{"location":"ReleaseScripts/","title":"Release Scripts","text":"<p>How to use the release scripts to create binary artifacts</p> <p>This is a quick write-up of how the scripts to create a binary release artifact are meant to be used.</p> <p>The release.sh script will call pack.sh which will call build.sh and test.sh. Only if testing completes successfully, the artifact, a gzipped tar archive (format open for discussion) is created. The command after <code>--githash</code> fetches the git hash of the current commit. The script checks out this git hash and restores the current commit after successful completion. This is a bit of a shortcoming as you have to issue a manual <code>git checkout -</code> if the script fails and terminates early.</p>"},{"location":"ReleaseScripts/#signing","title":"Signing","text":"<p>The release manager will have to sign the artifacts to verify that the provided software has been created by that person. To create an appropriate GPG key, these instructions can be adapted to our needs. The keys of Daphne release managers will be provided in this file. Ideally, future release managers sign each others keys. Key signing is a form of showing that the one key owner trusts the other.</p>"},{"location":"ReleaseScripts/#the-procedure-preliminary-for-v01","title":"The Procedure (Preliminary for v0.1)","text":"<ol> <li>Get into a bash shell and change to your working copy (aka daphne root) directory.</li> <li>Create the artifacts (plain Daphne): <code>./release.sh --version 0.1 --githash `git rev-parse HEAD`</code></li> <li>Create additional artifacts with extra features compiled in: <code>./release.sh --version 0.1 --githash `git rev-parse HEAD` --feature cuda</code> Note that this adds additional constraints on the binaries (e.g., if CUDA support is compiled in, the executable will fail to load on a system without the CUDA SDK properly installed)_</li> <li>Copy the artifacts to a machine where you have your top secret signing key installed (can be skipped if this is the build machine): <code>rsync -vuPah &lt;hostname&gt;:path/to/daphne/artifacts .</code></li> <li> <p>Signing and checksumming:</p> <pre><code>cd artifacts\n~/path/to/daphne/release.sh --version 0.1 --artifact ./daphne-0.1-bin.tgz --gpgkey &lt;GPG_KEY_ID&gt; --githash `cat daphne-0.1-bin.githash` \n</code></pre> </li> <li> <p>repeat for other feature artifacts</p> </li> <li> <p>Tag &amp; push The previous signing command will provide you with two more git commands to tag the commit that the artfiacts were made from and to push these tags to github.     This should look something like this:</p> <pre><code>git tag -a -u B28F8F4D 0.1 312b2b50b4e60b3c5157c3365ec38383d35e28d8\ngit push git@github.com:corepointer/daphne.git --tags\n</code></pre> </li> <li> <p>Upload &amp; release:</p> <ul> <li>Click the \"create new release\" link on the front page of the Daphne github repository (right column under \"Releases\").</li> <li>Select the tag for the release, create a title, add release notes (highlights of this release, list of contributors, maybe a detailed change log at the end)</li> <li>Upload the artifacts: All the <code>&lt;filename&gt;.{tgz,tgz.asc,tgz.sha512sum}</code> files before either saving as draft for further polishing or finally release the new version.</li> </ul> </li> </ol>"},{"location":"RunningDaphneLocally/","title":"Running DAPHNE Locally","text":"<p>Running DAPHNE in a local environment.</p> <p>This document explains how to run DAPHNE on a local machine. For more details on running DAPHNE in a distributed setup, please see the documentation on the distributed runtime and distributed deployment.</p> <p>Before DAPHNE can be executed, the system must be built using <code>./build.sh</code> (for more details see Getting Started). The main executable of the DAPHNE system is <code>bin/daphne</code>. The general scheme of an invocation of DAPHNE looks as follows:</p> <pre><code>bin/daphne [options] script [arguments]\n</code></pre> <p>Where <code>script</code> is a DaphneDSL file.</p> <p>Example:</p> <pre><code>bin/daphne scripts/examples/hello-world.daph\n</code></pre> <p>Note that the present working directory should be the root directory <code>daphne/</code> when invoking the system (this requirement will be relaxed in the future).</p>"},{"location":"RunningDaphneLocally/#passing-script-arguments","title":"Passing Script Arguments","text":"<p>Arguments to the DaphneDSL script can be provided as space-separated pairs of the form <code>key=value</code>. These can the accessed as <code>$key</code> in the DaphneDSL script.</p> <p>Example:</p> <pre><code>bin/daphne test/api/cli/algorithms/kmeans.daphne r=1000 f=20 c=5 i=10\n</code></pre> <p>This example executes a simplified variant of the k-means clustering algorithm on random data with 1000 rows and 20 features using 5 centroids and a fixed number of 10 iterations.</p> <p><code>value</code> must be a valid DaphneDSL literal, e.g., <code>key=123</code> (signed 64-bit integer), <code>key=-12.3</code> (double-precision floating-point), or <code>key=\"hello\"</code> (string). Note that the quotation marks <code>\"</code> are part of the string literal, so they must be escaped on a terminal, e.g., by <code>key=\\\"hello\\\"</code>. If there are whitespaces in the string, it is necessary to surround the literal with additional quotation marks, like <code>key=\"\\\"hello world\\\"\"</code></p>"},{"location":"RunningDaphneLocally/#command-line-arguments","title":"Command-Line Arguments","text":"<p>The behavior of <code>daphne</code> can be influenced by numerous command-line arguments (the <code>options</code> mentioned above). To see the full list of available options, invoke <code>bin/daphne --help</code>.</p> <p>In the following, a few noteworthy general options are mentioned. Note that some of the more specific options are described in the documentation pages on the respective topics, e.g., distributed execution, scheduling, configuration, FPGA configuration, etc.</p> <ul> <li> <p><code>--explain</code></p> <p>Prints the MLIR-based intermediate representation (IR), the so-called DaphneIR, after the specified compiler passes. For instance, to see the IR after parsing (and some initial simplifications) and after property inference, invoke</p> <pre><code>bin/daphne --explain parsing_simplified,property_inference test/api/cli/algorithms/kmeans.daphne r=1000 f=20 c=5 i=10\n</code></pre> </li> <li> <p><code>--vec</code></p> <p>Turns on DAPHNE's vectorized execution engine, which fuses qualifying operations into vectorized pipelines. Experimental feature.</p> </li> <li> <p><code>--select-matrix-repr</code></p> <p>Turns on the automatic selection of a suitable matrix representation (currently dense or sparse (CSR)). Experimental feature.</p> </li> </ul>"},{"location":"RunningDaphneLocally/#return-codes","title":"Return Codes","text":"<p>If <code>daphne</code> terminates normally, one of the following status codes is returned:</p> code meaning example 0 success everything went well 1 parser error a syntax error in a DaphneDSL script 2 compiler/pass error an operation was provided with inputs of incompatible shapes 3 runtime/execution error a kernel was invoked with invalid arguments"},{"location":"RunningDaphneLocally/#typical-errors-and-troubleshooting","title":"Typical Errors and Troubleshooting","text":""},{"location":"RunningDaphneLocally/#parser-error-pass-error-execution-error","title":"<code>Parser error: ...</code>/<code>Pass error: ...</code>/<code>Execution error: ...</code>","text":"<p>One of the three types of errors mentioned above occured. In many (but not yet all) cases, there will be an error message indicating what went wrong.</p> <p>Examples:</p> <ul> <li> <p>Wrong way of passing string literals as DaphneDSL script arguments.</p> <pre><code>line 1:0 mismatched input 'foo' expecting {'true', 'false', INT_LITERAL, FLOAT_LITERAL, STRING_LITERAL}\nParser error: unexpected literal\n</code></pre> <p>Maybe you tried to pass a string as an argument to a DaphneDSL script and forgot the quotation marks or they got lost. Pass strings as <code>bin/daphne script.daphne foo=\\\"abc\\\"</code> (not <code>foo=abc</code> or <code>foo=\"abc\"</code>) on a terminal.</p> </li> <li> <p>Missing metadata file.</p> <pre><code>Parser error: Could not open file 'data/foo.csv.meta' for reading meta data.\n</code></pre> <p>Maybe you try to read a dataset called <code>data/foo.csv</code>, but the required metadata file <code>data/foo.csv.meta</code> does not exist.</p> </li> <li> <p>Using the old file metadata format.</p> <pre><code>Parser error: [json.exception.parse_error.101] parse error at line 1, column 7: syntax error while parsing value - unexpected ','; expected end of input\n</code></pre> <p>Maybe you try to read a dataset with <code>readMatrix()</code> or <code>readFrame()</code> in DaphneDSL, but the file metadata file does not have the right structure. Note that we changed the initial one-line text-based format to a more human-readable JSON-based format.</p> </li> </ul>"},{"location":"RunningDaphneLocally/#jit-session-error-symbols-not-found","title":"<code>JIT session error: Symbols not found: ...</code>","text":"<p>This error occurs when the execution of a DaphneDSL script requires invoking a kernel with an input/output type combination that was not pre-compiled. The first line indicates which kernel is missing for which type combination.</p> <p>Ultimately, DAPHNE will circumvent this situation automatically by knowing which kernels were pre-compiled and utilizing only those (while employing casts to adapt the types of the arguments and results, where necessary).</p> <p>At the moment, users can try to work around this by introducing casts in the DaphneDSL script. Developers can fix this problem by adding the respective instantiation in <code>src/runtime/local/kernels/kernels.json</code>.</p> <p>Example:</p> <pre><code>JIT session error: Symbols not found: [ _ewAdd__int32_t__int32_t__int32_t ]\nJIT-Engine invocation failed: Failed to materialize symbols: { (main, { _mlir_ciface_main, _mlir_main, _mlir__mlir_ciface_main, main }) }Program aborted due to an unhandled Error:\nFailed to materialize symbols: { (main, { _mlir_ciface_main, _mlir_main, _mlir__mlir_ciface_main, main }) }\nAborted (core dumped)\n</code></pre>"},{"location":"RunningDaphneLocally/#failed-to-create-memorybuffer-for","title":"<code>Failed to create MemoryBuffer for: ...</code>","text":"<p>This error occurs when <code>daphne</code> is not invoked from the repository's root directory <code>daphne/</code> as <code>bin/daphne</code>. It will be fixed in the future (see issue #445). In the meantime, please always invoke <code>daphne</code> from the repository's root directory <code>daphne/</code>.</p> <p>Example:</p> <pre><code>Failed to create MemoryBuffer for: lib/libAllKernels.so\nError: No such file or directory\n</code></pre> <p>Typically followed by an error or the type <code>JIT session error: Symbols not found: ...</code>, which is described above.</p>"},{"location":"SchedulingOptions/","title":"DAPHNE Scheduling","text":"<p>This document describes the use of the pipeline and task scheduling mechanisms currently supported in the DAPHNE system.</p>"},{"location":"SchedulingOptions/#scheduling-decisions","title":"Scheduling Decisions","text":"<p>The DAPHNE system considers four types of scheduling decisions: work partitioning, assignment, ordering and timing.</p> <ul> <li>Work partitioning refers to the partitioning of the work into units of work (or tasks) according to a certain granularity (fine or coarse, equal or variable).</li> <li>Work assignment refers to mapping (or placing) the units of work (or tasks) onto individual units of execution (processes or threads).</li> <li>Work ordering refers to the order in which the tasks are executed. We rely on the vectorized execution engine, therefore, tasks within a vectorized pipeline have no dependencies and can be executed in any order.</li> <li>Work timing refers to the times at which the units of work are set to begin execution on the assigned units of execution.</li> </ul> <p>Work Partitioning: The the DAPHNE prototype supports twelve partitioning schemes: Static (STATIC), Self-scheduling (SS), Guided self-scheduling (GSS), Trapezoid self-scheduling (TSS), Trapezoid Factoring self-scheduling (TFSS), Fixed-increase self-scheduling (FISS), Variable-increase self-scheduling (VISS), Performance loop-based self-scheduling (PLS), Modified version of Static (MSTATIC), Modified version of fixed size chunk self-scheduling (MFSC), and Probabilistic self-scheduling (PSS). The granularity of the tasks generated and scheduled by the DAPHNE system follows one of these partitioning schemes (See Section 4.1.1.1 in Deliverable 5.1 [D5.1].</p> <p>Work Assignment: The current snapshot of the DAPHNE prototype supports two main assignment mechanisms: Single centralized work queue and Multiple work queues.  When work assignment relies on a centralized work queue (CENTRALIZED), workers follow the self-scheduling principle, i.e., whenever a worker is free and idle, it obtains a task from a central queue. When work assignment relies on multiple work queues, workers follow the work-stealing principle, i.e., whenever workers are free, idle, and have no tasks in their queues, they steal tasks from the work queue of each other. Work queues can be per worker (PERCPU) or per group of workers (PERGROUP).  In work-stealing, workers need to apply a victim selection mechanism to find a queue and steal work from it. The currently supported victim selection mechanisms are SEQ (steal from the next adjacent worker), SEQPRI (Steal from the next adjacent worker, but prioritize same NUMA domain), RANDOM (Steal from a random worker), RANDOMPRI (Steal from a random worker, but prioritize same NUMA domain).</p>"},{"location":"SchedulingOptions/#scheduling-options","title":"Scheduling Options","text":"<p>To list all possible execution options of the DAPHNE system, one needs to execute the following</p> <pre><code>$ ./bin/daphne --help\n</code></pre> <p>The output of this command shows all DAPHNE compilation and execution parameters including the scheduling options that are currently support. The output below shows only the scheduling options that we will cover in this document.</p> <pre><code>&gt; This program compiles and executes a DaphneDSL script.\nUSAGE: daphne [options] script [arguments]\nOPTIONS:\nAdvanced Scheduling Knobs:\n  Choose task partitioning scheme:\n      --STATIC             - Static (default)\n      --SS                 - Self-scheduling\n      --GSS                - Guided self-scheduling\n      --TSS                - Trapezoid self-scheduling\n      --FAC2               - Factoring self-scheduling\n      --TFSS               - Trapezoid Factoring self-scheduling\n      --FISS               - Fixed-increase self-scheduling\n      --VISS               - Variable-increase self-scheduling\n      --PLS                - Performance loop-based self-scheduling\n      --MSTATIC            - Modified version of Static, i.e., instead of n/p, it uses n/(4*p) where n is number of tasks and p is number of threads\n      --MFSC               - Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC\n      --PSS                - Probabilistic self-scheduling\n  Choose queue setup scheme:\n      --CENTRALIZED        - One queue (default)\n      --PERGROUP           - One queue per CPU group\n      --PERCPU             - One queue per CPU core\n  Choose work stealing victim selection logic:\n      --SEQ                - Steal from next adjacent worker\n      --SEQPRI             - Steal from next adjacent worker, prioritize same NUMA domain\n      --RANDOM             - Steal from random worker\n      --RANDOMPRI          - Steal from random worker, prioritize same NUMA domain\n  --debug-mt            - Prints debug information about the Multithreading Wrapper\n  --grain-size=&lt;int&gt;    - Define the minimum grain size of a task (default is 1)\n  --hyperthreading      - Utilize multiple logical CPUs located on the same physical CPU\n  --num-threads=&lt;int&gt;   - Define the number of the CPU threads used by the vectorized execution engine (default is equal to the number of physcial cores on the target node that executes the code)\n  --pin-workers         - Pin workers to CPU cores\n  --pre-partition       - Partition rows into the number of queues before applying scheduling technique\n  --vec                 - Enable vectorized execution engine\nDAPHNE Options:\n  --args=&lt;string&gt;       - Alternative way of specifying arguments to the DaphneDSL script; must be a comma-separated list of name-value-pairs, e.g., `--args x=1,y=2.2`\n  --config=&lt;filename&gt;   - A JSON file that contains the DAPHNE configuration\n  --cuda                - Use CUDA\n  --distributed         - Enable distributed runtime\n  --explain=&lt;value&gt;     - Show DaphneIR after certain compiler passes (separate multiple values by comma, the order is irrelevant)\n    =parsing            -   Show DaphneIR after parsing\n    =parsing_simplified -   Show DaphneIR after parsing and some simplifications\n    =sql                -   Show DaphneIR after SQL parsing\n    =property_inference -   Show DaphneIR after property inference\n    =vectorized         -   Show DaphneIR after vectorization\n    =obj_ref_mgnt       -   Show DaphneIR after managing object references\n    =kernels            -   Show DaphneIR after kernel lowering\n    =llvm               -   Show DaphneIR after llvm lowering\n  --libdir=&lt;string&gt;     - The directory containing kernel libraries\n  --no-obj-ref-mgnt     - Switch off garbage collection by not managing data objects' reference counters\n  --select-matrix-repr  - Automatically choose physical matrix representations (e.g., dense/sparse)\nGeneric Options:\n  --help                - Display available options (--help-hidden for more)\n  --help-list           - Display list of available options (--help-list-hidden for more)\n  --version             - Display the version of this program\nEXAMPLES:\n daphne example.daphne\n  daphne --vec example.daphne x=1 y=2.2 z=\"foo\"\n  daphne --vec --args x=1,y=2.2,z=\"foo\" example.daphne\n</code></pre> <p>NOTE: the DAPHNE system relies on the vectorized (tile) execution engine to support parallelism at the node level. The vectorized execution engine takes decision concerning work partition and assignment during applications\u2019 execution. Therefore, one needs always to use the option --vec with any of the scheduling options that we present in this document.</p>"},{"location":"SchedulingOptions/#multithreading-options","title":"Multithreading Options","text":"<ul> <li> <p>Number of threads: A DAPHNE user can control the total number of threads spawn by the DAPHNE runtime system use the following parameter --num-threads. This parameter should be non-zero positive value. Illegal integer values will be ignored by the system and the default value will be used.  The default value of --num-threads is equal to the total number of physical cores of the host machine. The option can be used as below, e.g., the DAPHNE system spawns only 4 threads.</p> <pre><code>./bin/daphne --vec --num-threads=4 some_daphne_script.daphne\n</code></pre> </li> <li> <p>Thread Pinning: A DAPHNE user can decide if the DAPHNE system pins its threads to the physical cores. Currently, the DAPHNE system supports one simple pining strategy, namely, round-robin strategy.  By default, the DAPHNE system does not pin its threads. The option --pin-workers can be used to activate thread pinning as follows</p> <pre><code>./bin/daphne --vec --pin-threads some_daphne_script.daphne\n</code></pre> </li> <li> <p>Hyperthreading: if a host machine supports hyperthreading, a DAPHNE user can decide to use logical cores, i.e., if the \u2013num-threads is not specified, the DAPHNE system sets the total number of threads to the count of the physical cores. However, when the user specify the following parameter --hyperthreading, the DAPHNE system sets the number of threads to the count of the logical cores.</p> <pre><code>./bin/daphne --vec --hyperthreading some_daphne_script.daphne\n</code></pre> </li> </ul>"},{"location":"SchedulingOptions/#work-partitioning-options","title":"Work Partitioning Options","text":"<ul> <li> <p>Partition Scheme: A DAPHNE user selects the partition scheme by passing the name of the partition scheme as an argument to the DAPHNE system. If the user does not specify a partition scheme, the default partition scheme (STATIC) will be used. As an example, the following command uses GSS as a partition scheme.</p> <pre><code>./bin/daphne --vec --GSS some_daphne_script.daphne\n</code></pre> </li> <li> <p>Task granularity: The DAPHNE user can exploit the --grain-size parameter to set the minimum size of the tasks generated by the DAPHNE system. This parameter should be non-zero positive value. Illegal integer values will be ignored by the system and the default value will be used.  The default value of --grain-size is 1, i.e., the data associated with a task represents 1 row of the input matrix. As an example, the following command uses SS as a partition scheme with minimum task size of 100</p> <pre><code>./bin/daphne --vec --SS --grain-size=100 some_daphne_script.daphne\n</code></pre> </li> </ul>"},{"location":"SchedulingOptions/#work-assignment-options","title":"Work Assignment Options","text":"<ul> <li> <p>Single centralized work queue: By default, the DAPHNE system uses a single centralized work queue. However, the user may explicitly use the following parameter --CENTRALIZED to ensure the use of single centralized work queue.</p> <pre><code>./bin/daphne --vec --GSS --CENTRALIZED some_daphne_script.daphne\n</code></pre> </li> <li> <p>Multiple work queues: a DAPHNE user can exploit the use of multiple work queues by passing one of the following parameters --PERCPU or --PERGROUP. The two parameters cannot be used together, and if --CENTRALIZED is used with any of them, --CENTRALIZED will be ignored by the system.</p> </li> <li> <p>parameter --PERGROUP ensures that the DAPHNE system creates a number of groups equals to the number of NUMA domains on the target host machine. The DAPHNE system assigns equal number of workers (threads) to each of the groups. Workers within the same group share one work queue. The \u2013-PERGROUP can be used as follows</p> <pre><code>./bin/daphne --vec --PERGROUP some_daphne_script.daphne\n</code></pre> </li> <li> <p>The parameter --PERCPU ensures that the DAPHNE system creates a number of queues equal to the total number of workers (threads), i.e., each worker is assigned to a single work queue. The parameter --PERCPU can be used as follows</p> <pre><code>./bin/daphne --vec --PERCPU some_daphne_script.daphne\n</code></pre> </li> <li> <p>Victim Selection: A DAPHNE user can choose a victim selection strategy by passing one of the following parameters --SEQ, --SEQPRI, --RANDOM, and --RANDOMPRI. These parameters activate different victim selection strategies as follows</p> <ul> <li>--SEQ activates a sequential victim selection strategy, i.e., the ith worker steals form the (i+1)th  worker. The last worker steals from the first worker.</li> <li>--SEQPRI is similar to --SEQ except that --SEQPRI priorities workers assigned to the same NUMA domain. When the host machine has one NUMA domain,</li> <li>--SEQ and --SEQPRI have no difference.</li> <li>--RANDOM activates a random victim selection strategy, i.e., the ith worker steals form a randomly chosen worker.</li> <li>--RANDOMPRI is similar to --RANDOM except that --RANDOM priorities workers assigned to the same NUMA domain. When the host machine has one NUMA domain, --RANDOMPRI and --RANDOMPRI have no difference.</li> </ul> </li> </ul> <p>NOTE: When the user does not choose one of these parameters, the DAPHNE system considers --SEQ as a default victim selection strategy.</p> <p>As an example, the following command uses --SEQPRI as a victim selection strategy.</p> <pre><code>./bin/daphne --vec --PERGROUP --SEQPRI some_daphne_script.daphne\n</code></pre>"},{"location":"SchedulingOptions/#references","title":"References","text":"<p>D4.1 DAPHNE: D4.1 DSL Runtime Design, 11/2021</p> <p>D5.1 DAPHNE: D5.1 Scheduler Design for Pipelines and Tasks, 11/2021</p>"},{"location":"DaphneDSL/Builtins/","title":"Built-in Functions","text":"<p>DaphneDSL offers numerous built-in functions, which can be used in every DaphneDSL script without requiring any imports. The general syntax for calling a built-in function is <code>func(param1, param2, ...)</code> (see the DaphneDSL Language Reference).</p> <p>This document provides an overview of the DaphneDSL built-in functions. Note that we are still extending this set of built-in functions. Furthermore, we also plan to create a library of higher-level ML primitives allowing users to productively implement integrated data analysis pipelines at a much higher level of abstraction. Those library functions will internally be implemented using the built-in functions described in this document.</p> <p>We use the following notation (deviating from the DaphneDSL function syntax):</p> <ul> <li>square brackets <code>[]</code> mean that a parameter is optional</li> <li><code>...</code> stands for an arbitrary repetition of the previous parameter (including zero).</li> <li><code>/</code> means alternative options, e.g., <code>matrix/frame</code> means the parameter could be a matrix or a frame</li> </ul>"},{"location":"DaphneDSL/Builtins/#list-of-categories","title":"List of categories","text":"<p>DaphneDSL's built-in functions can be categorized as follows:</p> <ul> <li>Data generation</li> <li>Matrix/frame meta data</li> <li>Elementwise unary</li> <li>Elementwise binary</li> <li>Outer binary (generalized outer product)</li> <li>Aggregation and statistical</li> <li>Reorganization</li> <li>Matrix decomposition &amp; co</li> <li>Deep neural network</li> <li>Other matrix operations</li> <li>Extended relational algebra</li> <li>Conversions and casts</li> <li>Input/output</li> <li>Data preprocessing</li> <li>Measurements</li> <li>List operations</li> </ul>"},{"location":"DaphneDSL/Builtins/#data-generation","title":"Data generation","text":"<ul> <li> <p><code>fill</code><code>(value:scalar, numRows:size, numCols:size)</code></p> <p>Creates a (<code>numRows</code> x <code>numCols</code>) matrix and sets all elements to <code>value</code>.</p> </li> <li> <p><code>createFrame</code><code>(column:matrix, ...[, labels:str, ...])</code></p> <p>Creates a frame from an arbitrary number of column matrices. Optionally, a label can be specified for each column (the number of provided columns and labels must be equal).</p> </li> <li> <p><code>diagMatrix</code><code>(arg:matrix)</code></p> <p>Creates an (n x n) diagonal matrix by placing the elements of the given (n x 1) column-matrix <code>arg</code> on the diagonal of an otherwise empty (zero) square matrix.</p> </li> <li> <p><code>rand</code><code>(numRows:size, numCols:size, min:scalar, max:scalar, sparsity:double, seed:si64)</code></p> <p>Generates a (<code>numRows</code> x <code>numCols</code>) matrix of random values. The values are drawn uniformly from the range [<code>min</code>, <code>max</code>] (both inclusive). A value of zero in the <code>min</code> parameter will be ignored, as the insertion of zeros in the output is controlled by the <code>sparsity</code> parameter. The <code>sparsity</code> can be chosen between <code>0.0</code> (all zeros) and <code>1.0</code> (all non-zeros). The <code>seed</code> can be set to <code>-1</code> (randomly chooses a seed), or be provided explicitly to enable reproducible random values.</p> </li> <li> <p><code>sample</code><code>(range:scalar, size:size, withReplacement:bool, seed:si64)</code></p> <p>Generates a (<code>size</code> x 1) column-matrix of values drawn from the range [0, <code>range - 1</code>]. The parameter <code>withReplacement</code> determines if a value can be drawn multiple times (<code>true</code>) or not (<code>false</code>). The <code>seed</code> can be set to <code>-1</code> (randomly chooses a seed), or be provided explicitly to enable reproducible random values.</p> </li> <li> <p><code>seq</code><code>(from:scalar, to:scalar[, inc:scalar])</code></p> <p>Generates a column matrix containing an arithmetic sequence of values starting at <code>from</code>, going through <code>to</code>, in increments of <code>inc</code>. Note that <code>from</code> may be greater than <code>to</code>, and <code>inc</code> may be negative. The scalar <code>inc</code> is an optional argument and defaults to <code>1</code>.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#matrixframe-meta-data","title":"Matrix/frame meta data","text":"<p>The following built-in functions allow to find out meta data of matrices and frames.</p> <ul> <li> <p><code>typeOf</code><code>(arg:scalar/matrix/frame)</code></p> <p>Returns a string with the data type of <code>arg</code>, which can be a scalar, matrix, or a frame containing mixed value types. For matrices and frames, this includes their dimensions and value type or value type per column in the case of frames. Value types are named after their C++ representation.</p> </li> <li> <p><code>nrow</code><code>(arg:matrix/frame)</code></p> <p>Returns the number of rows in <code>arg</code>.</p> </li> <li> <p><code>ncol</code><code>(arg:matrix/frame)</code></p> <p>Returns the number of columns in <code>arg</code>.</p> </li> <li> <p><code>ncell</code><code>(arg:matrix/frame)</code></p> <p>Returns the number of cells in <code>arg</code>. This is the product of the number of rows and the number of columns.</p> </li> <li> <p><code>sparsity</code><code>(arg:matrix)</code></p> <p>Returns the DAPHNE compiler's estimate of the argument's sparsity. Note that this value may deviate from the actual sparsity of the data at run-time.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#elementwise-unary","title":"Elementwise unary","text":"<p>The following built-in functions all follow the same scheme:</p> <ul> <li> <p><code>unaryFunc</code><code>(arg:scalar/matrix)</code></p> <p>Applies the respective unary function (see table below) to the given scalar <code>arg</code> or to each element of the given matrix <code>arg</code>.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#arithmeticgeneral-math","title":"Arithmetic/general math","text":"function meaning <code>abs</code> absolute value <code>sign</code> signum (<code>1</code> for positive, <code>0</code> for zero, <code>-1</code> for negative) <code>exp</code> exponentiation (e to the power of <code>arg</code>) <code>ln</code> natural logarithm (logarithm of <code>arg</code> to the base of e) <code>sqrt</code> square root"},{"location":"DaphneDSL/Builtins/#trigonometrichyperbolic","title":"Trigonometric/Hyperbolic","text":"<p><code>arg</code> unit must be radians (conversion: \\(x^\\circ * \\frac{\\pi}{180^\\circ} = y\\) radians)</p> function meaning <code>sin</code> sine <code>cos</code> cosine <code>tan</code> tangent <code>asin</code> arc sine  (inverse of sine) <code>acos</code> arc cosine (inverse of cosine) <code>atan</code> arc tangent (inverse of tangent) <code>sinh</code> hyperbolic sine \\(\\left( \\frac{\\exp(\\text{arg}) \\, - \\, \\exp(\\text{ - arg})}{2} \\right)\\) <code>cosh</code> hyperbolic cosine \\(\\left( \\frac{\\exp(\\text{arg}) \\, + \\, \\exp(\\text{ - arg})}{2} \\right)\\) <code>tanh</code> hyperbolic tangent \\(\\left( \\frac{\\text{sinh arg}}{\\text{cosh arg}} \\right)\\)"},{"location":"DaphneDSL/Builtins/#rounding","title":"Rounding","text":"function meaning <code>round</code> round to nearest <code>floor</code> round down <code>ceil</code> round up"},{"location":"DaphneDSL/Builtins/#comparison","title":"Comparison","text":"function meaning <code>isNan</code> <code>1</code> if argument is NaN, <code>0</code> otherwise"},{"location":"DaphneDSL/Builtins/#elementwise-binary","title":"Elementwise binary","text":"<p>DaphneDSL supports various elementwise binary operations. Some of those can be used through operators in infix notation, e.g., <code>+</code>; and some through built-in functions, e.g., <code>log()</code>. Some operations even support both, e.g., <code>pow(a, b)</code> and <code>a^b</code> have the same semantics.</p> <p>The built-in functions all follow the same scheme:</p> <ul> <li> <p><code>binaryFunc</code><code>(lhs:scalar/matrix, rhs:scalar/matrix)</code></p> <p>Applies the respective binary function (see table below) to the corresponding pairs of a value in the left-hand-side argument <code>lhs</code> and the right-hand-side argument <code>rhs</code>. Regarding the combinations of scalars and matrices, the same broadcasting semantics apply as for binary operations like <code>+</code>, <code>*</code>, etc. (see the DaphneDSL Language Reference).</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#arithmetic","title":"Arithmetic","text":"function operator meaning <code>+</code> addition <code>-</code> subtraction <code>*</code> multiplication <code>/</code> division <code>pow</code> <code>^</code> exponentiation (<code>lhs</code> to the power of <code>rhs</code>) <code>log</code> logarithm (logarithm of <code>lhs</code> to the base of <code>rhs</code>) <code>mod</code> <code>%</code> modulo"},{"location":"DaphneDSL/Builtins/#minmax","title":"Min/max","text":"function operator meaning <code>min</code> minimum <code>max</code> maximum"},{"location":"DaphneDSL/Builtins/#logical","title":"Logical","text":"function operator meaning <code>&amp;&amp;</code> logical conjunction <code>||</code> logical disjunction"},{"location":"DaphneDSL/Builtins/#strings","title":"Strings","text":"function operator meaning <code>concat</code> <code>+</code> string concatenation"},{"location":"DaphneDSL/Builtins/#comparison_1","title":"Comparison","text":"function operator meaning <code>==</code> equal <code>!=</code> not equal <code>&lt;</code> less than <code>&lt;=</code> less or equal <code>&gt;</code> greater than <code>&gt;=</code> greater or equal"},{"location":"DaphneDSL/Builtins/#outer-binary-generalized-outer-product","title":"Outer binary (generalized outer product)","text":"<p>The following built-in functions all follow the same scheme:</p> <ul> <li><code>outerBinaryFunc</code><code>(lhs:matrix, rhs:matrix)</code></li> </ul> <p>The argument <code>lhs</code> is expected to be a column (m x 1) matrix, and the argument <code>rhs</code> is expected to be a row (1 x n) matrix.   The result is a (m x n) matrix, whereby the element at position (i, j) is calculated by applying the respective binary function (see the table below) to the i-th element in <code>lhs</code> and the j-th element in <code>rhs</code>.   Schematically, this looks as follows (where <code>\u2218</code> is some binary operation):   <pre><code>       |    b0    b1 ...    bn rhs\n    ---+----------------------\nlhs a0 | a0\u2218b0 a0\u2218b1 ... a0\u2218bn res\n    a1 | a1\u2218b0 a1\u2218b1 ... a1\u2218bn\n    .. | ..... .....     .....\n    am | am\u2218b0 am\u2218b1 ... am\u2218bn\n</code></pre></p>"},{"location":"DaphneDSL/Builtins/#arithmetic_1","title":"Arithmetic","text":"function meaning <code>outerAdd</code> addition <code>outerSub</code> subtraction <code>outerMul</code> multiplication (the well-known outer product) <code>outerDiv</code> division <code>outerPow</code> exponentiation (<code>lhs</code> to the power of <code>rhs</code>) <code>outerLog</code> logarithm (logarithm of <code>lhs</code> to the base of <code>rhs</code>) <code>outerMod</code> modulo"},{"location":"DaphneDSL/Builtins/#minmax_1","title":"Min/max","text":"function meaning <code>outerMin</code> minimum <code>outerMax</code> maximum"},{"location":"DaphneDSL/Builtins/#logical_1","title":"Logical","text":"function meaning <code>outerAnd</code> logical conjunction <code>outerOr</code> logical disjunction <code>outerXor</code> logical exclusive disjunction (not supported yet)"},{"location":"DaphneDSL/Builtins/#strings_1","title":"Strings","text":"function meaning <code>outerConcat</code> string concatenation (not supported yet)"},{"location":"DaphneDSL/Builtins/#comparison_2","title":"Comparison","text":"function meaning <code>outerEq</code> equal <code>outerNeq</code> not equal <code>outerLt</code> less than <code>outerLe</code> less or equal <code>outerGt</code> greater than <code>outerGe</code> greater or equal"},{"location":"DaphneDSL/Builtins/#aggregation-and-statistical","title":"Aggregation and statistical","text":""},{"location":"DaphneDSL/Builtins/#fullrowcolumn-aggregation","title":"Full/row/column aggregation","text":"<p>The following built-in functions all follow the same scheme:</p> <ul> <li> <p><code>agg</code><code>(arg:matrix)</code></p> <p>Full aggregation over all elements of the matrix <code>arg</code> using aggregation function <code>agg</code> (see table below). Returns a scalar.</p> </li> <li> <p><code>agg</code><code>(arg:matrix, axis:si64)</code></p> <p>Row or column aggregation over a (n x m) matrix <code>arg</code> using aggregation function <code>agg</code> (see table below).</p> <ul> <li><code>axis</code> == 0: calculate one aggregate per row; the result is a (n x 1) (column) matrix</li> <li><code>axis</code> == 1: calculate one aggregate per column; the result is a (1 x m) (row) matrix</li> </ul> </li> </ul> function meaning <code>sum</code> summation <code>aggMin</code> minimum <code>aggMax</code> maximum <code>mean</code> arithmetic mean <code>var</code> variance <code>stddev</code> standard deviation <code>idxMin</code> argmin (the index of the minimum value, only for row/column-wise aggregation) <code>idxMax</code> argmax (the index of the maximum value, only for row/column-wise aggregation)"},{"location":"DaphneDSL/Builtins/#cumulative-aggregation","title":"Cumulative aggregation","text":"<p>The following built-in functions all follow the same scheme:</p> <ul> <li> <p><code>cumAgg</code><code>(arg:matrix)</code></p> <p>Cumulative aggregation over each column of the matrix <code>arg</code>. Returns a matrix of the same shape as <code>arg</code>.</p> </li> </ul> function meaning <code>cumSum</code> cumulative sum <code>cumProd</code> cumulative product <code>cumMin</code> cumulative minimum <code>cumMax</code> cumulative maximum"},{"location":"DaphneDSL/Builtins/#reorganization","title":"Reorganization","text":"<ul> <li> <p><code>reshape</code><code>(arg:matrix, numRows:size, numCols:size)</code></p> <p>Changes the shape of <code>arg</code> to (<code>numRows</code> x <code>numCols</code>). Note that the number of cells must be retained, i.e., the product of <code>numRows</code> and <code>numCols</code> must be equal to the product of the number of rows in <code>arg</code> and the number of columns in <code>arg</code>.</p> </li> <li> <p><code>transpose/t</code><code>(arg:matrix)</code></p> <p>Transposes the given matrix <code>arg</code>.</p> </li> <li> <p><code>cbind</code><code>(lhs:matrix/frame, rhs:matrix/frame)</code></p> <p>Concatenates two matrices or two frames horizontally. The two inputs must have the same number of rows.</p> </li> <li> <p><code>rbind</code><code>(lhs:matrix/frame, rhs:matrix/frame)</code></p> <p>Concatenates two matrices or two frames vertically. The two inputs must have the same number of columns.</p> </li> <li> <p><code>reverse</code><code>(arg:matrix)</code></p> <p>Reverses the rows in the given matrix <code>arg</code>.</p> </li> <li> <p><code>order</code><code>(arg:matrix/frame, colIdxs:size, ..., ascs:bool, ..., returnIndexes:bool)</code></p> <p>Sorts the given matrix or frame by an arbitrary number of columns. The columns are specified in terms of their indexes (counting starts at zero). Each column can be sorted either in ascending (<code>true</code>) or descending (<code>false</code>) order (as determined by parameter <code>ascs</code>). The provided number of columns and sort orders must match. The parameter <code>returnIndexes</code> determines whether to return the sorted data (<code>false</code>) or a column-matrix of positions representing the permutation applied by the sorting (<code>true</code>).</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#matrix-decomposition-co","title":"Matrix decomposition &amp; co","text":"<ul> <li> <p><code>eigen</code><code>(arg:matrix)</code></p> <p>Calculates the eigenvalues and eigenvectors of the given matrix. This built-in function has two results: (1) the eigenvalues as a column matrix, and (2) the eigenvectors as a matrix, where each column is an eigenvector.</p> </li> </ul> <p>We plan to support additional matrix decompositions like <code>lu</code>, <code>qr</code>, and <code>svd</code> in the future.</p>"},{"location":"DaphneDSL/Builtins/#deep-neural-network","title":"Deep neural network","text":"<p>Note that most of these operations only have a CUDNN-based kernel for GPU execution at the moment.</p> <ul> <li> <p><code>affine</code><code>(inputData:matrix, weightData:matrix, biasData:matrix)</code></p> </li> <li> <p><code>avg_pool2d</code><code>(inputData:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, poolHeight:size, poolWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size)</code></p> <p>Performs average pooling operation.</p> </li> <li> <p><code>max_pool2d</code><code>(inputData:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, poolHeight:size, poolWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size)</code></p> <p>Performs max pooling operation.</p> </li> <li> <p><code>batch_norm2d</code><code>(inputData:matrix, gamma, beta, emaMean, emaVar, eps)</code></p> <p>Performs batch normalization operation.</p> </li> <li> <p><code>biasAdd</code><code>(input:matrix, bias:matrix)</code></p> <p>Adds the (1 x <code>numChannels</code>) row-matrix <code>bias</code> to the <code>input</code> with the given number of channels.</p> </li> <li> <p><code>conv2d</code><code>(input:matrix, filter:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, filterHeight:size, filterWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size)</code></p> <p>2D convolution.</p> </li> <li> <p><code>relu</code><code>(inputData:matrix)</code></p> </li> <li> <p><code>softmax</code><code>(inputData:matrix)</code></p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#other-matrix-operations","title":"Other matrix operations","text":"<ul> <li> <p><code>diagVector</code><code>(arg:matrix)</code></p> <p>Extracts the diagonal of the given (n x n) matrix <code>arg</code> as a (n x 1) column-matrix.</p> </li> <li> <p><code>lowerTri</code><code>(arg:matrix, diag:bool, values:bool)</code></p> <p>Extracts the lower triangle of the given square matrix <code>arg</code> by setting all elements in the upper triangle to zero. If <code>diag</code> is <code>true</code>, the elements on the diagonal are retained; otherwise, they are set to zero, too. If <code>values</code> is <code>true</code>, the non-zero elements in the lower triangle are retained; otherwise, they are set to one.</p> </li> <li> <p><code>upperTri</code><code>(arg:matrix, diag:bool, values:bool)</code></p> <p>Extracts the upper triangle of the given square matrix <code>arg</code> by setting all elements in the lower triangle to zero. If <code>diag</code> is <code>true</code>, the elements on the diagonal are retained; otherwise, they are set to zero, too. If <code>values</code> is <code>true</code>, the non-zero elements in the upper triangle are retained; otherwise, they are set to one.</p> </li> <li> <p><code>solve</code><code>(A:matrix, b:matrix)</code></p> <p>Solves the system of linear equations given by the (n x n) matrix <code>A</code> and the (n x 1) column-matrix <code>b</code> and returns the result as a (n x 1) column-matrix.</p> </li> <li> <p><code>replace</code><code>(arg:matrix, pattern:scalar, replacement:scalar)</code></p> <p>Replaces all occurrences of the element <code>pattern</code> in the matrix <code>arg</code> by the element <code>replacement</code>.</p> </li> <li> <p><code>ctable</code><code>(ys:matrix, xs:matrix[, weight:scalar][, numRows:int, numCols:int])</code></p> <p>Returns the contingency table of two (n x 1) column-matrices <code>ys</code> and <code>xs</code>. The resulting matrix <code>res</code> consists of <code>max(ys) + 1</code> rows and <code>max(xs) + 1</code> columns. More precisely, \\(\\text{res}[x, y] = \\left| \\{ k \\bigm| \\text{ys}[k, 0] = y \\wedge \\text{xs}[k, 0] = x, \\; 0 \\leq k \\leq n-1 \\} \\right| * \\text{weight} \\quad \\forall x \\in \\text{xs}, y \\in \\text{ys}\\).</p> <p>In other words, starting with an all-zero result matrix, all pairs of values \\(\\{ (\\text{xs}[k, 0],\\text{ys}[k, 0]) \\mid 0 \\leq k \\leq n-1 \\}\\) are used to index the result matrix and increase the corresponding value by <code>weight</code>. Note that <code>ys</code> and <code>xs</code> must not contain negative numbers.</p> <p>The scalar weight is an optional argument and defaults to <code>1.0</code>. The weight also determines the value type of the result.</p> <p>Moreover, optionally, the result shape in terms of the number of rows and columns can be specified. If omited, it defaults to the smallest numbers required to accommodate all given <code>y</code>/<code>x</code>-coordinates, as expressed above. If specified, the result can be either cropped or padded with zeros to the desired shape. If a value less than zero is provided as the number of rows/columns, the respective dimension will also be determined from the input data.</p> <p>This built-in function can be called with 2, 3, 4, or 5 arguments, depending on which optional arguments are given.</p> </li> <li> <p><code>syrk</code><code>(A:martix)</code></p> <p>Calculates <code>t(A) @ A</code> by symmetric rank-k update operations.</p> </li> <li> <p><code>gemv</code><code>(A:matrix, x:matrix)</code></p> <p>Calcuates <code>t(A) @ x</code> for the given (n x m) matrix <code>A</code> and (n x 1) column-matrix <code>x</code>.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#extended-relational-algebra","title":"Extended relational algebra","text":"<p>DaphneDSL supports relational algebra on frames in two ways: On the one hand, entire SQL queries can be executed over previously registered views. This aspect is described in detail in a separate tutorial. On the other hand, built-in functions for individual operations of extended relational algebra can be used on frames in DaphneDSL.</p>"},{"location":"DaphneDSL/Builtins/#entire-sql-query","title":"Entire SQL query","text":"<ul> <li> <p><code>registerView</code><code>(viewName:str, arg:frame)</code></p> <p>Registers the frame <code>arg</code> to be accessible to SQL queries by the name <code>viewName</code>.</p> </li> <li> <p><code>sql</code><code>(query:str)</code></p> <p>Executes the SQL query <code>query</code> on the frames previously registered with <code>registerView()</code> and returns the result as a frame.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#set-operations","title":"Set Operations","text":"<p>We will support set operations such as <code>intersect</code>, <code>merge</code>, and <code>except</code>.</p>"},{"location":"DaphneDSL/Builtins/#cartesian-product-and-joins","title":"Cartesian product and joins","text":"<ul> <li> <p><code>cartesian</code><code>(lhs:frame, rhs:frame)</code></p> <p>Calculates the cartesian (cross) product of the two input frames.</p> </li> <li> <p><code>innerJoin</code><code>(lhs:frame, rhs:frame, lhsOn:str, rhsOn:str)</code></p> <p>Performs an inner join of the two input frames on <code>lhs</code>.<code>lhsOn</code> == <code>rhs</code>.<code>rhsOn</code>.</p> </li> <li> <p><code>semiJoin</code><code>(lhs:frame, rhs:frame, lhsOn:str, rhsOn:str)</code></p> <p>Performs a semi join of the two input frames on <code>lhs</code>.<code>lhsOn</code> == <code>rhs</code>.<code>rhsOn</code>. Returns only the columns belonging to <code>lhs</code>.</p> </li> <li> <p><code>groupJoin</code><code>(lhs:frame, rhs:frame, lhsOn:str, rhsOn:str, rhsAgg:str)</code></p> <p>Group-join of <code>lhs</code> and <code>rhs</code> on <code>lhs.lhsOn == rhs.rhsOn</code> with summation of <code>rhs.rhsAgg</code>.</p> </li> </ul> <p>We will support more variants of joins, including (left/right) outer joins, theta joins, anti-joins, etc.</p>"},{"location":"DaphneDSL/Builtins/#frame-label-manipulation","title":"Frame label manipulation","text":"<ul> <li> <p><code>setColLabels</code><code>(arg:frame, labels:str, ...)</code></p> <p>Sets the column labels of the given frame <code>arg</code> to the given <code>labels</code>. There must be as many <code>labels</code> as columns in <code>arg</code>.</p> </li> <li> <p><code>setColLabelsPrefix</code><code>(arg:frame, predfix:str, ...)</code></p> <p>Prepends the given <code>prefix</code> to the labels of all columns in <code>arg</code>.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#conversions-and-casts","title":"Conversions and casts","text":"<p>Note that DaphneDSL offers casts in form of the <code>as.()</code>-expression. See the DaphneDSL Language Reference for details.</p> <ul> <li> <p><code>quantize</code><code>(arg:matrix&lt;f32&gt;, min:f32, max:f32)</code></p> <p>Performs a <code>min</code>/<code>max</code> quantization of the values in <code>arg</code>. The result matrix is of value type <code>ui8</code>.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#inputoutput","title":"Input/output","text":"<p>DAPHNE supports local file I/O for various file formats. The format is determined by the specified file name extension. Currently, the following formats are supported:</p> <ul> <li>\".csv\": comma-separated values</li> <li>\".mtx\": matrix market</li> <li>\".parquet\": Apache Parquet format</li> <li>\".dbdf\": DAPHNE's binary data format</li> </ul> <p>For both reading and writing, file names can be specified as absolute or relative paths.</p> <p>For most formats, DAPHNE requires additional information on the data and value types as well as dimensions, when reading files. These must be provided in a separate <code>.meta</code>-file.</p> <ul> <li> <p><code>print</code><code>(arg:scalar/matrix/frame[, newline:bool[, toStderr:bool]])</code></p> <p>Prints the given scalar, matrix, or frame <code>arg</code> to <code>stdout</code>. The parameter <code>newline</code> is optional; <code>true</code> (the default) means a new line is started after <code>arg</code>, <code>false</code> means no new line is started. The parameter <code>toStderr</code> is optional; <code>true</code> means the text is printed to <code>stderr</code>, <code>false</code> (the default) means it is printed to <code>stdout</code>.</p> </li> <li> <p><code>readFrame</code><code>(filename:str)</code></p> <p>Reads the file <code>filename</code> into a frame. Assumes that a <code>.meta</code>-file is present for the specified <code>filename</code>.</p> </li> <li> <p><code>readMatrix</code><code>(filename:str)</code></p> <p>Reads the file <code>filename</code> into a matrix. Assumes that a <code>.meta</code>-file is present for the specified <code>filename</code>.</p> </li> <li> <p><code>write/writeFrame/writeMatrix</code><code>(arg:matrix/frame, filename:str)</code></p> <p>Writes the given matrix or frame <code>arg</code> into the specified file <code>filename</code>. Note that the type of <code>arg</code> determines how to store the data; thus, it suffices to call <code>write()</code> (but <code>writeFrame()</code> and <code>writeMatrix()</code> can be used synonymously for consistency with reading). At the same time, this creates a <code>.meta</code>-file for the written file, so that it can be read again using <code>readMatrix()</code>/<code>readFrame()</code>.</p> </li> <li> <p><code>stop</code><code>([message:str])</code></p> <p>Terminates the DaphneDSL script execution with the given optional message.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#data-preprocessing","title":"Data preprocessing","text":"<ul> <li> <p><code>oneHot</code><code>(arg:matrix, info:matrix&lt;si64&gt;)</code></p> <p>Applies one-hot-encoding to the given (n x m) matrix <code>arg</code>. The (1 x m) row-matrix <code>info</code> specifies the details (in the following, d[j] is short for <code>info[0, j]</code>):</p> <ul> <li>If d[j] == -1, then the j-th column of <code>arg</code> will remain as it is.</li> <li>If d[j] == 0, then the j-th column of <code>arg</code> will be omitted in the output.</li> <li> <p>If d[j] &gt; 0, then the j-th column of <code>arg</code> will be encoded to a vector of length d[j].</p> <p>More precisely, if d[j] &gt; 0 the j-th column of <code>arg</code> must contain only integral values in the range [0, d[j] - 1], and will be replaced by d[j] columns containing only zeros and ones. For each row i in <code>arg</code>, the value in the <code>as.scalar(arg[i, j])</code>-th of those columns is set to 1, while all others are set to 0.</p> </li> </ul> </li> <li> <p><code>recode</code><code>(arg:matrix, orderPreserving:bool)</code></p> <p>Applies dictionary encoding to the given (n x 1) matrix <code>arg</code>, i.e., assigns an integral code to each distinct value in <code>arg</code>. The codes start at 0. Iff the parameter <code>orderPreserving</code> is <code>true</code>, then the distinct values are sorted in ascending order before assigning codes. That way, range predicates are possible on the encoded output. Otherwise, new codes are assigned to distinct values are they are encountered. That way, only point predicates are possible on the encoded output.</p> <p>There are two results:</p> <ul> <li>The first result is the encoded data, a (n x 1) matrix of the codes for each element in the input <code>arg</code>.</li> <li>The second result is the decoding dictionary, a (#distinct(<code>arg</code>) x 1) matrix of the distinct values in <code>arg</code>.     The value at position i is the value that is mapped to the code i.</li> </ul> <p>The encoded data can be decoded by right indexing on the dictionary using the codes as positions.</p> <p>Example:</p> <pre><code>data = [10, 5, 20, 5, 20];\ncodes, dict = recode(data, false);\nprint(codes);\nprint(dict);\ndecoded = dict[codes, ];\nprint(decoded);\n</code></pre> <pre><code>DenseMatrix(5x1, int64_t)\n0\n1\n2\n1\n2\nDenseMatrix(3x1, int64_t)\n10\n5\n20\nDenseMatrix(5x1, int64_t)\n10\n5\n20\n5\n20\n</code></pre> </li> <li> <p><code>bin</code><code>(arg:matrix, numBins:size[, min:scalar, max:scalar])</code></p> <p>Applies binning to the given matrix <code>arg</code>, i.e., each value is mapped to the number of its bin (counting starts at zero). The bin boundaries are determined by splitting the interval [<code>min</code>, <code>max</code>] into <code>numBins</code> equi-width bins. The number of bins <code>numBins</code> is required. Specifying <code>min</code> and <code>max</code> is optional; if they are omitted, they are automatically calculated, whereby NaNs in <code>arg</code> are not taken into account.</p> <p>Example:</p> <pre><code>print(bin(t([10, 20, 30, 40, 50, 60, 70]), 3));\nprint(bin(t([10, 20, 30, 40, 50, 60, 70]), 3, 10, 70));\nprint(bin(t([5.0, 20.0, nan, 40.0, inf, 60.0, 100.0]), 3, 10.0, 70.0));\n</code></pre> <pre><code>DenseMatrix(1x7, int64_t)\n0 0 0 1 1 2 2\nDenseMatrix(1x7, int64_t)\n0 0 0 1 1 2 2\nDenseMatrix(1x7, double)\n0 0 nan 1 2 2 2\n</code></pre> </li> </ul>"},{"location":"DaphneDSL/Builtins/#measurements","title":"Measurements","text":"<ul> <li> <p><code>now</code><code>()</code></p> <p>Returns the current time since the epoch in nano seconds.</p> </li> </ul>"},{"location":"DaphneDSL/Builtins/#list-operations","title":"List operations","text":"<ul> <li> <p><code>createList</code><code>(elm:matrix, ...)</code></p> <p>Creates and returns a new list from the given elements <code>elm</code>. At least one element must be specified.</p> </li> <li> <p><code>length</code><code>(lst:list)</code></p> <p>Returns the number of elements in the given list <code>lst</code>.</p> </li> <li> <p><code>append</code><code>(lst:list, elm:matrix)</code></p> <p>Appends the given matrix <code>elm</code> to the given list <code>lst</code>. Returns the result as a new list (the argument list stays unchanged).</p> </li> <li> <p><code>remove</code><code>(lst:list, idx:size)</code></p> <p>Removes the element at position <code>idx</code> (counting starts at zero) from the given list <code>lst</code>. Returns (1) the result as a new list (the argument list stays unchanged), and (2) the removed element.</p> </li> </ul>"},{"location":"DaphneDSL/Imports/","title":"Imports","text":"<p>How to import functions from other Daphne scripts</p> <p>Example usage:</p> <pre><code>import \"bar.daphne\";\nimport \"foo.daphne\" as \"utils\";\n\nprint(bar.x);\nprint(utils.x);\n</code></pre> <p><code>UserConfig.json</code> now has a new field <code>daphnedsl_import_paths</code>, which maps e.g., library names to a list of paths, see example:</p> <pre><code>    \"daphnedsl_import_paths\": \n    {\n        \"default_dirs\": [\"test/api/cli/import/sandbox\", \"some/other/path\"],\n        \"algorithms\": [\"test/api/cli/import/sandbox/algos\"]\n    }\n</code></pre> <p>NOTE: <code>default_dirs</code> can hold many paths and it will look for the one specified file in each, whereas any other library names have a list consisting of one directory,  from which all files will be imported (can be easily extended to multiple directories).</p> <p>Example:</p> <pre><code>import \"a.daphne\";\nimport \"algorithms\";\n\nprint(a.x);\nprint(algorithms.kmeans.someVar);\n</code></pre> <p>The first import will first check if the relative path exists, then it will look for it relative to paths in <code>default_dirs</code>. If the specified file exists for more than one relative path, an error will be thrown. The second import goes to <code>algorithms</code> directory from <code>UserConfig</code> and imports all files from it.</p> <p>Paths from <code>UserConfig</code> get to <code>DaphneDSLVisitor</code> from <code>daphne.cpp</code> via <code>DaphneUserConfig</code>.</p> <p>Variable name collision resolution: Whenever we stumble upon equal prefixes (e.g., files with the same name in different directories), a parent directory of the file where conflict is detected is prepended before prefix.</p> <p>Example:</p> <pre><code>import \"somedir/a.daphne\";\nimport \"otherdir/a.daphne\";\n\nprint(a.x);\nprint(otherdir.a.x);\n</code></pre> <p>NOTE: the parent directory may be prepended even though you never specified it (e.g., the import script is in the same directory as the original script).</p> <p>Example:</p> <pre><code>import \"somedir/a.daphne\";\nimport \"a.daphne\";\n\nprint(a.x);\nprint(otherdir.a.x);\n</code></pre> <p>Libraries and aliases: Currently, the following example is allowed:</p> <pre><code>import \"algorithms\";\nimport \"sandbox/b.daphne\" as \"algorithms\"; \n\nprint(algorithms.x);\nprint(algorithms.kmeans1.someVar);\n</code></pre> <p>Even though both prefixes will begin with <code>algorithms.</code>, the entire library content's prefix is extended with filenames. It is up to user to not confuse yourself.</p> <p>Cascade imports: Any variables/functions imported into the script we are currently importing will be discarded. Example import scheme: <code>A&lt;-B&lt;-C</code>. A imports B, B imports C. B uses some vars/functions from C, but A doesn't \"see\" any of C's content.</p>"},{"location":"DaphneDSL/LanguageRef/","title":"Language Reference","text":"<p>DaphneDSL is DAPHNE's domain-specific language (DSL). DaphneDSL is written in plain text files, typically ending with <code>.daphne</code> or <code>.daph</code>. It is a case-sensitive language inspired by ML systems as well as languages and libraries for numerical computation like Julia, Python NumPy, R, and SystemDS DML. Its syntax is inspired by C/Java-like languages.</p>"},{"location":"DaphneDSL/LanguageRef/#hello-world","title":"Hello World","text":"<p>A simple hello-world script can look as follows:</p> <pre><code>print(\"hello world\");\n</code></pre> <p>Assuming this script is stored in the file <code>hello.daphne</code>, it can be executed by the following command:</p> <pre><code>bin/daphne hello.daphne\n</code></pre> <p>The remainder of this document discusses the language features of DaphneDSL in detail as they are right now, but note that DaphneDSL is still evolving.</p>"},{"location":"DaphneDSL/LanguageRef/#variables","title":"Variables","text":"<p>Variables are used to refer to values.</p> <p>Valid identifiers start with a letter (<code>a-z</code>, <code>A-Z</code>) or an underscore (<code>_</code>) that can be followed by any number of letters (<code>a-z</code>, <code>A-Z</code>), underscores (<code>_</code>), and decimal digits (<code>0-9</code>).</p> <p>The following reserved keywords must not be used as identifiers: <code>if</code>, <code>else</code>, <code>while</code>, <code>do</code>, <code>for</code>, <code>in</code>, <code>true</code>, <code>false</code>, <code>as</code>, <code>def</code>, <code>return</code>, <code>import</code>, <code>matrix</code>, <code>frame</code>, <code>scalar</code>, <code>list</code>, <code>f64</code>, <code>f32</code>, <code>si64</code>, <code>si8</code>, <code>ui64</code>, <code>ui32</code>, <code>ui8</code>, <code>str</code>, <code>nan</code>, and <code>inf</code>.</p> <p>Examples:</p> <pre><code>X\ny\n_hello123\na_long_Variable123_456NAME\n</code></pre> <p>Variables do not need to be (and cannot be) declared. Instead, simply assign a value to a variable and its type will be inferred. Variables must have been assigned to before they are used in an expression.</p>"},{"location":"DaphneDSL/LanguageRef/#types","title":"Types","text":"<p>DaphneDSL differentiates data types and value types.</p> <p>Currently, DaphneDSL supports the following abstract data types:</p> <ul> <li><code>matrix</code>: homogeneous value type for all cells</li> <li><code>frame</code>: a table with columns of potentially different value types</li> <li><code>scalar</code>: a single value</li> <li><code>list</code>: an ordered sequence of elements of homogeneous data/value type; currently, only matrices can be elements of lists</li> </ul> <p>Value types specify the representation of individual values. We currently support:</p> <ul> <li>floating-point numbers of various widths: <code>f64</code>, <code>f32</code></li> <li>signed and unsigned integers of various widths: <code>si64</code>, <code>si32</code>, <code>si8</code>, <code>ui64</code>, <code>ui32</code>, <code>ui8</code></li> <li>strings <code>str</code> (currently only for scalars, support for matrix elements is still experimental)</li> <li>booleans <code>bool</code> (currently only for scalars and dense matrices)</li> </ul> <p>Data types and value types can be combined, e.g.:</p> <ul> <li><code>matrix&lt;f64&gt;</code> is a matrix of double-precision floating point values</li> </ul>"},{"location":"DaphneDSL/LanguageRef/#comments","title":"Comments","text":"<p>DaphneDSL supports single-line comments (starting with <code>#</code> or <code>//</code>) and multi-line comments (everything enclosed in <code>/*</code> and <code>*/</code>).</p> <p>Examples:</p> <pre><code># this is a comment\nprint(\"Hello World!\"); // this is also a comment\n/* comments can\nspan multiple\nlines */\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#expressions","title":"Expressions","text":""},{"location":"DaphneDSL/LanguageRef/#simple-expressions","title":"Simple Expressions","text":"<p>Simple expressions constitute the basis of all expressions, and DaphneDSL offers three kinds:</p>"},{"location":"DaphneDSL/LanguageRef/#literals","title":"Literals","text":"<p>Literals represent hard-coded values and can be of various data and value types:</p>"},{"location":"DaphneDSL/LanguageRef/#scalar-literals","title":"Scalar literals","text":"<p>Integer literals are specified in decimal digits. By default, they have the type <code>si64</code>, but if the optional suffix <code>u</code> is appended, the type is <code>ui64</code>. They support the digit separators <code>'</code> and <code>_</code>.</p> <p>Examples: <code>0</code>, <code>123</code>, <code>-456</code>, <code>1_000</code>, <code>18446744073709551615u</code></p> <p>Floating-point literals are specified in decimal or scientific notation. The following literals stand for special floating-point values: <code>nan</code>, <code>inf</code>, <code>-inf</code>. By default, the type of a floating-point literal is <code>f64</code>, but if an <code>f</code> is appended to the literal, it is of type <code>f32</code> instead. Floating-point literals support the digit separators <code>'</code> and <code>_</code>.</p> <p>Examples: <code>0.0</code>, <code>123.0</code>, <code>-456.78f</code>, <code>inf</code>, <code>nan</code>, <code>1.2e5f</code>, <code>1E-14</code></p> <p>Boolean literals can be <code>false</code> and <code>true</code>.</p> <p>String literals are enclosed in quotation marks <code>\"</code>. Special characters must be escaped using a backslash:</p> <ul> <li><code>\\n</code>: new line</li> <li><code>\\t</code>: tab</li> <li><code>\\\"</code>: quotation mark</li> <li><code>\\\\</code>: backslash</li> <li><code>\\b</code>: backspace</li> <li><code>\\f</code>: line feed</li> <li><code>\\r</code>: carriage return</li> </ul> <p>Examples:</p> <pre><code>\"Hello World!\"\n\"line 1\\nline 2\\nline 3\"\n\"This is \\\"hello.daphne\\\".\"\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#matrix-literals","title":"Matrix literals","text":"<p>A matrix literal consists of a comma-separated list of elements enclosed in square brackets, optionally followed by parentheses with a comma separated pair of dimensions (number of rows and/or columns). The elements and the dimensions can be complex expressions. The matrix's value type is the most general value type among its elements. Matrix literals are by default column matrices if no dimensions are specified. If only one dimension is given, the other one will be inferred automatically. Note that the built-in function <code>reshape</code> can also be used to modify the shape of matrices.</p> <p>Examples:</p> <pre><code>[1, 0, -4.0]                    # matrix&lt;f64&gt; with shape (3 x 1)\n[1, 2, sqrt(3), 4](1,)          # matrix&lt;f64&gt; with shape (1 x 4)\n[1, 2, 3, sum([4, 5])](2, 2)    # matrix&lt;si64&gt; with shape (2 x 2)\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#frame-literals","title":"Frame literals","text":"<p>Frame literals can be defined using either columns or rows and are enclosed in curly brackets.</p> <p>Frame literals of columns consist of comma-separated pairs of a <code>str</code>-typed label and a (n x 1) matrix, in the format <code>label: column</code>. Both labels and columns can be complex expressions.</p> <p>Examples:</p> <pre><code># frame with two columns: \"col 1\" of type f64 and \"col 2\" of type si64\n{\"col 1\": [1, 2, sqrt(3)], \"col 2\": seq(1, 3, 1)}\n# frame with two columns: \"col 1\" of type si64 and \"col 2\" of type f64\n{\"col 1\": [1, 2, 3], \"col 2\": [1.1, -2.2, 3.3]}\n</code></pre> <p>Frame literals of rows consist of comma-separated rows, whereby each row is enclosed in square brackets. The first row contains the <code>str</code>-typed column labels. All remaining rows represent the data of the frame. Both labels and elements of a row can be complex expressions. However, it is currently not possible to specify an entire row by an expression.</p> <p>Examples:</p> <pre><code># equivalent to the 2nd example above, specified by rows\n{[\"col 1\", \"col 2\"], [1, 1.1], [2, -2.2], [3, 3.3]}\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#variable-expressions","title":"Variable Expressions","text":"<p>Variables are referenced by their name.</p> <p>Examples:</p> <pre><code>x\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#script-arguments","title":"Script arguments","text":"<p>Script arguments are named literals that can be passed to a DaphneDSL script. They are referenced by a dollar sign <code>$</code> followed by the argument's name.</p> <p>Examples:</p> <p><code>bin/daphne my_script.daphne a=12 b=-2 c=false d=true e=2.0 f=-1.1 g=\\\"string\\\" h=\"\\\"white spaces\\\"\"</code></p> <pre><code>print($g);\nmyVar = $a + $b;\n</code></pre> <p>Note that matrix literals are not supported as script arguments yet. Check out Running DAPHNE Locally for more information.</p>"},{"location":"DaphneDSL/LanguageRef/#complex-expressions","title":"Complex Expressions","text":"<p>DaphneDSL offers several ways to build more complex expressions.</p>"},{"location":"DaphneDSL/LanguageRef/#operators","title":"Operators","text":"<p>DaphneDSL currently supports the following binary operators:</p> Operator Meaning <code>-</code>, <code>+</code> additive inverse (unary operators) <code>@</code> matrix multiplication (highest precedence) <code>^</code> exponentiation <code>%</code> modulo <code>*</code>, <code>/</code> multiplication, division <code>+</code>, <code>-</code> addition/string concatenation, subtraction <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code> comparison <code>&amp;&amp;</code> logical AND <code>\\|\\|</code> logical OR (lowest precedence) <p>We plan to add more unary and binary operators in the future.</p> <p>Matrix multiplication (<code>@</code>): The inputs must be matrices of compatible shapes, and the output is always a matrix.</p> <p>All other binary operators: The following table shows which combinations of inputs are allowed and which result they yield:</p> Left input Right input Result Details scalar scalar scalar binary operation of two scalars matrix (n x m) scalar matrix (n x m) element-wise operation of each value with scalar scalar matrix (n x m) matrix (n x m) element-wise operation of scalar with each value (*) matrix (n x m) matrix (n x m) matrix (n x m) element-wise operation on corresponding values matrix (n x m) matrix (1 x m) matrix (n x m) broadcasting of row-vector matrix (n x m) matrix (n x 1) matrix (n x m) broadcasting of column-vector <p>(*) Scalar-<code>op</code>-matrix operations are so far only supported for <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>; for <code>/</code> only if the matrix is of a floating-point value type.</p> <p>In the future, we will fully support scalar-<code>op</code>-matrix operations as well as row/column-matrices as the left-hand-side operands.</p> <p>Examples:</p> <pre><code>1.5 * X @ y + 0.001\nx == 1 &amp;&amp; y &lt; 3.5\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#parentheses","title":"Parentheses","text":"<p>Parentheses can be used to manually control operator precedence.</p> <p>Examples:</p> <pre><code>1 * (2 + 3)\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#indexing","title":"Indexing","text":"<p>(Right) indexing enables the extraction of a part of the rows and/or columns of a data object (matrix/frame) into a new data object. The result is always a data object of the same data type as the input (even 1 x 1 results need to be casted to scalars explicitly, if needed).</p> <p>The rows and columns to extract can be specified independently in any of the following ways:</p>"},{"location":"DaphneDSL/LanguageRef/#omit-indexing","title":"Omit indexing","text":"<p>Omitting the specification of rows/columns means extracting all rows/columns.</p> <p>Examples:</p> <pre><code>X[, ] # same as X (all rows and columns)\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#indexing-by-position","title":"Indexing by position","text":"<p>This is supported for addressing rows and columns in matrices and frames.</p> <ul> <li>Single row/column position:   Extracts only the specified row/column.</li> </ul> <p>Examples:</p> <pre><code>X[2, 3] # extracts the cell in row 2, column 3 as a 1 x 1 matrix\n</code></pre> <ul> <li>Row/column range:   Extracts all rows/columns between a lower bound (inclusive) and an upper bound (exclusive).   The lower and upper bounds can be omitted independently of each other.   In that case, they are replaced by zero and the number of rows/columns, respectively.</li> </ul> <p>Examples:</p> <pre><code>X[2:5, 3] # extracts rows 2, 3, 4 of column 3\nX[2, 3:]  # extracts row 2 of all columns from column 3 onward\nX[:5, 3]  # extracts rows 0, 1, 2, 3, 4 of column 3\nX[:, 3]   # extracts all rows of column 3, same as X[, 3]\n</code></pre> <ul> <li>Arbitrary sequence of row/column positions:   Expects a sequence of row/column positions as a column (n x 1) matrix.   There are no restrictions on these positions, except that they must be in bounds.   In particular, they do not need to be contiguous, sorted, or unique.</li> </ul> <p>Examples:</p> <pre><code>X[ [5, 1, 3], ] # extracts rows 5, 1, and 3\nX[, [2, 2, 2] ] # extracts column 2 three times\n</code></pre> <p>Note that, when using matrix literals to specify the positions, a space must be left between the opening/closing bracket <code>[</code>/<code>]</code> of the indexing and that of the matrix literal, in order to avoid confusion with the indexing by bit vector.</p> <p>A few remarks on positions:</p> <ul> <li>Counting starts at zero.   For instance, a 5 x 3 matrix has row positions 0, 1, 2, 3, and 4, and column positions 0, 1, and 2.</li> <li>They must be non-negative.</li> <li>They can be provided as integers or floating-point numbers (the latter are rounded down to integers).</li> <li>They can be given as literals or as any expression evaluating to a suitable value.</li> </ul> <p>Examples:</p> <pre><code>X[1.2, ]              # same as X[1, ]\nX[1.9, ]              # same as X[1, ]\nX[i, (j + 2*sum(Y)):] # expressions\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#indexing-by-label","title":"Indexing by label","text":"<p>So far, this is only supported for addressing columns of frames.</p> <ul> <li>Single column label:   Extracts only the column with the given label.</li> </ul> <p>Examples:</p> <pre><code>X[, \"revenue\"]        # extracts the column labeled \"revenue\"\nX[100:200, \"revenue\"] # extracts rows 100 through 199 of the column labeled \"revenue\"\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#indexing-by-bit-vector","title":"Indexing by bit vector","text":"<p>This is not supported for addressing columns of frames yet.</p> <p>For each row/column, a single zero/one entry (\"bit\") must be provided. More precisely, a (n x 1) matrix is required on data objects with n rows, and a (m x 1) matrix is required on data objects with m columns. Only the rows/columns with a corresponding 1-value in the bit vector are present in the result.</p> <p>Note that double square brackets (<code>[[...]]</code>) must be used to distinguish indexing by bit vector from indexing by an arbitrary sequence of positions.</p> <p>Examples:</p> <pre><code># Assume X is a 4x3 matrix.\nX[[[0, 1, 1, 0], ]]           # extracts rows 1 and 2\n                              # same as X[[1, 2], ]\nX[[, [1, 0, 1] ]]             # extracts columns 0 and 2\n                              # same as X[, [0, 2]]\nX[[[0, 1, 1, 0], [1, 0, 1] ]] # extracts columns 0 and 2 of rows 1 and 2\n                              # same as X[[1, 2], [0, 2]]\n</code></pre> <p>Note that, when using a matrix literal to provide the column bit vector, there must be a space between the closing bracket <code>]</code> of the matrix literal and the closing double bracket <code>]]</code> of the indexing expression, e.g., <code>X[[, [0] ]]</code> instead of <code>X[[, [0]]]</code>.</p>"},{"location":"DaphneDSL/LanguageRef/#casts","title":"Casts","text":"<p>Values can be casted to a particular type explicitly. Currently, it is possible to cast:</p> <ul> <li>between scalars of different types</li> <li>between matrices of different value types</li> <li>between matrix and frame</li> <li>between scalar and 1x1 matrix/frame</li> </ul> <p>Casts can either fully specify the target data and value type, or specify only the target data type or the target value type. In the latter case, the unspecified part of the type will be retained from the argument.</p> <p>Examples:</p> <pre><code>as.scalar&lt;f64&gt;(x)  # casts x to f64 scalar\nas.matrix&lt;ui32&gt;(x) # casts x to a matrix of ui32\n\nas.scalar(x) # casts x to a scalar of the same value type as x\nas.matrix(x) # casts x to a matrix of the same value type as x\nas.frame(x)  # casts x to a frame whose column types are the value type of x\n\nas.f32(x) # casts x to the same data type as x, but with value type f32\nas.ui8(x) # casts x to the same data type as x, but with value type ui8\n</code></pre> <p>Note that casting to frames does not support changing the value/column type yet, i.e., expressions like <code>as.frame&lt;f64, si32, f32&gt;(x)</code> and <code>as.f64(x)</code> (on a frame <code>x</code>) do not work yet.</p>"},{"location":"DaphneDSL/LanguageRef/#function-calls","title":"Function calls","text":"<p>Function calls can address built-in functions as well as user-defined functions, but the syntax is the same in both cases: The name of the function followed by a comma-separated list of positional parameters in parentheses.</p> <p>Examples:</p> <pre><code>print(\"hello\");\nt(myMatrix);\nseq(0, 10, 2);\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#conditional-expression","title":"Conditional expression","text":"<p>DaphneDSL supports the conditional expression with the general syntax:</p> <pre><code>condition ? then-value : else-value\n</code></pre> <p>The condition can be either a scalar or a matrix.</p> <ul> <li>Condition is a scalar:   If the condition is <code>true</code> (when casted to boolean), then the result is the <code>then-value</code>.   Otherwise, the result is the <code>else-value</code>.   The <code>then-value</code> and the <code>else-value</code> must have the same type.</li> <li>Condition is a matrix (elementwise application):   In this case, the condition matrix can be of any value type, but must only contain 0 or 1 values of that type (for all other values, the behavior is unspecified).   The <code>then-value</code> and <code>else-value</code> must be matrices of the same shape as the condition and must have the same value type as each other.   The <code>?:</code>-operator is applied in an elementwise fashion, i.e., individually for each triple of corresponding elements in condition/<code>then-value</code>/<code>else-value</code>.   The <code>then-value</code> and <code>else-value</code> may also be scalars, in which case they are treated like matrices with a constant value.   The result is a matrix of the same shape as the condition and the same value type as the <code>then-value</code>/<code>else-value</code>.</li> </ul> <p>Examples:</p> <pre><code>(i &gt; 5) ? 42.0 : -42.0                      # 42.0 if i &gt; 5, -42.0 otherwise\n[1, 0, 0, 1] ? [1.0, 2.0, 3.0, 4.0] : 99.9  # [1.0, 99.9, 99.9, 4.0]\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#statements","title":"Statements","text":"<p>At the highest level, a DaphneDSL script is a sequence of statements. Statements comprise assignments, various forms of control flow, and declarations of user-defined functions.</p>"},{"location":"DaphneDSL/LanguageRef/#expression-statement","title":"Expression statement","text":"<p>Every expression followed by a semicolon <code>;</code> can be used as a statement. This is useful for expressions (especially function calls) which do not return a value. Nevertheless, it can also be used for expressions with one or more return values, in which case these values are ignored.</p> <p>Examples:</p> <pre><code>print(\"hello\"); # built-in function without return value\n1 + 2;          # value is ignored, useless but possible\ndoSomething();  # possible return values are ignored, but the execution \n                # of the user-defined function could have side effects\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#assignment-statement","title":"Assignment statement","text":"<p>The return value(s) of an expression can be assigned to one (or more) variable(s).</p> <p>Single-assignments are used for expressions with exactly one return value.</p> <p>Examples:</p> <pre><code>x = 1 + 2;\n</code></pre> <p>Multi-assignments are used for expressions with more than one return value.</p> <p>Examples:</p> <pre><code>evals, evecs = eigen(A); # eigen() returns two values, the (n x 1)-matrix of\n                         # eigen-values and the (n x n)-matrix of eigen-vectors\n                         # of the input matrix A.\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#indexing_1","title":"Indexing","text":"<p>The value of an expression can also be assigned to a partition of an existing data object. This is done by (left) indexing, whose syntax is similar to (right) indexing in expressions.</p> <p>Currently, left indexing is supported only for matrices. Furthermore, the rows/columns cannot be addressed by arbitrary positions lists or bit vectors (yet).</p> <p>Examples:</p> <pre><code>X[5, 2]       = [123];            # insert (1 x 1)-matrix\nX[10:20, 2:5] = fill(123, 10, 3); # insert (10 x 3)-matrix\n</code></pre> <p>The following conditions must be fulfilled:</p> <ul> <li>The left-hand-side variable must have been initialized.</li> <li>The left-hand-side variable must be of data type matrix.</li> <li>The right-hand-side expression must return a matrix.</li> <li>The shapes of the partition addressed on the left-hand side and the return value of the right-hand-side expression must match.</li> <li>The value type of the left-hand-side and right-hand-side matrices must match.</li> </ul> <p>Left indexing can be used with both single and multi-assignments. With the latter, it can be used with each variable on the left-hand side individually and independently.</p> <p>Examples:</p> <pre><code>x, Y[3, :], Z = calculateSomething();\n</code></pre> <p>Copy-on-write semantics</p> <p>Left indexing enables the modification of existing data objects, whereby the semantics is copy-on-write. That is, if two different variables represent the same runtime data object, then left indexing on one of these variables does not have any effects on the other one. This is achieved by transparently copying the data as necessary.</p> <p>Examples:</p> <pre><code>A = ...;           # some matrix\nB = A;             # copy-by-reference\nB[..., ...] = ...; # copy-on-write: changes B, but no effect on A\nA[..., ...] = ...; # copy-on-write: changes A, but no effect on B\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#control-flow-statements","title":"Control Flow statements","text":"<p>DaphneDSL supports block statements, conditional branching, and various kinds of loops. These control flow constructs can be nested arbitrarily. Besides that, the <code>stop()</code> built-in function can be used to terminate the DaphneDSL script execution.</p>"},{"location":"DaphneDSL/LanguageRef/#block-statement","title":"Block statement","text":"<p>A block statement allows to view an enclosed sequence of statements like a single statement. This is very useful in combination with the control flow statements described below. Besides that, a block statement starts a new scope in terms of visibility of variables. Within a block, all variables from outside the block can be read and written. However, variables created inside a block are not visible anymore after the block.</p> <p>The syntax of a block statement is:</p> <pre><code>{\n    statement1\n    statement2\n    ...\n}\n</code></pre> <p>Examples:</p> <pre><code>x = 1;\n{\n    print(x); # read access\n    x = 2;    # write access\n    y = 1;    # variable created inside the block\n}\nprint(x);     # prints 2\nprint(y);     # error\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#if-then-else","title":"If-then-else","text":"<p>The syntax of an if-then-else statement is as follows:</p> <pre><code>if (condition)\n    then-statement\nelse\n    else-statement\n</code></pre> <p>condition is an expression returning a single value. If this value is <code>true</code> (when casted to value type <code>bool</code>, if necessary), the then-statement is executed. Otherwise, the else-statement is executed, if it is present. Note that the else-branch (keyword and statement) may be omitted. Furthermore, then-statement and else-statement can be block statements, to allow any number of statements in the then and else-branches.</p> <p>Examples:</p> <pre><code>if (sum(X) == 0)\n    X = X + 1;\n</code></pre> <pre><code>if (2 * x &gt; y) {\n    z = z / 2;\n    a = true;\n}\nelse\n    z = z * 2;\n</code></pre> <pre><code>if (a)\n    print(\"a\");\nelse if (b)\n    print(\"not a, but b\");\nelse\n    print(\"neither a nor b\");\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#loops","title":"Loops","text":"<p>DaphneDSL supports for-loops, while-loops, and do-while-loops. In the future we plan to support also parfor-loops as well as <code>break</code> and <code>continue</code> statements.</p>"},{"location":"DaphneDSL/LanguageRef/#for-loops","title":"For-Loops","text":"<p>For-loops are used to iterate over the elements of a sequence of integers. The syntax of a for-loop is as follows:</p> <pre><code>for (var in start:end[:step])\n    body-statement\n</code></pre> <p>var must be a valid identifier and is assigned the values from start to end in increments of step. start, end, and step are expressions evaluating to a single number. step is optional and defaults to 1 if end is greater than start, or -1 otherwise. In that sense, for-loops can also be used to count backwards by setting start greater than end. The body-statement is executed for each value in the sequence, and within the body-statement, this value is accessible via the read-only variable <code>var</code>. Note that the body-statement may be a block statement enclosing an arbitrary number of statements.</p> <p>Examples:</p> <pre><code>for(i in 1:3)\n    print(i); # 1 2 3\n</code></pre> <pre><code>x = 0; y = 0;\nfor(i in 10:1:-3) {\n    x = x + i;\n    y = y + 1;\n}\nprint(x); # 22\nprint(y); #  4\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#while-loops","title":"While-Loops","text":"<p>While loops are used to execute a (block of) statement(s) as long as an arbitrary condition holds true. The syntax of a while-loop is as follows:</p> <pre><code>while (condition)\n    body-statement\n</code></pre> <p>condition is an expression returning a single value, and is evaluated before each iteration. If this value is <code>true</code> (when casted to value type <code>bool</code>, if necessary), the body-statement is executed, and the loop starts anew. Otherwise, the program continues after the loop. Note that the body-statement may be a block statement enclosing an arbitrary number of statements.</p> <p>Examples:</p> <pre><code>i = 0;\nwhile(i &lt; 10 &amp;&amp; !converged) {\n    A = A @ B;\n    converged = sum(A);\n    i = i + 1;\n}\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#do-while-loops","title":"Do-While-Loops","text":"<p>Do-while-loops are a variant of while-loops, which checks the condition after each iteration. Consequently, a do-while-loop always executes at least one iteration. The syntax of a do-while-loop is as follows:</p> <pre><code>do\n    body-statement\nwhile (condition);\n</code></pre> <p>The semicolon at the end is optional. Note that the body-statement may be a block statement enclosing an arbitrary number of statements.</p> <p>Examples:</p> <pre><code>i = 5;\ndo {\n    A = sqrt(A);\n    i = i - 1;\n} while (mean(A) &gt; 100 &amp;&amp; i &gt; 0);\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#user-defined-functions-udfs","title":"User-defined Functions (UDFs)","text":"<p>DaphneDSL allows users to define their own functions. The syntax of a function definition looks as follows:</p> <pre><code>def funcName(paramName1[:paramType1], paramName2[:paramType2], ...) [-&gt; returnType1, returnType2, ...] {\n    statement1\n    statement2\n    ...\n}\n</code></pre> <p>The function name must be a valid and unique identifier. A function can have zero, one, or more parameters, and their names must be valid and unique identifiers. Furthermore, a function may return zero, one, or more values. The types of parameters are optional and can be provided or omitted for each parameter individually. The types of the return values are optional, but if omitted, exactly one return value is implicitly assumed. Functions with multiple return values must specify the types of all return values. See also typed and untyped functions below. The body of a function definition is always a block statement, i.e., it must be enclosed in curly braces <code>{}</code> even if it is just a single statement.</p> <p>So far, DaphneDSL supports only positional parameters to functions, but in the future, we plan to support named keyword arguments as well.</p> <p>Functions must be defined in the top-level scope of a DaphneDSL script, i.e., a function definition must not be nested within a control-flow statement or within another function definition.</p>"},{"location":"DaphneDSL/LanguageRef/#returning-values","title":"Returning Values","text":"<p>User-defined functions can return zero, one, or more (comma-separated) values using the <code>return</code>-statement. The number of returned values must match the function signature.</p> <p>Examples: <pre><code>return;       # don't return any values\nreturn x;     # return exactly one value\nreturn x, y;  # return two values\n</code></pre></p> <p>Currently, the return statement must be the last statement of a function. Alternatively, it can be nested into if-then-else (early return), as long as it is ensured that there is exactly one return statement at the end of each path through the function (experimental).</p> <p>Examples:</p> <p><pre><code>def fib(n: si64) -&gt; si64 {\n    if (n &lt;= 0)\n        return 0;\n    if (n &lt;= 1)\n        return 1;\n    return fib(n - 1) + fib(n - 2);\n}\n</code></pre> <pre><code>def nextTwo(a: si64) -&gt; si64, si64 {\n    return a + 1, a + 2;\n}\n</code></pre></p>"},{"location":"DaphneDSL/LanguageRef/#calling-user-defined-functions","title":"Calling User-defined Functions","text":"<p>A user-defined function can be called like any other (built-in) function (see function-call expressions above).</p> <p>Examples:</p> <pre><code>x = 2 * fib(5) + 123;\ny, z = nextTwo(123);\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#typed-and-untyped-functions-experimental","title":"Typed and Untyped Functions (experimental)","text":"<p>DaphneDSL supports both typed and untyped functions.</p> <p>The definition of a typed function specifies the data and value types of all parameters and return values. Hence, a typed function can only be called with inputs of the specified types (if a provided input has an unexpected type, it is automatically casted to the expected type, if possible), and always returns outputs of the specified types. A typed function is compiled exactly once and specialized to the specified parameter and return types.</p> <p>In contrast to that, the definition of an untyped function leaves the data and value type, or just the value type, of one or more parameters and/or return values unspecified. At call sites, a value of any type, or any value type, can be passed to an untyped parameter. As a consequence, an untyped function is compiled and specialized on demand according to the types at a call site. Consistently, the types of untyped return values are infered from the parameter types and operations.</p>"},{"location":"DaphneDSL/LanguageRef/#compiler-hints","title":"Compiler Hints","text":"<p>One of DAPHNE's strengths is its (WIP) ability to make various decisions on its own, e.g., regarding physical data representation (such as dense/sparse), physical operators (kernels), and data/operator placement (such as local/distributed, CPU/GPU/FPGA, computational storage). However, expert users may optionally provide hints to influence compiler decisions. This feature is useful for experimentation and in the context of DAPHNE's extensibility. For instance, a user could force the use of a certain custom kernel at a certain point in a larger DaphneDSL script to measure the impact of that custom kernel, even if the DAPHNE compiler would normally not choose that kernel in that situation.</p> <p>The support for compiler hints is still experimental and it is currently not guaranteed that the DAPHNE compiler respects these hints.</p>"},{"location":"DaphneDSL/LanguageRef/#kernel-hints","title":"Kernel Hints","text":"<p>Users can provide hints on the physical kernel that should be used for a specific occurrence of a DaphneDSL operation. So far, kernel hints are only supported for DaphneDSL built-in functions. Here, the name of the pre-compiled kernel function can optionally be attached to the name of the built-in function, separated by <code>::</code>.</p> <p>Examples:</p> <pre><code>res = sum::my_custom_sum_kernel(X);\n</code></pre>"},{"location":"DaphneDSL/LanguageRef/#example-scripts","title":"Example Scripts","text":"<p>A few example DaphneDSL scripts can be found in:</p> <ul> <li>scripts/algorithms/</li> <li>scripts/examples/</li> <li>test/api/cli/algorithms/</li> </ul>"},{"location":"DaphneLib/APIRef/","title":"API Reference","text":"<p>This document is a hand-crafted reference of the DaphneLib API. A general introduction to DaphneLib (DAPHNE's Python API) can be found in a separate document. DaphneLib offers numerous methods for obtaining DAPHNE matrices and frames as well as for building complex computations based on them, including complex control flow with if-then-else, loops, and user-defined functions. Ultimately, DaphneLib will support all DaphneDSL built-in functions on matrices and frames. Futhermore, we also plan to create a library of higher-level primitives allowing users to productively implement integrated data analysis pipelines at a much higher level of abstraction.</p> <p>At the moment, the documentation is still rather incomplete. However, as the methods largely map to DaphneDSL built-in functions, you can find some more information in the List of DaphneDSL built-in functions, for the time being.</p>"},{"location":"DaphneLib/APIRef/#obtaining-daphne-matrices-and-frames","title":"Obtaining DAPHNE Matrices and Frames","text":""},{"location":"DaphneLib/APIRef/#daphnecontext-api-reference","title":"<code>DaphneContext</code> API Reference","text":"<p>Importing data from other Python libraries:</p> <ul> <li><code>from_numpy</code><code>(mat: np.array, shared_memory=True, verbose=False, return_shape=False) -&gt; Matrix</code></li> <li><code>from_pandas</code><code>(df: pd.DataFrame, shared_memory=True, verbose=False, keepIndex=False) -&gt; Frame</code></li> <li><code>from_tensorflow</code><code>(tensor: tf.Tensor, shared_memory=True, verbose=False, return_shape=False) -&gt; Matrix</code></li> <li><code>from_pytorch</code><code>(tensor: torch.Tensor, shared_memory=True, verbose=False, return_shape=False) -&gt; Matrix</code></li> </ul> <p>Generating data in DAPHNE:</p> <ul> <li><code>fill</code><code>(arg, rows:int, cols:int) -&gt; Matrix</code></li> <li><code>seq</code><code>(start, end, inc = 1) -&gt; Matrix</code></li> <li><code>rand</code><code>(rows: int, cols: int, min: Union[float, int] = None, max: Union[float, int] = None, sparsity: Union[float, int] = 0, seed: Union[float, int] = 0) -&gt; Matrix</code></li> <li><code>createFrame</code><code>(columns: List[Matrix], labels: List[str] = None) -&gt; 'Frame'</code></li> <li><code>diagMatrix</code><code>(self, arg: Matrix) -&gt; 'Matrix'</code></li> <li><code>sample</code><code>(range, size, withReplacement: bool, seed = -1) -&gt; 'Matrix'</code></li> </ul> <p>Reading files using DAPHNE's readers:</p> <ul> <li><code>readMatrix</code><code>(file:str) -&gt; Matrix</code></li> <li><code>readFrame</code><code>(file:str) -&gt; Frame</code></li> </ul> <p>Extended relational algebra:</p> <ul> <li><code>sql</code><code>(query) -&gt; Frame</code></li> </ul>"},{"location":"DaphneLib/APIRef/#building-complex-computations","title":"Building Complex Computations","text":"<p>Complex computations can be built using Python operators (see DaphneLib) and using DAPHNE matrix/frame/scalar methods. In the following, we describe only the latter.</p>"},{"location":"DaphneLib/APIRef/#matrix-api-reference","title":"<code>Matrix</code> API Reference","text":"<p>Matrix meta data:</p> <ul> <li><code>ncol</code><code>()</code></li> <li><code>nrow</code><code>()</code></li> <li><code>ncell</code><code>()</code></li> </ul> <p>Elementwise unary:</p> <ul> <li><code>abs</code><code>()</code></li> <li><code>sign</code><code>()</code></li> <li><code>exp</code><code>()</code></li> <li><code>ln</code><code>()</code></li> <li><code>sqrt</code><code>()</code></li> <li><code>round</code><code>()</code></li> <li><code>floor</code><code>()</code></li> <li><code>ceil</code><code>()</code></li> <li><code>sin</code><code>()</code></li> <li><code>cos</code><code>()</code></li> <li><code>tan</code><code>()</code></li> <li><code>asin</code><code>()</code></li> <li><code>acos</code><code>()</code></li> <li><code>atan</code><code>()</code></li> <li><code>sinh</code><code>()</code></li> <li><code>cosh</code><code>()</code></li> <li><code>tanh</code><code>()</code></li> <li><code>isNan</code><code>()</code></li> </ul> <p>Elementwise binary:</p> <ul> <li><code>pow</code><code>(other: 'Matrix')</code></li> <li><code>log</code><code>(other: 'Matrix')</code></li> <li><code>mod</code><code>(other: 'Matrix')</code></li> <li><code>max</code><code>(other: 'Matrix')</code></li> <li><code>min</code><code>(other: 'Matrix')</code></li> </ul> <p>Outer binary:</p> <ul> <li><code>outerAdd</code><code>(other: 'Matrix')</code></li> <li><code>outerSub</code><code>(other: 'Matrix')</code></li> <li><code>outerMul</code><code>(other: 'Matrix')</code></li> <li><code>outerDiv</code><code>(other: 'Matrix')</code></li> <li><code>outerPow</code><code>(other: 'Matrix')</code></li> <li><code>outerLog</code><code>(other: 'Matrix')</code></li> <li><code>outerMod</code><code>(other: 'Matrix')</code></li> <li><code>outerMin</code><code>(other: 'Matrix')</code></li> <li><code>outerMax</code><code>(other: 'Matrix')</code></li> <li><code>outerAnd</code><code>(other: 'Matrix')</code></li> <li><code>outerOr</code><code>(other: 'Matrix')</code></li> <li><code>outerXor</code><code>(other: 'Matrix')</code> (not supported yet)</li> <li><code>outerConcat</code><code>(other: 'Matrix')</code> (not supported yet)</li> <li><code>outerEq</code><code>(other: 'Matrix')</code></li> <li><code>outerNeq</code><code>(other: 'Matrix')</code></li> <li><code>outerLt</code><code>(other: 'Matrix')</code></li> <li><code>outerLe</code><code>(other: 'Matrix')</code></li> <li><code>outerGt</code><code>(other: 'Matrix')</code></li> <li><code>outerGe</code><code>(other: 'Matrix')</code></li> </ul> <p>Aggregation:</p> <ul> <li><code>sum</code><code>(axis: int = None)</code></li> <li><code>aggMin</code><code>(axis: int = None)</code></li> <li><code>aggMax</code><code>(axis: int = None)</code></li> <li><code>mean</code><code>(axis: int = None)</code></li> <li><code>var</code><code>(axis: int = None)</code></li> <li><code>stddev</code><code>(axis: int = None)</code></li> <li><code>idxMin</code><code>(axis: int)</code></li> <li><code>idxMax</code><code>(axis: int)</code></li> </ul> <p>Cumulative aggregation</p> <ul> <li><code>cumSum</code><code>()</code></li> <li><code>cumProd</code><code>()</code></li> <li><code>cumMin</code><code>()</code></li> <li><code>cumMax</code><code>()</code></li> </ul> <p>Reorganization:</p> <ul> <li><code>t</code><code>()</code></li> <li><code>reshape</code><code>(numRows: int, numCols: int)</code></li> <li><code>cbind</code><code>(other: Matrix)</code></li> <li><code>rbind</code><code>(other: Matrix)</code></li> <li><code>reverse</code><code>()</code></li> <li><code>lowerTri</code><code>(diag: bool, values: bool)</code></li> <li><code>upperTri</code><code>(diag: bool, values: bool)</code></li> <li><code>replace</code><code>(pattern, replacement)</code></li> <li><code>order</code><code>(colIdxs: List[int], ascs: List[bool], returnIndexes: bool)</code></li> </ul> <p>Data preprocessing:</p> <ul> <li><code>oneHot</code><code>(info:matrix)</code></li> <li><code>bin</code><code>(numBins:int, Min = None, Max = None)</code></li> </ul> <p>Other matrix operations:</p> <ul> <li><code>diagVector</code><code>()</code></li> <li><code>solve</code><code>(other: 'Matrix')</code></li> </ul> <p>Input/output:</p> <ul> <li><code>print</code><code>()</code></li> <li><code>write</code><code>(file: str)</code></li> </ul> <p>Conversions and casts:</p> <ul> <li><code>asType</code><code>(dtype=None, vtype=None) -&gt; Matrix</code></li> </ul> <p>Conditional:</p> <ul> <li><code>ifElse</code><code>(thenVal: Union['Matrix', 'Scalar'], elseVal: Union['Matrix', 'Scalar'])</code></li> </ul>"},{"location":"DaphneLib/APIRef/#frame-api-reference","title":"<code>Frame</code> API Reference","text":"<p>Frame meta data:</p> <ul> <li><code>nrow</code><code>()</code></li> <li><code>ncol</code><code>()</code></li> <li><code>ncell</code><code>()</code></li> </ul> <p>Frame label manipulation:</p> <ul> <li><code>setColLabels</code><code>(labels)</code></li> <li><code>setColLabelsPrefix</code><code>(prefix)</code></li> </ul> <p>Reorganization:</p> <ul> <li><code>cbind</code><code>(other)</code></li> <li><code>rbind</code><code>(other)</code></li> <li><code>order</code><code>(colIdxs: List[int], ascs: List[bool], returnIndexes: bool)</code></li> </ul> <p>Extended relational algebra:</p> <ul> <li><code>registerView</code><code>(table_name: str)</code></li> <li><code>cartesian</code><code>(other)</code></li> <li><code>innerJoin</code><code>(right_frame, left_on, right_on)</code></li> </ul> <p>Input/output:</p> <ul> <li><code>print</code><code>()</code></li> <li><code>write</code><code>(file: str)</code></li> </ul> <p>Conversions and casts:</p> <ul> <li><code>toMatrix</code><code>(value_type=\"f64\") -&gt; Matrix</code></li> </ul>"},{"location":"DaphneLib/APIRef/#scalar-api-reference","title":"<code>Scalar</code> API Reference","text":"<p>Elementwise unary:</p> <ul> <li><code>abs</code><code>()</code></li> <li><code>sign</code><code>()</code></li> <li><code>exp</code><code>()</code></li> <li><code>ln</code><code>()</code></li> <li><code>sqrt</code><code>()</code></li> <li><code>round</code><code>()</code></li> <li><code>floor</code><code>()</code></li> <li><code>ceil</code><code>()</code></li> <li><code>sin</code><code>()</code></li> <li><code>cos</code><code>()</code></li> <li><code>tan</code><code>()</code></li> <li><code>asin</code><code>()</code></li> <li><code>acos</code><code>()</code></li> <li><code>atan</code><code>()</code></li> <li><code>sinh</code><code>()</code></li> <li><code>cosh</code><code>()</code></li> <li><code>tanh</code><code>()</code></li> <li><code>isNan</code><code>()</code></li> </ul> <p>Elementwise binary:</p> <ul> <li><code>pow</code><code>(other)</code></li> <li><code>log</code><code>(other)</code></li> <li><code>mod</code><code>(other)</code></li> <li><code>min</code><code>(other)</code></li> <li><code>max</code><code>(other)</code></li> </ul> <p>Input/output:</p> <ul> <li><code>print</code><code>()</code></li> </ul>"},{"location":"DaphneLib/APIRef/#daphnecontext-api-reference_1","title":"<code>DaphneContext</code> API Reference","text":"<p>Logical operators</p> <p>Logical and (<code>&amp;&amp;</code>) and or (<code>||</code>) operators can be used for the conditions for while-loops and do-while-loops as well as for the predicates for if-then-else statements. Note that these logical operators may be provided in another way than via the <code>DaphneContext</code> in the future.</p> <ul> <li><code>logical_and</code><code>(left_operand: Scalar, right_operand: Scalar) -&gt; Scalar</code></li> <li><code>logical_or</code><code>(left_operand: Scalar, right_operand: Scalar) -&gt; Scalar</code></li> </ul>"},{"location":"DaphneLib/APIRef/#building-complex-control-structures","title":"Building Complex Control Structures","text":"<p>Complex control structures like if-then-else, for-loops, while-loops and do-while-loops can be built using methods of the <code>DaphneContext</code>. These control structures can be used to manipulate matrices, frames, and scalars, and are lazily evaluated. Futhermore, user-defined functions can be created to build reusable code which can then be again lazily evaluated. User-defined functions can manipulate matrices, frames, and scalars, too.</p>"},{"location":"DaphneLib/APIRef/#daphnecontext-api-reference_2","title":"<code>DaphneContext</code> API Reference","text":"<p>If-then-else</p> <ul> <li><code>cond</code><code>(input_nodes, pred, then_fn, else_fn)</code><ul> <li>input_nodes: Iterable[VALID_COMPUTED_TYPES]</li> <li>pred: Callable (0 arguments, 1 return value)</li> <li>then_fn: Callable (n arguments, n return values, n=[1, ...])</li> <li>else_fn: Callable (n arguments, n return values, n=[1, ...])</li> <li>returns: Tuple[VALID_COMPUTED_TYPES] (length n)</li> </ul> </li> </ul> <p>Loops</p> <ul> <li><code>for_loop</code><code>(input_nodes, callback, start, end, step)</code><ul> <li>input_nodes: Iterable[VALID_COMPUTED_TYPES]</li> <li>callback: Callable  (n+1 arguments, n return values, n=[1, ...]; the last argument is the iteration variable)</li> <li>start: int</li> <li>end: int</li> <li>step: Union[int, None]</li> <li>returns: Tuple[VALID_COMPUTED_TYPES]  (length n)</li> </ul> </li> <li><code>while_loop</code><code>(input_nodes, cond, callback)</code><ul> <li>input_nodes: Iterable[VALID_COMPUTED_TYPES]</li> <li>cond: Callable  (n arguments, 1 return value, n=[1, ...])</li> <li>callback: Callable  (n arguments, n return values)</li> <li>returns: Tuple[VALID_COMPUTED_TYPES]  (length n)</li> </ul> </li> <li><code>do_while_loop</code><code>(input_nodes, cond, callback)</code><ul> <li>input_nodes: Iterable[VALID_COMPUTED_TYPES]</li> <li>cond: Callable  (n arguments, 1 return value, n=[1, ...])</li> <li>callback: Callable  (n arguments, n return values)</li> <li>returns: Tuple[VALID_COMPUTED_TYPES]  (length n)</li> </ul> </li> </ul> <p>User-defined functions</p> <ul> <li><code>@function</code>, <code>function</code><code>(callback)</code><ul> <li>callback: Callable<ul> <li>This function requires adding typing hints in case the arguments are supposed to be handled as <code>Scalar</code> or <code>Frame</code>, all arguments without hints are handled as <code>Matrix</code> objects.   Hinting <code>Matrix</code> is optional.   Wrong or missing typing hints can trigger errors before and during computing (lazy evaluation).</li> </ul> </li> <li>returns: Tuple[VALID_COMPUTED_TYPES]  (length equals the return values of callback)</li> <li>if the decorator <code>@function</code> is used the callback is defined right below it like regular Python method</li> </ul> </li> </ul>"},{"location":"DaphneLib/Numpy2DaphneLib/","title":"Porting Numpy to DaphneLib in Python Scripts","text":"<p>A large part of the operations offered by Numpy can already be expressed in DaphneLib. By porting Numpy-based Python scripts to DaphneLib-based Python scripts, existing workflows can be executed in DAPHNE. In some cases, this translation is straightforward, especially when a Numpy operation can be replaced by a DaphneLib operation with the same name. However, in some cases this mapping may not be obvious. Some Numpy operations must be replaced by a single DaphneLib operation with a different name or by a more general/specialized operation. Some Numpy operations must be replaced by a complex expression of DaphneLib operations.</p> <p>In the following, we provide a few non-exhaustive guidelines on porting Numpy-based Python scripts to DaphneLib-based Python scripts. These guidelines represent some cases we have stumbled upon so far. However, we hope that they convey some general insights into how to approach the translation. In general, the DaphneLib API is subject to future changes, so things that are difficult at the moment may become easier in the future.</p>"},{"location":"DaphneLib/Numpy2DaphneLib/#some-noteworthy-differences-of-numpy-and-daphnelib","title":"Some Noteworthy Differences of Numpy and DaphneLib","text":"<p>Without claim of completeness.</p> <ul> <li> <p>Numpy supports n-dimensional data objects.     DAPHNE natively only supports 2-dimensional data objects.     1-d data objects can be represented in DAPHNE as 2-d objects where one dimension is 1.     General n-d objects can be represented in DAPHNE as 2-d objects where all but one dimension are linearized into the second dimension.     Furthermore, native support for n-d tensors is work-in-progress in DAPHNE.</p> </li> <li> <p>Both Numpy and DAPHNE support various types for the elements of a matrix.     These are called dtypes in Numpy and value types in DAPHNE.     Numpy's dtypes can be mapped to DAPHNE value types with the following table:</p> Numpy dtype DAPHNE vtype <code>np.float64</code> <code>\"f64\"</code> <code>np.float32</code> <code>\"f32\"</code> <code>np.int64</code> <code>\"si64\"</code> <code>np.int32</code> <code>\"si32\"</code> <code>np.int8</code> <code>\"si8\"</code> <code>np.uint64</code> <code>\"ui64\"</code> <code>np.uint32</code> <code>\"ui32\"</code> <code>np.uint8</code> <code>\"ui8\"</code> </li> <li> <p>The Numpy API offers access to low-level aspects that DAPHNE does not, because DAPHNE decides (or will decide) these points automatically to optimize the program.     Examples include: uninitialized data, the memory layout/order (e.g., C, F, ...), device placement, providing an output object, etc.     DAPHNE supports (or will support) hints for expert users to optionally make such decision by hand.     Such hints are already partly supported in DaphneDSL, but not supported in DaphneLib yet.</p> </li> </ul>"},{"location":"DaphneLib/Numpy2DaphneLib/#concrete-example-operations","title":"Concrete Example Operations","text":"<p>In the following, we assume the Numpy 2.0 API. Porting for other Numpy versions may be possible by following similar lines of thought.</p> <ul> <li> <p><code>numpy.</code><code>astype</code><code>(x, dtype, /, *, copy=True)</code></p> <p>Parameters</p> <ul> <li><code>x</code>: supported</li> <li><code>dtype</code>: supported<ul> <li>Numpy dtypes are mapped to DAPHNE value types using the table above.</li> </ul> </li> <li><code>copy</code>: not supported</li> </ul> <p>Example</p> <ul> <li> <p>Numpy</p> <pre><code>Y = X.astype(np.float32)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = X.asType(vtype=\"f32\")\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>column_stack</code><code>(tup)</code></p> <p>Parameters</p> <ul> <li><code>tup</code>: supported</li> </ul> <p>Example</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.column_stack((X1, X2, X3))\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code># Ensure that X1, X2, X3 are column matrices.\nX1c = X1.reshape(X1.ncell(), 1)\nX2c = X2.reshape(X2.ncell(), 1)\nX3c = X3.reshape(X3.ncell(), 1)\n\nY = X1c.cbind(X2c).cbind(X3c)\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>concatenate</code><code>((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")</code></p> <p>Parameters</p> <ul> <li><code>(a1, a2, ...)</code>: supported<ul> <li>only two at a time</li> </ul> </li> <li><code>axis</code>: supported<ul> <li>via different operation names (<code>rbind</code>/<code>cbind</code>)</li> </ul> </li> <li><code>out</code>: not supported</li> <li><code>dtype</code>: supported<ul> <li>documentation coming later</li> </ul> </li> </ul> <p>Example 1</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.concatenate((X1, X2, X3), axis=1)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = X1.cbind(X2).cbind(X3)\n</code></pre> </li> </ul> <p>Example 2</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.concatenate((X1, X2), axis=0)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = X1.rbind(X2)\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>copy</code><code>(a, order='K', subok=False)</code></p> <p>Explicit copying is generally not supported/necessary in DAPHNE. Editable references to a data object are not possible in DAPHNE, since references are copy-on-write.</p> </li> <li> <p><code>numpy.</code><code>empty</code><code>(shape, dtype=float, order='C', *, device=None, like=None)</code></p> <p>Explicitly creating an uninitialized matrix is not possible in DAPHNE. As a workaround, one could create an initialized matrix.</p> <p>Parameters</p> <ul> <li><code>shape</code>: supported</li> <li><code>dtype</code>: supported<ul> <li>Numpy dtypes are mapped to DAPHNE value types by the table above.</li> </ul> </li> <li><code>order</code>: not supported</li> <li><code>device</code>: not supported</li> <li><code>like</code>: not supported</li> </ul> <p>Example</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.empty(shape=(3, 2), dtype=np.float32)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = dc.fill(0, 3, 2).asType(vtype=\"f32\")\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>full</code><code>(shape, fill_value, dtype=None, order='C', *, device=None, like=None)</code></p> <p>Parameters</p> <ul> <li><code>shape</code>: supported<ul> <li>only 2d shapes like <code>(m, n)</code></li> <li>currently bug in DaphneLib when at least one dimension is <code>0</code></li> </ul> </li> <li><code>fill_value</code>: supported</li> <li><code>dtype</code>: supported<ul> <li>documentation coming later</li> </ul> </li> <li><code>order</code>: not supported<ul> <li>deliberately no direct control in DAPHNE</li> </ul> </li> <li><code>device</code>: not supported<ul> <li>deliberately no direct control in DAPHNE</li> </ul> </li> <li><code>like</code>: not supported</li> </ul> <p>Example</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.full((m, n), v)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = dc.fill(v, m, n)\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>isnan</code><code>(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature]) = &lt;ufunc 'isnan'&gt;</code></p> <p>Parameters</p> <ul> <li><code>x</code>: supported</li> <li>all other parameters: not supported</li> </ul> <p>Example</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.isnan(X)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = X.isNan()\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>logical_not</code><code>(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature]) = &lt;ufunc 'logical_not'&gt;</code></p> <p> Will be directly supported in DaphneLib in the future.</p> <p>Parameters</p> <ul> <li><code>x</code>: supported</li> <li>all other parameters: not supported</li> </ul> <p>Example</p> <ul> <li> <p>We assume that <code>X</code> is of dtype <code>np.bool</code>.     DaphneLib doesn't support the exchange of boolean arrays with Numpy yet.     Thus, for the DaphneLib variant, <code>X</code> must be changed to an integral dtype before the transfer to DaphneLib, e.g., by <code>X = X.astype(np.int64)</code>;     and the result must be changed back to boolean dtype, e.g., by <code>Y = Y.astype(np.bool)</code>.</p> </li> <li> <p>Numpy</p> <pre><code>Y = np.logical_not(X)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = (X - 1).abs()\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>random.choice</code><code>(a, size=None, replace=True, p=None)</code></p> <p>Parameters</p> <ul> <li><code>a</code>: supported<ul> <li>Note that all DAPHNE data objects are 2d.</li> </ul> </li> <li><code>size</code>: supported<ul> <li>Only 2d shapes are supported.</li> </ul> </li> <li><code>replace</code>: supported</li> <li><code>p</code>: not supported<ul> <li>Could be achieved with workarounds. TODO</li> </ul> </li> </ul> <p>Example 1 (draw 1d array from arange without replacement)</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.random.choice(10, 5, False)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = dc.sample(10, 5, False)\n</code></pre> </li> <li> <p>Note that Numpy returns a 1d array, while DAPHNE returns a 5x1 column matrix.</p> </li> </ul> <p>Example 2 (draw 2d array from 1d array with replacement)</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.random.choice(X, (3, 2), True)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code># Assumes that `X` is a column (m x 1) matrix.\nY = X[dc.sample(X.nrow(), 3 * 2, True), :].reshape(3, 2)\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>random.permutation</code><code>(x)</code></p> <p>Parameters</p> <ul> <li><code>x</code>: supported<ul> <li>only 2d arrays/matrices are supported</li> </ul> </li> </ul> <p>Example</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.random.permutation(X)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>nr = X.nrow()\nnc = X.ncol()\nY = X.reshape(X.ncell(), 1)[dc.sample(X.ncell(), X.ncell(), False), :].reshape(nr, nc)\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>repeat</code><code>(a, repeats, axis=None)</code></p> <p>Parameters</p> <ul> <li><code>a</code>: supported</li> <li><code>repeats</code>: supported<ul> <li>only scalars are supported</li> </ul> </li> <li><code>axis</code>: supported<ul> <li>only <code>None</code>, <code>0</code>, and <code>1</code> are supported</li> </ul> </li> </ul> <p>Example 1 (repeat column matrix along row dimension)</p> <ul> <li> <p>Assuming <code>X</code> is a column (m x 1) matrix.</p> </li> <li> <p>Numpy</p> <pre><code>Y = np.repeat(X, 3, 0)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = X.outerAdd(dc.fill(0, 1, 3)).reshape(X.nrow() * 3, 1)\n</code></pre> </li> </ul> <p>Example 2 (repeat row matrix along row dimension)</p> <ul> <li> <p>Assuming <code>X</code> is a row (1 x n) matrix.</p> </li> <li> <p>Numpy</p> <pre><code>Y = np.repeat(X, 3, 0)\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = dc.fill(0, 3, 1).outerAdd(X)\n</code></pre> </li> </ul> </li> <li> <p><code>numpy.</code><code>tile</code><code>(A, reps)</code></p> <p>Parameters</p> <ul> <li><code>A</code>: supported<ul> <li>Only 2d matrices are supported.</li> </ul> </li> <li><code>reps</code>: supported<ul> <li>Only scalars and 2x1 matrices are supported, i.e., repetition along row and/or column dimension.</li> </ul> </li> </ul> <p>Example</p> <ul> <li> <p>Numpy</p> <pre><code>Y = np.tile(X, (3, 2))\n</code></pre> </li> <li> <p>DaphneLib</p> <pre><code>Y = X[dc.seq(0, X.nrow() * 3 - 1).mod(X.nrow()), dc.seq(0, X.ncol() * 2 - 1).mod(X.ncol())]\n</code></pre> </li> </ul> </li> </ul>"},{"location":"DaphneLib/Overview/","title":"Overview: DAPHNE's Python API","text":"<p>DaphneLib is a simple user-facing Python API that allows calling individual basic and higher-level DAPHNE built-in functions. The overall design follows similar abstractions like PySpark and Dask by using lazy evaluation. When the evaluation is triggered, DaphneLib assembles and executes a DaphneDSL script that uses the entire DAPHNE compilation and runtime stack, including all optimizations. Users can easily mix and match DAPHNE computations with other Python libraries and plotting functionality.</p> <p>DaphneLib is still in an experimental stage, feedback and bug reports via GitHub issues are highly welcome.</p>"},{"location":"DaphneLib/Overview/#prerequisites","title":"Prerequisites","text":"<p>Provide DAPHNE:</p> <ul> <li><code>libdaphnelib.so</code> and <code>libAllKernels.so</code> must be present</li> <li>Building the project with <code>./build.sh --target daphnelib</code> achieves this (this creates a <code>lib</code> dir in the <code>daphne</code> project root)</li> <li>OR use the <code>lib/</code> dir of a release</li> <li><code>LD_LIBRARY_PATH</code> must be set (e.g., executed from <code>daphne/</code>: <code>export LD_LIBRARY_PATH=$PWD/lib:$LD_LIBRARY_PATH</code>)</li> <li>Set the environment variable <code>DAPHNELIB_DIR_PATH</code> to the path were the libraries (<code>*.so</code> files) are placed, e.g., <code>path/to/daphne/lib/</code></li> </ul>"},{"location":"DaphneLib/Overview/#installation","title":"Installation","text":"<ul> <li>There are two options to install the Python package <code>daphne</code> (DaphneLib)</li> <li>Via github url: <code>pip install git+https://github.com/daphne-eu/daphne.git@main#subdirectory=src/api/python</code></li> <li>OR clone the DAPHNE repository and install from source files: <code>pip install daphne/src/api/python</code></li> <li> <p>Recommendation: Use a virtual environment</p> <pre><code>python3 -m venv my_venv\nsource my_venv/bin/activate\npip install ...\n</code></pre> </li> </ul>"},{"location":"DaphneLib/Overview/#use-without-installation","title":"Use Without Installation","text":"<ul> <li>In a cloned DAPHNE repository, DaphneLib can also be used without installing the Python package <code>daphne</code></li> <li>To this end, <code>python3</code> must be told where to find the package by adding the respective directory to the Python path</li> <li>From the DAPHNE root directory, execute: <code>export PYTHONPATH=\"$PYTHONPATH:$PWD/src/api/python/\"</code></li> <li>OR execute the script <code>run_python.sh</code> (instead of <code>python3</code>) from the DAPHNE root directory, e.g., <code>./run_python.sh myScript.py</code></li> </ul>"},{"location":"DaphneLib/Overview/#introductory-example","title":"Introductory Example","text":"<p>The following simple example script generates a 5x3 matrix of random values in \\([0, 1)\\) using numpy, imports the data to DAPHNE, and shifts and scales the data such that each column has a mean of 0 and a standard deviation of 1.</p> <pre><code># (1) Import DaphneLib.\nfrom daphne.context.daphne_context import DaphneContext\nimport numpy as np\n\n# (2) Create DaphneContext.\ndc = DaphneContext()\n\n# (3) Obtain a DAPHNE matrix.\nX = dc.from_numpy(np.random.rand(5, 3))\n\n# (4) Define calculations in DAPHNE.\nY = (X - X.mean(axis=1)) / X.stddev(axis=1)\n\n# (5) Compute result in DAPHNE.\nprint(Y.compute())\n</code></pre> <p>First, DAPHNE's Python library must be imported (1).</p> <p>Then, a <code>DaphneContext</code> must be created (2). The <code>DaphneContext</code> offers means to obtain DAPHNE matrices and frames, which serve as the starting point for defining complex computations. Here, we generate some random data in numpy and import it to DAPHNE (3).</p> <p>Based on DAPHNE matrices/frames/scalars (and Python scalars), complex expressions can be defined (4) using Python operators (such as <code>-</code> and <code>/</code> above) and methods on the DAPHNE matrices/frames/scalars (such as <code>mean()</code> and <code>stddev()</code> above). The results of these expressions again represent DAPHNE matrices/frames/scalars.</p> <p>Up until here, no acutal computations are performed. Instead, an internal DAG (directed acyclic graph) representation of the computation is built. When calling <code>compute()</code> on the result (5), the DAG is automatically optimized and executed by DAPHNE. This principle is known as lazy evaluation. (Internally, a DaphneDSL script is created, which is sent through the entire DAPHNE compiler and runtime stack, thereby profiting from all optimizations in DAPHNE.) The result is returned as a <code>numpy.ndarray</code> (for DAPHNE matrices), as a <code>pandas.DataFrame</code> (for DAPHNE frames), or as a plain Python scalar (for DAPHNE scalars), and can then be further used in Python.</p> <p>The script above can be executed by:</p> <pre><code>python3 scripts/examples/daphnelib/shift-and-scale.py\n</code></pre> <p>The remainder of this document presents the core features of DaphneLib as they are right now, but note that DaphneLib is still under active development.</p>"},{"location":"DaphneLib/Overview/#data-and-value-types","title":"Data and Value Types","text":"<p>DAPHNE differentiates data types and value types.</p> <p>Currently, DAPHNE supports the following abstract data types:</p> <ul> <li><code>matrix</code>: homogeneous value type for all cells</li> <li><code>frame</code>: a table with columns of potentially different value types</li> <li><code>scalar</code>: a single value</li> </ul> <p>Value types specify the representation of individual values. We currently support:</p> <ul> <li>floating-point numbers of various widths: <code>f64</code>, <code>f32</code></li> <li>signed and unsigned integers of various widths: <code>si64</code>, <code>si32</code>, <code>si8</code>, <code>ui64</code>, <code>ui32</code>, <code>ui8</code></li> <li>strings <code>str</code> (currently only for scalars, support for matrix elements is still experimental)</li> <li>booleans <code>bool</code> (currently only for scalars)</li> </ul> <p>Data types and value types can be combined, e.g.:</p> <ul> <li><code>matrix&lt;f64&gt;</code> is a matrix of double-precision floating point values</li> </ul> <p>In DaphneLib, each node of the computation DAG has one of the types <code>daphne.operator.nodes.matrix.Matrix</code>, <code>daphne.operator.nodes.frame.Frame</code>, or <code>daphne.operator.nodes.scalar.Scalar</code>. The type of a node determines which methods can be invoked on it (see DaphneLib API reference).</p>"},{"location":"DaphneLib/Overview/#obtaining-daphne-matrices-and-frames","title":"Obtaining DAPHNE Matrices and Frames","text":"<p>The <code>DaphneContext</code> offers means to obtain DAPHNE matrices and frames, which serve as the starting point for defining complex computations. More precisely, DAPHNE matrices and frames can be obtained in the following ways:</p> <ul> <li>importing data from other Python libraries (e.g., numpy and pandas)</li> <li>generating data in DAPHNE (e.g., random data, constants, or sequences)</li> <li>reading files using DAPHNE's readers (e.g., CSV, Matrix Market, Parquet, DAPHNE binary format)</li> </ul> <p>A comprehensive list can be found in the DaphneLib API reference.</p>"},{"location":"DaphneLib/Overview/#building-complex-computations","title":"Building Complex Computations","text":"<p>Based on DAPHNE matrices/frames/scalars and Python scalars, complex expressions can be defined by</p> <ul> <li>Python operators</li> <li>DAPHNE matrix/frame/scalar methods</li> </ul> <p>The results of these expressions again represent DAPHNE matrices/frames/scalars.</p>"},{"location":"DaphneLib/Overview/#python-operators","title":"Python Operators","text":""},{"location":"DaphneLib/Overview/#binary-operators","title":"Binary Operators","text":"<p>DaphneLib currently supports the following binary operators on DAPHNE matrices/frames/scalars:</p> Operator Meaning <code>@</code> matrix multiplication <code>**</code> exponentiation <code>*</code>, <code>/</code>, <code>%</code> multiplication, division, modulo <code>+</code>, <code>-</code> addition/string concatenation, subtraction <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code> comparison <p>We plan to add more operators, including unary operators.</p> <p>Matrix multiplication (<code>@</code>): The inputs must be matrices of compatible shapes, and the output is always a matrix.</p> <p>All other operators: The following table shows which combinations of inputs are allowed and which result they yield:</p> Left input Right input Result Details scalar scalar scalar binary operation of two scalars matrix (n x m) scalar matrix (n x m) element-wise operation of each value with scalar scalar matrix (n x m) matrix (n x m) element-wise operation of scalar with each value (*) matrix (n x m) matrix (n x m) matrix (n x m) element-wise operation on corresponding values matrix (n x m) matrix (1 x m) matrix (n x m) broadcasting of row-vector matrix (n x m) matrix (n x 1) matrix (n x m) broadcasting of column-vector <p>(*) Scalar-<code>op</code>-matrix operations are so far only supported for <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>; for <code>/</code> only if the matrix is of a floating-point value type.</p> <p>In the future, we will fully support scalar-<code>op</code>-matrix operations as well as row/column-matrices as the left-hand-side operands.</p> <p>Examples:</p> <pre><code>1.5 * X @ y + 0.001\n</code></pre>"},{"location":"DaphneLib/Overview/#indexing","title":"Indexing","text":"<p>DaphneLib supports right and left indexing on DAPHNE matrices using Python's square bracket <code>[]</code> operator to extract elements from a matrix or set elements into a matrix, respectively.</p> <p>Extracting elements (right indexing) supports indexing by integer (e.g., <code>3</code>), slice (e.g, <code>:</code>, <code>:3</code>, <code>1:4</code>), or a DaphneLib column matrix of positions (do not need to be unique or sorted). The extracted elements are always returned as a DaphneLib matrix, even if it is just a single element.</p> <p>Setting elements (left indexing) currently only supports indexing by integer and slice. The elements to insert must always be provided as a DaphneLib matrix, even if it is just a single element.</p> <p>Examples:</p> <pre><code>from daphne.context.daphne_context import DaphneContext\n\ndc = DaphneContext()\n\n# 10x5 matrix containing the numbers from 0 to 49.\nX = dc.seq(0, 49).reshape(10, 5)\n\n# Extract rows from 1 (inclusive) to 4 (exclusive).\nX[1:4, :].print().compute()\n\n# Extract column 3.\nX[:, 3].print().compute()\n\n# Extract columns 2 (inclusive) to 4 (exclusive) of rows [0, 3, 6].\nrowIdxs = dc.seq(0, 6, 3)\nX[rowIdxs, 2:4].print().compute()\n\n# Set columns 1 (inclusive) to 4 (exclusive) of row 3 to zero.\nX[3, 1:4] = dc.fill(0, 1, 3)\nX.print().compute()\n</code></pre>"},{"location":"DaphneLib/Overview/#matrixframescalar-methods","title":"Matrix/Frame/Scalar Methods","text":"<p>DaphneLib's classes <code>Matrix</code>, <code>Frame</code>, and <code>Scalar</code> offer a range of methods to call DAPHNE built-in functions. A comprehensive list can be found in the DaphneLib API reference.</p> <p>Examples:</p> <pre><code>X.t()\nX.sqrt()\nX.cbind(Y)\n</code></pre>"},{"location":"DaphneLib/Overview/#data-exchange-with-other-python-libraries","title":"Data Exchange with other Python Libraries","text":"<p>DaphneLib supports efficient data exchange with other well-known Python libraries, in both directions. The data transfer from other Python libraries to DaphneLib can be triggered through the <code>from_...()</code> methods of the <code>DaphneContext</code> (e.g., <code>from_numpy()</code>). A comprehensive list of these methods can be found in the DaphneLib API reference. The data transfer from DaphneLib back to Python happens during the call to <code>compute()</code>. If the result of the computation in DAPHNE is a matrix, <code>compute()</code> returns a <code>numpy.ndarray</code> (or optionally a <code>tensorflow.Tensor</code> or <code>torch.Tensor</code>); if the result is a frame, it returns a <code>pandas.DataFrame</code>; and if the result is a scalar, it returns a plain Python scalar.</p> <p>So far, DaphneLib can exchange data with numpy, pandas, TensorFlow, and PyTorch. By default, the data transfer is via shared memory (and in many cases zero-copy).</p>"},{"location":"DaphneLib/Overview/#data-exchange-with-numpy","title":"Data Exchange with numpy","text":"<p>Example:</p> <pre><code>from daphne.context.daphne_context import DaphneContext\nimport numpy as np\n\ndc = DaphneContext()\n\n# Create data in numpy.\na = np.arange(8.0).reshape((2, 4))\n\n# Transfer data to DaphneLib (lazily evaluated).\nX = dc.from_numpy(a)\n\nprint(\"How DAPHNE sees the data from numpy:\")\nX.print().compute()\n\n# Add 100 to each value in X.\nX = X + 100.0\n\n# Compute in DAPHNE, transfer result back to Python.\nprint(\"\\nResult of adding 100 to each value, back in Python:\")\nprint(X.compute())\n</code></pre> <p>Run by:</p> <pre><code>python3 scripts/examples/daphnelib/data-exchange-numpy.py\n</code></pre> <p>Output:</p> <pre><code>How DAPHNE sees the data from numpy:\nDenseMatrix(2x4, double)\n0 1 2 3\n4 5 6 7\n\nResult of adding 100 to each value, back in Python:\n[[100. 101. 102. 103.]\n [104. 105. 106. 107.]]\n</code></pre>"},{"location":"DaphneLib/Overview/#data-exchange-with-pandas","title":"Data Exchange with pandas","text":"<p>Example:</p> <pre><code>from daphne.context.daphne_context import DaphneContext\nimport pandas as pd\n\ndc = DaphneContext()\n\n# Create data in pandas.\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [1.1, -2.2, 3.3]})\n\n# Transfer data to DaphneLib (lazily evaluated).\nF = dc.from_pandas(df)\n\nprint(\"How DAPHNE sees the data from pandas:\")\nF.print().compute()\n\n# Append F to itself.\nF = F.rbind(F)\n\n# Compute in DAPHNE, transfer result back to Python.\nprint(\"\\nResult of appending the frame to itself, back in Python:\")\nprint(F.compute())\n</code></pre> <p>Run by:</p> <pre><code>python3 scripts/examples/daphnelib/data-exchange-pandas.py\n</code></pre> <p>Output:</p> <pre><code>How DAPHNE sees the data from pandas:\nFrame(3x2, [a:int64_t, b:double])\n1 1.1\n2 -2.2\n3 3.3\n\nResult of appending the frame to itself, back in Python:\n   a    b\n0  2 -2.2\n1  3  3.3\n2  1  1.1\n3  2 -2.2\n4  3  3.3\n</code></pre>"},{"location":"DaphneLib/Overview/#data-exchange-with-tensorflow","title":"Data Exchange with TensorFlow","text":"<p>Example:</p> <pre><code>from daphne.context.daphne_context import DaphneContext\nimport tensorflow as tf\nimport numpy as np\n\ndc = DaphneContext()\n\nprint(\"========== 2D TENSOR EXAMPLE ==========\\n\")\n\n# Create data in TensorFlow/numpy.\nt2d = tf.constant(np.random.random(size=(2, 4)))\n\nprint(\"Original 2d tensor in TensorFlow:\")\nprint(t2d)\n\n# Transfer data to DaphneLib (lazily evaluated).\nT2D = dc.from_tensorflow(t2d)\n\nprint(\"\\nHow DAPHNE sees the 2d tensor from TensorFlow:\")\nT2D.print().compute()\n\n# Add 100 to each value in T2D.\nT2D = T2D + 100.0\n\n# Compute in DAPHNE, transfer result back to Python.\nprint(\"\\nResult of adding 100, back in Python:\")\nprint(T2D.compute(asTensorFlow=True))\n\nprint(\"\\n========== 3D TENSOR EXAMPLE ==========\\n\")\n\n# Create data in TensorFlow/numpy.\nt3d = tf.constant(np.random.random(size=(2, 2, 2)))\n\nprint(\"Original 3d tensor in TensorFlow:\")\nprint(t3d)\n\n# Transfer data to DaphneLib (lazily evaluated).\nT3D, T3D_shape = dc.from_tensorflow(t3d, return_shape=True)\n\nprint(\"\\nHow DAPHNE sees the 3d tensor from TensorFlow:\")\nT3D.print().compute()\n\n# Add 100 to each value in T3D.\nT3D = T3D + 100.0\n\n# Compute in DAPHNE, transfer result back to Python.\nprint(\"\\nResult of adding 100, back in Python:\")\nprint(T3D.compute(asTensorFlow=True))\nprint(\"\\nResult of adding 100, back in Python (with original shape):\")\nprint(T3D.compute(asTensorFlow=True, shape=T3D_shape))\n</code></pre> <p>Run by:</p> <pre><code>python3 scripts/examples/daphnelib/data-exchange-tensorflow.py\n</code></pre> <p>Output (random numbers may vary):</p> <pre><code>========== 2D TENSOR EXAMPLE ==========\n\nOriginal 2d tensor in TensorFlow:\ntf.Tensor(\n[[0.09682179 0.09636572 0.78658016 0.68227129]\n [0.64356184 0.96337785 0.07931763 0.97951051]], shape=(2, 4), dtype=float64)\n\nHow DAPHNE sees the 2d tensor from TensorFlow:\nDenseMatrix(2x4, double)\n0.0968218 0.0963657 0.78658 0.682271\n0.643562 0.963378 0.0793176 0.979511\n\nResult of adding 100, back in Python:\ntf.Tensor(\n[[100.09682179 100.09636572 100.78658016 100.68227129]\n [100.64356184 100.96337785 100.07931763 100.97951051]], shape=(2, 4), dtype=float64)\n\n========== 3D TENSOR EXAMPLE ==========\n\nOriginal 3d tensor in TensorFlow:\ntf.Tensor(\n[[[0.40088013 0.02324858]\n  [0.87607911 0.91645907]]\n\n [[0.10591184 0.92419294]\n  [0.5397723  0.24957817]]], shape=(2, 2, 2), dtype=float64)\n\nHow DAPHNE sees the 3d tensor from TensorFlow:\nDenseMatrix(2x4, double)\n0.40088 0.0232486 0.876079 0.916459\n0.105912 0.924193 0.539772 0.249578\n\nResult of adding 100, back in Python:\ntf.Tensor(\n[[100.40088013 100.02324858 100.87607911 100.91645907]\n [100.10591184 100.92419294 100.5397723  100.24957817]], shape=(2, 4), dtype=float64)\n\nResult of adding 100, back in Python (with original shape):\ntf.Tensor(\n[[[100.40088013 100.02324858]\n  [100.87607911 100.91645907]]\n\n [[100.10591184 100.92419294]\n  [100.5397723  100.24957817]]], shape=(2, 2, 2), dtype=float64)\n</code></pre>"},{"location":"DaphneLib/Overview/#data-exchange-with-pytorch","title":"Data Exchange with PyTorch","text":"<p>Example:</p> <pre><code>from daphne.context.daphne_context import DaphneContext\nimport torch\nimport numpy as np\n\ndc = DaphneContext()\n\nprint(\"========== 2D TENSOR EXAMPLE ==========\\n\")\n\n# Create data in PyTorch/numpy.\nt2d = torch.tensor(np.random.random(size=(2, 4)))\n\nprint(\"Original 2d tensor in PyTorch:\")\nprint(t2d)\n\n# Transfer data to DaphneLib (lazily evaluated).\nT2D = dc.from_pytorch(t2d)\n\nprint(\"\\nHow DAPHNE sees the 2d tensor from PyTorch:\")\nT2D.print().compute()\n\n# Add 100 to each value in T2D.\nT2D = T2D + 100.0\n\n# Compute in DAPHNE, transfer result back to Python.\nprint(\"\\nResult of adding 100, back in Python:\")\nprint(T2D.compute(asPyTorch=True))\n\nprint(\"\\n========== 3D TENSOR EXAMPLE ==========\\n\")\n\n# Create data in PyTorch/numpy.\nt3d = torch.tensor(np.random.random(size=(2, 2, 2)))\n\nprint(\"Original 3d tensor in PyTorch:\")\nprint(t3d)\n\n# Transfer data to DaphneLib (lazily evaluated).\nT3D, T3D_shape = dc.from_pytorch(t3d, return_shape=True)\n\nprint(\"\\nHow DAPHNE sees the 3d tensor from PyTorch:\")\nT3D.print().compute()\n\n# Add 100 to each value in T3D.\nT3D = T3D + 100.0\n\n# Compute in DAPHNE, transfer result back to Python.\nprint(\"\\nResult of adding 100, back in Python:\")\nprint(T3D.compute(asPyTorch=True))\nprint(\"\\nResult of adding 100, back in Python (with original shape):\")\nprint(T3D.compute(asPyTorch=True, shape=T3D_shape))\n</code></pre> <p>Run by:</p> <pre><code>python3 scripts/examples/daphnelib/data-exchange-pytorch.py\n</code></pre> <p>Output (random numbers may vary):</p> <pre><code>========== 2D TENSOR EXAMPLE ==========\n\nOriginal 2d tensor in PyTorch:\ntensor([[0.1205, 0.8747, 0.1717, 0.0216],\n        [0.7999, 0.6932, 0.4386, 0.0873]], dtype=torch.float64)\n\nHow DAPHNE sees the 2d tensor from PyTorch:\nDenseMatrix(2x4, double)\n0.120505 0.874691 0.171693 0.0215546\n0.799858 0.693205 0.438637 0.0872659\n\nResult of adding 100, back in Python:\ntensor([[100.1205, 100.8747, 100.1717, 100.0216],\n        [100.7999, 100.6932, 100.4386, 100.0873]], dtype=torch.float64)\n\n========== 3D TENSOR EXAMPLE ==========\n\nOriginal 3d tensor in PyTorch:\ntensor([[[0.5474, 0.9653],\n         [0.7891, 0.0573]],\n\n        [[0.4116, 0.6326],\n         [0.3148, 0.3607]]], dtype=torch.float64)\n\nHow DAPHNE sees the 3d tensor from PyTorch:\nDenseMatrix(2x4, double)\n0.547449 0.965315 0.78909 0.0572619\n0.411593 0.632629 0.314841 0.360657\n\nResult of adding 100, back in Python:\ntensor([[100.5474, 100.9653, 100.7891, 100.0573],\n        [100.4116, 100.6326, 100.3148, 100.3607]], dtype=torch.float64)\n\nResult of adding 100, back in Python (with original shape):\ntensor([[[100.5474, 100.9653],\n         [100.7891, 100.0573]],\n\n        [[100.4116, 100.6326],\n         [100.3148, 100.3607]]], dtype=torch.float64)\n</code></pre>"},{"location":"DaphneLib/Overview/#known-limitations","title":"Known Limitations","text":"<p>DaphneLib is still in an early development stage. Thus, there are a few limitations that users should be aware of. We plan to fix all of these limitations in the future.</p> <ul> <li>Using DAPHNE's command-line arguments to influence its behavior is not supported yet.</li> <li>Some DaphneDSL built-in functions are not represented by DaphneLib methods yet.</li> <li>High-level primitives for integrated data analysis pipelines, which are implemented in DaphneDSL, cannot be called from DaphneLib yet.</li> </ul>"},{"location":"development/BuildingDaphne/","title":"Building DAPHNE","text":"<p>The DAPHNE project provides a full-fledged build script. After cloning, it does everything from dependency setup to generation of the executable.</p>"},{"location":"development/BuildingDaphne/#what-does-the-build-script-do-simplified","title":"What does the build script do? (simplified)","text":"<ul> <li>Download &amp; build all code dependencies</li> <li>Build Daphne</li> <li>Clean Project</li> </ul>"},{"location":"development/BuildingDaphne/#how-long-does-a-build-take","title":"How long does a build take?","text":"<p>The first run will take a while, due to long compilation times of the dependencies (~1 hour on a 16 vcore desktop, ~10 minutes on a 128 vcore cluster node). But they only have to be compiled once (except updates). Following builds only take a few seconds/minutes.</p> <p>Contents:</p> <ul> <li>Usage of the build script</li> <li>Extension of the build script</li> </ul>"},{"location":"development/BuildingDaphne/#usage-of-the-build-script","title":"Usage of the build script","text":"<p>This section shows the possibilities of the build script.</p>"},{"location":"development/BuildingDaphne/#build","title":"Build","text":"<p>The default command to build the default target daphne.</p> <pre><code>./build.sh\n</code></pre> <p>Print the cli build help page. This also shows all the following options.</p> <pre><code>./build.sh --help\n</code></pre> <p>Build a specific target.</p> <pre><code>./build.sh --target \"target\"\n</code></pre> <p>For example the following builds the main test target.</p> <pre><code>./build.sh --target \"run_tests\"\n</code></pre>"},{"location":"development/BuildingDaphne/#clean","title":"Clean","text":"<p>Clean all build directories, i.e., the daphne build dir <code>&lt;project_root&gt;/build</code> and the build output in <code>&lt;project_root&gt;/bin</code> and <code>&lt;project_root&gt;/lib</code></p> <pre><code>./build.sh --clean\n</code></pre> <p>Clean all downloads and extracted archive directories, i.e., <code>&lt;thirdparty_dir&gt;</code>/download-cache, <code>&lt;thirdparty_dir&gt;</code>/sources and <code>&lt;thirdparty_dir&gt;</code>/*.download.success files:</p> <pre><code>./build.sh --cleanCache\n</code></pre> <p>Clean third party build output, i.e., <code>&lt;thirdparty_dir&gt;/installed</code>, <code>&lt;thirdparty_dir&gt;/build</code> and <code>&lt;thirdparty_dir&gt;</code>/*.install.success files:</p> <pre><code>./build.sh --cleanDeps\n</code></pre> <p>Clean everything (DAPHNE build output and third party directory)</p> <pre><code>./build.sh --cleanAll\n</code></pre>"},{"location":"development/BuildingDaphne/#minimize-compile-times-of-dependencies","title":"Minimize Compile Times of Dependencies","text":"<p>The most time-consuming part of getting DAPHNE compiled is building the third party dependencies. To avoid this, one can either use a prebuilt container image (in combination with some parameters to the build script see below) or at least build the dependencies once and subsequently point to the directory where the third party dependencies get installed. The bulid script must be invoked with the following two parameters to achieve this:</p> <p><code>./build.sh --no-deps --installPrefix path/to/installed/deps</code></p> <p>If you have built DAPHNE and change the installPrefix directory, it is required to clean up and build again: <code>./build.sh --clean</code></p>"},{"location":"development/BuildingDaphne/#options","title":"Options","text":"<p>All possible options for the build script:</p> Option Effect -h, --help Print the help page --installPrefix &lt;path&gt; Install third party dependencies in <code>&lt;path&gt;</code> (default: <code>&lt;project_root&gt;/thirdparty/installed</code>) --clean Clean DAPHNE build output (<code>&lt;project_root&gt;/{bin,build,lib}</code>) --cleanCache Clean downloaded and extracted third party artifacts --cleanDeps Clean third party dependency build output and installed files --cleanAll Clean DAPHNE build output and reset the third party directory to the state in the git repo --target &lt;target&gt; Build specific target -nf, --no-fancy Disable colorized output --no-deps Avoid building third party dependencies -y, --yes Accept prompt (e.g., when executing the clean command) --cuda Compile with support for GPU operations using the CUDA SDK --debug Compile the daphne binary with debug symbols --oneapi Compile with support for accelerated operations using the OneAPI SDK --fpgaopencl Compile with support for FPGA operations using the Intel FPGA SDK or OneAPI+FPGA Add-On --no-papi Compile without support for PAPI-based profiling"},{"location":"development/BuildingDaphne/#extension","title":"Extension","text":""},{"location":"development/BuildingDaphne/#overview-over-the-build-script","title":"Overview over the build script","text":"<p>The build script is divided into sections, visualized by</p> <pre><code>#******************************************************************************\n# #1 Section name\n#******************************************************************************\n</code></pre> <p>Each section should only contain functionality related to the section name.</p> <p>The following list contains a rough overview over the sections and the concrete functions or functionality done here.</p> <ol> <li>Help message</li> <li>printHelp() // prints help message</li> <li>Build message helper</li> <li>daphne_msg( &lt;message&gt; ) // prints a status message in DAPHNE style</li> <li>printableTimestamp( &lt;timestamp&gt; ) // converts a unix epoch timestamp into a human readable string (e.g., 5min 20s 100ms)</li> <li>printLogo() // prints a DAPHNE logo to the console</li> <li>Clean build directories</li> <li>clean( &lt;array ref dirs&gt; &lt;array ref files&gt; ) // removes all given directories (1. parameter) and all given files (2. parameter) from disk</li> <li>cleanBuildDirs() // cleans build dirs (daphne and dependency build dirs)</li> <li>cleanAll() // cleans daphne build dir and wipes all dependencies from disk (resetting the third party directory)</li> <li>cleanDeps() // removes third party build output</li> <li>cleanCache() // removes downloaded third party artifacts (but leaving git submodules (only LLVM/MLIR at the time of writing)</li> <li>Create / Check Indicator-files</li> <li>dependency_install_success( &lt;dep&gt; ) // used after successful build of a dependency; creates related indicator file</li> <li>dependency_download_success(&lt;dep&gt;) // used after successful download of a dependency; creates related indicator file</li> <li>is_dependency_installed( &lt;dep&gt; ) // checks if dependency is already installed/built successfully</li> <li>is_dependency_downloaded( &lt;dep&gt; ) // checks if dependency is already downloaded successfully</li> <li>Versions of third party dependencies</li> <li>Versions of the software dependencies are configured here</li> <li>Set some prefixes, paths and dirs</li> <li>Definition of project related paths</li> <li>Configuration of path prefixes. For example all build directories are prefixed with <code>buildPrefix</code>. If fast storage       is available on the system, build directories could be redirected with this central configuration.</li> <li>Parse arguments</li> <li>Parsing</li> <li>Updating git submodules</li> <li>Download and install third-party dependencies if requested (default is yes, omit with --no-deps))</li> <li>Antlr</li> <li>catch2</li> <li>OpenBLAS</li> <li>nlohmannjson</li> <li>abseil-cpp - Required by gRPC. Compiled separately to apply a patch.</li> <li>MPI (Default is MPI library is OpenMPI but cut can be any)</li> <li>gRPC</li> <li>Arrow / Parquet</li> <li>MLIR</li> <li>Build DAPHNE target</li> <li>Compilation of the DAPHNE-target ('daphne' is default)</li> </ol>"},{"location":"development/BuildingDaphne/#adding-a-dependency","title":"Adding a dependency","text":"<ol> <li>If the dependency is fixed to a specific version, add it to the dependency versions section (section 5).</li> <li>Create a new segment in section 8 for the new dependency.</li> <li>Define needed dependency variables:</li> <li>Directory Name (which is used by the script to locate the dependency in different stages)</li> <li>Create an internal version variable in form of an array with two entries. Those are used for internal versioning and updating of the dependency without rebuilding each time.<ol> <li>First: Name and version of the dependency as a string of the form <code>&lt;dep_name&gt;_v${dep_version}</code> (This one is updated, if a new version of the dependency is choosen.)</li> <li>Second: Thirdparty Version of the dependency as a string of the form <code>v1</code> (This one is incremented each time by hand, if something changes on the path system of the dependency or DAPHNE itself. This way already existing projects are updated automatically, if something changes.)</li> </ol> </li> <li>Optionals: Dep-specific paths, Dep-specific files, etc.</li> <li> <p>Download the dependency, encased by:</p> <pre><code># in segment 5\n&lt;dep&gt;_version=\"&lt;dep_version&gt;\"\n\n# in segment 8\n# ----\n# 8.x Your dependency\n# ----\n&lt;dep&gt;_dirname=\"&lt;dep_name&gt;\" # 3.1\n&lt;dep&gt;_version_internal=(\"&lt;dep_name&gt;_v${&lt;dep&gt;_version}\" \"v1\") # 3.2\n&lt;dep&gt;... # 3.3\nif ! is_dependency_downloaded \"${&lt;dep&gt;_version_internal[@]}\"; then\n\n    # do your stuff here\n\n    dependency_download_success \"${&lt;dep&gt;_version_internal[@]}\"\nfi\n</code></pre> <p>Hint: It is recommended to use the paths defined in section 6 for dependency downloads and installations. There are predefined paths like 'cacheDir', 'sourcePrefix', 'buildPrefix' and 'installPrefix'. Take a look at other dependencies to see how to use them. 1. Install the dependency (if necessary), encased by:</p> <pre><code>if ! is_dependency_installed \"${&lt;dep&gt;_version_internal[@]}\"; then\n\n    # do your stuff here\n\n    dependency_install_success \"${&lt;dep&gt;_version_internal[@]}\"\nfi\n</code></pre> </li> <li> <p>Define a flag for the build script if your dependency is optional or poses unnecessary    overhead for users (e.g., CUDA is optional as the CUDA SDK is a considerably sized package that only owners of Nvidia hardware would want to install).</p> </li> </ol> <p>See section 7 about argument parsing. Quick guide: define a variable and its default value and add an item to the argument handling loop.</p>"},{"location":"development/Contributing/","title":"Contributing to the DAPHNE System","text":"<p>Thank you for your interest in contributing to the DAPHNE system. Our goal is to build an open and inclusive community of developers around the system. Thus, contributions are highly welcome, both from within the DAPHNE project consortium and from external researchers/developers.</p> <p>In the following, you find some rough guidelines on contributing, which will most likely be extended and further clarified in the future.</p>"},{"location":"development/Contributing/#ways-of-contributing","title":"Ways of Contributing","text":"<p>There are various ways of contributing including (but not limited to): - actual implementation - writing test cases - writing documentation - reporting bugs or any other kind of issue - contributing to discussions</p> <p>We encourage open communication about the system through comments on issues and pull requests directly on GitHub. That way, discussions are made accessible and transparent to everyone interested. This is important to involve people and to avoid repetition in case multiple people have the same question/comment or encounter the same problem. So feel free to create an issue to start a discussion on a particular topic (including these contribution guidelines) or to report a bug or other problem.</p>"},{"location":"development/Contributing/#issue-tracking","title":"Issue tracking","text":"<p>All open/ongoing/completed work is tracked as issues on GitHub. These could be anything from precisely defined small tasks to requests for complex components. In any case, we will try to keep the book-keeping effort at a low level; at the same time, we should make each other aware of what everyone is working on to avoid duplicate work.</p> <p>If you would like to contribute and are looking for a task to work on, browse the list of issues. If you are a new contributor, you might want to watch out for \"good first issues\".</p> <p>Furthermore, everyone is invited to create issues, e.g., for tasks you want to work on or problems you encountered. This is also a good way to enable discussion on the topic. Note that there is a set of labels that can be attached to your issue to clarify what it is about and to make it more easy to find.</p> <p>Before you start working on an issue, please make sure to get assigned to the issue. New contributors need to leave a comment on the issue before they can get assigned. Collaborators can assign themselves.</p>"},{"location":"development/Contributing/#contributing-to-the-source-code","title":"Contributing to the Source Code","text":"<p>We appreciate that different contributors can have different levels of familiarity with the code base, and try to adapt to that accordingly.</p>"},{"location":"development/Contributing/#new-daphne-contributors","title":"New DAPHNE Contributors","text":"<p>Contributions from new people are always welcome, both from within the DAPHNE project consortium and external! We are aware that contributing to a new code base can be challenging in the beginning. Thus, we want to keep the barrier of entry low for new contributors. That is, please try your best to make a good-quality contribution and we will help you with constructive feedback.</p> <p>The procedure is roughly as follows:</p> <ol> <li>Get assigned to the issue to let others know you are going to work on it and to avoid duplicate work. Please leave a comment on the issue stating that you are going to work on it. After that, a collaborator will formally assign you.</li> <li>Fork the repository on GitHub and clone your fork (see GitHub docs).    We recommend cloning by <code>git clone --recursive https://github.com/&lt;USERNAME&gt;/daphne.git</code> (note the <code>--recursive</code>), as specified in Getting Started.</li> </ol> <p>You may skip this step and reuse your existing fork if you have contributed before. Simply update your fork with the recent changes from the original DAPHNE repository (see GitHub docs). 3. Create your own local branch: <code>git checkout -b BRANCH_NAME</code>.    <code>BRANCH_NAME</code> should clearly indicate what the branch is about; the recommended pattern is <code>123-some-short-title</code> (where <code>123</code> is the issue number). 4. Add as many commits as you like to your branch, and <code>git push</code> them to your fork.    Use <code>git push --set-upstream origin BRANCH_NAME</code> when you push the first time. 5. If you work longer on your contribution, make sure to get the most recent changes from the upstream (original DAPHNE system repository) from time to time (see GitHub docs). 6. Once you feel ready (for integration or for discussion/feedback), create a pull request on GitHub (see GitHub docs).    Normally, you'll want to ask for integration into <code>base:main</code>, the repo's default branch.    Please choose an expressive title and provide a short description of your changes.    Feel free to mark your pull request \"WIP: \" or \"Draft: \" in the title.    Note that you can add more commits to your pull request after you created it.    Ideally, the changes in the PR contain only the changes you made for that PR,    e.g, by rebasing your branch on top of the target branch. This makes it easier for others to    review your PR. 7. Resolve any open conflicts to the target branch of the PR. 8. You receive feedback on your proposed contribution.    You may be asked to apply certain changes, or we might apply straightforward adjustments ourselves before the integration. 9. If it looks good (potentially after some help), your contribution becomes a part of DAPHNE.</p>"},{"location":"development/Contributing/#experienced-daphne-contributors-collaborators","title":"Experienced DAPHNE Contributors (Collaborators)","text":"<p>We appreciate continued commitment to the DAPHNE system. Thus, frequent contributors can become collaborators on GitHub. Currently, this requires at least three non-trivial contributions to the system. Collaborators have direct write access to all branches of the repository, including the main branch.</p> <p>The goal is to make development easier for frequent contributors. Collaborators do not need to create a fork, and do not need to go through pull requests to integrate their changes. At the same time, this freedom comes with certain responsibilities, which are roughly sketched here:</p> <ol> <li>Please follow some simple guidelines when changing the code:</li> <li>Feel free to directly push to the main branch, but be mindful of what you commit, since it will affect everyone.      As a guideline, commits fundamentally changing how certain things work should be announced and discussed first, whereas small changes or changes local to \"your\" component are not critical.</li> <li>But never force push to the main branch, since it can lead to severe inconsistencies in the Git history.</li> <li>Even collaborators may still use pull requests (just like new contributors) to suggest larger changes.      This is also suitable whenever you feel unsure about a change or want to get feedback first.</li> <li>Please engage in the handling of pull requests; especially those affecting the components you are working on.    This includes:</li> <li>reading the code others suggest for integration</li> <li>trying if it works</li> <li>providing constructive and actionable feedback on improving the contribution prior to the integration</li> <li>actually merging a pull request in</li> </ol> <p>Balancing the handling of pull requests is important to keep the development process scalable.</p>"},{"location":"development/Contributing/#code-style","title":"Code Style","text":"<p>Before contributing, please make sure to run <code>clang-format</code> on your C++ (.h and .cpp) files. The codebase is currently formatted with <code>clang-format</code> version <code>18.1.3</code>. This is the default <code>clang-format</code> version when installing via <code>apt</code> on Ubuntu 24.04, and can easily be installed via <code>python -mpip install clang-format==18.1.3</code> on other systems. We provide a <code>.clang-format</code> file at the root of the repository. Most text editors and IDEs will have some kind of integration for detecting that file and automatically applying <code>clang-format</code>. <code>git-clang-format</code> can be used to format staged files. For more information about <code>clang-format</code>, <code>git-clang-format</code> and text editor integration, please see ClangFormat.</p>"},{"location":"development/ExtendingDistributedRuntime/","title":"Extending the DAPHNE Distributed Runtime","text":"<p>This doc is about implementing and extending the distributed runtime. This focuses on helping the Daphne developer understand the building blocks of the distributed runtime, both of the Daphne coordinator as well as the Daphne distributed worker. If you want to learn how to execute Daphne scripts on the distributed runtime, please see here.</p>"},{"location":"development/ExtendingDistributedRuntime/#background","title":"Background","text":"<p>Distributed runtime works similar to the vectorized engine. Compiler passes create pipelines by merging multiple operations. This allows Daphne to split data across multiple workers (nodes) and let them execute a pipeline in parallel. Daphne distributed runtime code can be found at <code>src/runtime/distributed</code> where it is split into two main parts. The coordinator and the worker.</p> <p>Daphne distributed runtime works on an hierarchical approach.</p> <ol> <li>Workers are not aware of the total execution. They handle a single task-pipeline and return outputs, one pipeline at a time.</li> <li>Workers are only aware of their chunk of data.</li> </ol>"},{"location":"development/ExtendingDistributedRuntime/#coordinator","title":"Coordinator","text":"<p>The coordinator is responsible for distributing data and broadcasting the IR code fragment, that is the task-pipeline. Each worker receives and compiles the IR optimizing it for it's local hardware. Coordinator code can be found at:</p> <pre><code>src/runtime/distributed/coordinator/kernels\n</code></pre>"},{"location":"development/ExtendingDistributedRuntime/#distributed-wrapper","title":"Distributed Wrapper","text":"<p><code>DistributedWrapper.h</code> kernel contains the Distributed wrapper class which is the main entry point for a distributed pipeline on the coordinator:</p> <pre><code>void DistributedWrapper::execute(\n                const char *mlirCode,       // The IR code fragment\n                DT ***res,                  // An array of pipeline outputs\n                const Structure **inputs,   // Array of pipeline inputs\n                size_t numInputs,           // number of inputs\n                size_t numOutputs,          // number of outputs\n                int64_t *outRows,           // number of Rows for each pipeline output\n                int64_t *outCols,           // number of Columns for each pipeline output\n                VectorSplit *splits,        // Compiler hints on how each input should be split\n                VectorCombine *combines)    // Compiler hints on how each output should be combined\n</code></pre> <p>Using the hints <code>splits</code> provided by the compiler we determine whether an input should be Distributed/Scattered (by rows or columns) or Broadcasted. Depending on which we call the corresponding kernel (<code>Distribute.h</code> or <code>Broadcast.h</code>, more below) which is then responsible for transferring data to the workers. <code>DistributedCompute.h</code> kernel is then called in order to broadcast the IR code fragment and start the actual computation. Finally, <code>DistributedCollect.h</code> kernel collects the final results (pipeline outputs).</p> <p><code>Distribute.h</code>, <code>Broadcast.h</code>, <code>DistributedCompute.h</code> and <code>DistributedCollect.h</code> similar to local runtime kernels use C++ template meta programming (more on how we utilize C++ templates on the local runtime here). Since Daphne supports many different distributed backends (e.g. gRPC, MPI, etc.) we can not provide a fully generic code that would work for all implementations. Thus we specialize each template for each distributed backend we want to support.</p> <p>The Daphne developer can work on a new distributed backend by simply providing a new template specialization of these 4 kernels with a different distributed backend.</p>"},{"location":"development/ExtendingDistributedRuntime/#distributed-backends","title":"Distributed backends","text":"<p>Daphne supports multiple different devices (GPUs, distributed nodes, FPGAs etc.). Because of that all Daphne objects (e.g. matrices) require tracking where their data is stored. That is why each Daphne object contains meta data providing that information. Below you can see a list of all the different devices a Daphne object can be allocated to.</p> <p>Please note that not all of them are supported yet and we might add more in the future.</p> <pre><code>enum class ALLOCATION_TYPE {\n    DIST_GRPC,\n    DIST_OPENMPI,\n    DIST_SPARK,\n    GPU_CUDA,\n    GPU_HIP,\n    HOST,\n    HOST_PINNED_CUDA,\n    FPGA_INT, // Intel\n    FPGA_XLX, // Xilinx\n    ONEAPI, // probably need separate ones for CPU/GPU/FPGA\n    NUM_ALLOC_TYPES\n};\n</code></pre> <p>As we described above, each kernel is partially specialized for each distributed backend. We specialize the templated distributed kernels for each distributed allocation type in the <code>enum class</code> shown above (e.g. <code>DIST_GRPC</code>).</p>"},{"location":"development/ExtendingDistributedRuntime/#templated-distributed-kernels","title":"Templated distributed kernels","text":"<p>Each kernel is responsible for two things.</p> <ol> <li>Handling the communication part. That is sending data to the workers.</li> <li>Updating the meta data. That is populating the meta data of an object which can then lead to reduced communication (if an object is already placed on nodes, we don't need to re-send it).</li> </ol> <p>You can find more on meta data implementation here.</p> <p>Below we see <code>Broadcast.h</code> templated kernel, along with it's gRPC specialization. <code>DT</code> is the type of the object being broadcasted.</p> <pre><code>// ****************************************************************************\n// Struct for partial template specialization\n// ****************************************************************************\n\n// DT is object type\n// AT is allocation type (distributed backend)\ntemplate&lt;ALLOCATION_TYPE AT, class DT&gt;\nstruct Broadcast {\n    static void apply(DT *mat, bool isScalar, DCTX(dctx)) = delete;\n};\n\n// ****************************************************************************\n// Convenience function\n// ****************************************************************************\n\ntemplate&lt;ALLOCATION_TYPE AT, class DT&gt;\nvoid broadcast(DT *&amp;mat, bool isScalar, DCTX(dctx))\n{\n    Broadcast&lt;AT, DT&gt;::apply(mat, isScalar, dctx);\n}\n\n// ****************************************************************************\n// (Partial) template specializations for different distributed backends\n// ****************************************************************************\n\n// ----------------------------------------------------------------------------\n// GRPC\n// ----------------------------------------------------------------------------\n\ntemplate&lt;class DT&gt;\nstruct Broadcast&lt;ALLOCATION_TYPE::DIST_GRPC, DT&gt;\n{\n    // Specific gRPC implementation...\n    ...\n</code></pre> <p>So for example if we wanted to add broadcast support for MPI all we need to do is provide the partial template specialization of Broadcast kernel for MPI.</p> <pre><code>// ----------------------------------------------------------------------------\n// MPI\n// ----------------------------------------------------------------------------\n\ntemplate&lt;class DT&gt;\nstruct Broadcast&lt;ALLOCATION_TYPE::DIST_MPI, DT&gt; \n{\n    // Specific MPI implementation...\n    ...\n</code></pre> <p>For now selection of a distributed backend is hardcoded here.</p>"},{"location":"development/ExtendingDistributedRuntime/#distributed-worker","title":"Distributed Worker","text":"<p>Worker code can be found here:</p> <pre><code>src/runtime/distributed/worker\n</code></pre> <p><code>WorkerImpl.h</code> contains the <code>WorkerImpl</code> class which provides all the logic for the distributed worker. There are 3 important methods in this class:</p> <ul> <li>The Store method, which stores an object in memory and returns an identifier.</li> <li>The Compute method, which receives the IR code fragment along with identifier of inputs, computes the pipeline and returns identifiers of pipeline outputs.</li> <li>And the Transfer method, which is used to return an object using an identifier.</li> </ul> <pre><code>/**\n * @brief Stores a matrix at worker's memory\n * \n * @param mat Structure * obj to store\n * @return StoredInfo Information regarding stored object (identifier, numRows, numCols)\n */\nStoredInfo Store(DT *mat) ;\n\n/**\n * @brief Computes a pipeline\n * \n * @param outputs vector to populate with results of the pipeline (identifier, numRows/cols, etc.)\n * @param inputs vector with inputs of pipeline (identifiers to use, etc.)\n * @param mlirCode mlir code fragment\n * @return WorkerImpl::Status tells us if everything went fine, with an optional error message\n */\nStatus Compute(vector&lt;StoredInfo&gt; *outputs, vector&lt;StoredInfo&gt; inputs, string mlirCode) ;\n\n/**\n * @brief Returns a matrix stored in worker's memory\n * \n * @param storedInfo Information regarding stored object (identifier, numRows, numCols)\n * @return Structure* Returns object\n */\nStructure * Transfer(StoredInfo storedInfo);\n</code></pre> <p>The developer can provide an implementation for a distributed worker by deriving <code>WorkerImpl</code> class. The derived class handles all the communication using the preferred distributed backend and invokes the parent methods for the logic. You can find the gRPC implementation of the distributed worker here:</p> <pre><code>src/runtime/distributed/worker/WorkerImplGrpc.h/.cpp\n</code></pre> <p><code>main.cpp</code> is the entry point of the distributed worker. A distributed implementation is created using a pointer to the parent class <code>WorkerImpl</code>. The distributed node then blocks and waits for the coordinator to send a request by invoking the virtual method:</p> <pre><code>virtual void Wait() { };\n</code></pre> <p>Each distributed worker implementation needs to override this method and implement it in order to wait for requests.</p>"},{"location":"development/ExtendingSchedulingKnobs/","title":"Extending DAPHNE with More Scheduling Knobs","text":"<p>This document focuses on how a daphne developer may extend the DAPHNE system by adding new scheduling techniques</p>"},{"location":"development/ExtendingSchedulingKnobs/#guidelines","title":"Guidelines","text":"<p>The daphne developer should consider the following files for adding a new scheduling technique</p> <ol> <li>src/runtime/local/vectorized/LoadPartitioning.h</li> <li>src/api/cli/daphne.cpp</li> </ol> <p>Adding the actual code of the technique:</p> <p>The first file <code>LoadPartitioning.h</code> contains the implementation of the currently supported scheduling techniques, i.e., the current version of DAPHNE uses self-scheduling techniques to partition the tasks. Also, it uses the self-scheduling principle for executing the tasks. For more details, please visit Scheduler design for tasks and pipelines.</p> <p>In this file, the developer should change two things:</p> <ol> <li> <p>The enumeration that is called <code>SelfSchedulingScheme</code>. The developer will have to add a name for the new technique, e.g., <code>MYTECH</code></p> <pre><code>enum SelfSchedulingScheme { STATIC=0, SS, GSS, TSS, FAC2, TFSS, FISS, VISS, PLS, MSTATIC, MFSC, PSS, MYTECH };\n</code></pre> </li> <li> <p>The function that is called <code>getNextChunk()</code>. This function has a switch case that selects the mathematical formula that corresponds to the chosen scheduling method. The developer has to add a new case to handle the new technique.</p> <pre><code>uint64_t getNextChunk(){\n   //...\n   switch (schedulingMethod){\n       //...\n       //Only the following part is what the developer has to add. The rest remains the same\n       case MYTECH:{ // the new technique\n           chunkSize= FORMULA;//Some Formula to calculate the chunksize (partition size)\n           break; \n       }\n       //...\n   }\n   //...\n   return chunkSize;\n}\n</code></pre> </li> </ol> <p>Enabling the selection of the newly added technique:</p> <p>The second file <code>daphne.cpp</code> contains the code that parses the command line arguments and passes them to the DAPHNE compiler and runtime. The developer has to add the new technique as a vaild option. Otherwise, the developer will not be able to use the newly added technique. There is a variable called <code>taskPartitioningScheme</code> and it is of type <code>opt&lt;SelfSchedulingScheme&gt;</code>. The developer should extend the declaration of <code>opt&lt;SelfSchedulingScheme&gt;</code> as follows:</p> <pre><code>opt&lt;SelfSchedulingScheme&gt; taskPartitioningScheme(\n        cat(daphneOptions), desc(\"Choose task partitioning scheme:\"),\n        values(\n            clEnumVal(STATIC , \"Static (default)\"),\n            clEnumVal(SS, \"Self-scheduling\"),\n            clEnumVal(GSS, \"Guided self-scheduling\"),\n            clEnumVal(TSS, \"Trapezoid self-scheduling\"),\n            clEnumVal(FAC2, \"Factoring self-scheduling\"),\n            clEnumVal(TFSS, \"Trapezoid Factoring self-scheduling\"),\n            clEnumVal(FISS, \"Fixed-increase self-scheduling\"),\n            clEnumVal(VISS, \"Variable-increase self-scheduling\"),\n            clEnumVal(PLS, \"Performance loop-based self-scheduling\"),\n            clEnumVal(MSTATIC, \"Modified version of Static, i.e., instead of n/p, it uses n/(4*p) where n is number of tasks and p is number of threads\"),\n            clEnumVal(MFSC, \"Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC\"),\n            clEnumVal(PSS, \"Probabilistic self-scheduling\"),\n            clEnumVal(MYTECH, \"some meaningful description to the abbreviation of the new technique\")\n        )\n); \n</code></pre> <p>Usage of the new technique:</p> <p>Daphne developers may now pass the new technique as an option when they execute a DaphneDSL script.</p> <pre><code>daphne --vec --MYTECH --grain-size 10 --num-threads 4 --PERCPU --SEQPRI --hyperthreading --debug-mt my_script.daphne\n</code></pre> <p>In this example, the daphne system will execute <code>my_script.daphne</code>  with the following configuration:</p> <ol> <li>the vectorized engine is enabled due to <code>--vec</code></li> <li>the DAPHNE runtime will use MYTECH for task partitioning due to <code>--MYTECH</code></li> <li>the minimum partition size will be 10 due to <code>--grain-size 10</code></li> <li>the vectorized engine will use 4 threads due to <code>--num-threads 4</code></li> <li>work stealing will be used with a separate queue for each CPU due to <code>--PERCPU</code></li> <li>the work stealing victim selection will be sequential prioritized due to <code>--SEQPRI</code></li> <li>the rows will be evenly distributed before the scheduling technique is applied due to <code>--pre-partition</code></li> <li>the CPU workers will be pinned to CPU cores due to <code>--pin-workers</code></li> <li>if the number of threads were not specified the number of logical CPU cores would be used (instead of physical CPU cores) due to <code>--hyperthreading</code></li> <li>Debugging information related to the multithreading of vectorizable operations will be printed due to <code>--debug-mt</code></li> </ol>"},{"location":"development/HandlingPRs/","title":"Pull Request (PR) Guideline","text":""},{"location":"development/HandlingPRs/#terminology","title":"Terminology","text":"<ul> <li>Contributor: the person who wants to contribute code by opening a PR</li> <li>Collaborator: a person who has the right to merge a branch into main (besides other rights) (official collaborator status on GitHub)</li> <li>Reviewer: a person who provides feedback on the contribution</li> </ul>"},{"location":"development/HandlingPRs/#disclaimer","title":"Disclaimer","text":"<p>These guidelines are mainly for DAPHNE collaborators. However, they could also be interesting for contributors to (a) understand how we handle PRs, and (b) learn which things we check, so they can try to prepare their contribution to speed up the review/merge procedure.</p> <p>Feel free to suggest changes to these guidelines by opening an issue or a pull request if you feel something is missing, could be improved, needs further clarification, etc.</p>"},{"location":"development/HandlingPRs/#goals-of-these-guidelines","title":"Goals of these Guidelines","text":"<ul> <li>Merge useful contributions (not necessarily perfect ones) into main without unnecessary delay</li> <li>Guarantee a certain quality level, especially since many people are working with the main branch</li> <li>Balance the load for handling PRs among collaborators</li> </ul>"},{"location":"development/HandlingPRs/#pr-reviewmerging-procedure","title":"PR Review/Merging Procedure","text":""},{"location":"development/HandlingPRs/#pr-creation","title":"PR creation","text":"<p>The contributor creates the PR.</p> <ul> <li>if the PR is marked as a draft, it is handled by an informal discussion depending on concrete questions by the contributor; if there are none, the PR is left alone for now</li> <li>if the PR is not marked as a draft, the review/merge procedure continues</li> </ul>"},{"location":"development/HandlingPRs/#initial-response-and-reviewer-assignment","title":"Initial response and reviewer assignment","text":"<p>The DAPHNE collaborators provide an initial response and assign one (or multiple) reviewers (usually from among themselves, but can also be non-collaborators).</p> <ul> <li>Initial response</li> <li>ideally within a few working days after the PR was opened</li> <li>thank contributor for the contribution</li> <li>have a quick glance to decide if the contribution is relevant (default: yes)</li> <li>Reviewer selection</li> <li>any collaborator (or non-collaborator) may volunteer</li> <li>collaborators may select reviewer in a discussion (use @mentions)</li> <li>who qualifies as a reviewer<ul> <li>collaborator experienced with the respective part of the code</li> <li>collaborator mentoring the contributor (e.g., in case of undergrad students)</li> <li>collaborator who wants to learn more about the respective part of the code base</li> <li>any other collaborator to balance the reviewing load among collaborators</li> </ul> </li> <li>there may be multiple reviewers</li> <li>Assignment of the reviewer(s)</li> <li>on GitHub</li> <li>ideally, reviewer(s) should communicate when review can be expected (based on their availability and urgency of the PR)</li> </ul>"},{"location":"development/HandlingPRs/#rounds-of-feedback-and-response","title":"Rounds of feedback and response","text":"<p>If necessary, the reviewer(s) and the contributor prepare the contribution for a merge by multiple (but ideally not more than one) rounds of feedback and response.</p> <p>Reviewer examines the contribution:</p> <ul> <li>read the code, look at the diff<ul> <li>level of detail<ul> <li>focus on integration into overall code base</li> <li>if really special topic, which reviewer is not familiar with, then no deep review possible/required</li> <li>especially for \"good first issues\": read code in detail</li> <li>be the stricter the more central the code is (the more people are affected by it)</li> </ul> </li> <li>clarify relation of PR to issue<ul> <li>if PR states to address an issue, check if it really does so</li> <li>it can be okay if a PR addresses just a part of a complex issue (if contribution still makes sense)</li> <li>briefly check if PR addresses further issues (if so, also mention that in feedback and commit message later)</li> <li>PR does not need to address an issue, but if it doesn't, check if contribution really belongs to the DAPHNE system itself (there might be useful contributions which should better reside in a separate repo, e.g., for the usage of DAPHNE, tools around DAPHNE, experiments/reproducibility, ...)</li> </ul> </li> <li>contribution DOs<ul> <li>readable code</li> <li>necessary API changes should be reflected in the documentation (e.g., DaphneDSL/DaphneLib, command line arguments, environment variables, ...)</li> <li>appropriate test cases (should be present and make sense, test expected cases, corner cases, and exceptional cases)</li> <li>comments/explanations in central pieces of the code</li> <li>meaningful placement of the contributed code in the directory hierarchy</li> <li>correct integration of additional third-party code (reasonable placement in directory hierarchy, license compatibility, ...)</li> <li>DAPHNE license header (should be checked automatically)</li> <li>...</li> </ul> </li> <li>contributions DON'Ts<ul> <li>obvious bugs (also think of corner cases)</li> <li>changes unrelated to the PR (should be addressed in separate PRs)</li> <li>significant performance degradation (in terms of building as well as executing DAPHNE) (such checks should be automated)</li> <li>files that should not be committed, because they are not useful to others, too large, or can be generated from other files (e.g., IDE project files, output logs, executables, container images, empty files, unrelated files, experimental results, diagrams, unused files, auto-generated files, ...)</li> <li>unnecessary API changes (e.g., DaphneDSL/DaphneLib, command line arguments, possibly environment variables, ...)</li> <li>reimplementation of things we already have or that should better be imported from some third-party library</li> <li>breaking existing code, formatting, tests, documentation, etc.</li> <li>confidential information (usernames, passwords, ...)</li> <li>paths on local system of contributor</li> <li>misleading comments</li> <li>copy-paste errors</li> <li>extreme code duplication</li> <li>useless prints (might even fail test cases)</li> <li>whitespace changes that unnecessarily blow up the diff (especially in files that otherwise have no changes)</li> <li>...</li> </ul> </li> <li>code style<ul> <li>don't be strict as long as we don't have a clearly defined code style which can be enforced automatically</li> <li>but watch out for things that make code hard to read, e.g.<ul> <li>wrong indentation</li> <li>lots of commented out lines (especially artifacts from development/debugging)</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>try out the code</p> <ul> <li> <p>check out the branch</p> <ul> <li> <p>If the contribution originates from a github fork, these steps will help to clone the PR's state into a branch of your working copy (example taken from PR #415):</p> <ul> <li> <p>Make sure your local copy of the main branch is up to date</p> <pre><code>git checkout main\ngit pull\n</code></pre> </li> <li> <p>Create a branch for the PR changes and pull them on top of that local branch</p> <pre><code>git checkout -b akroviakov-415-densemat-strings-kernels main\ngit pull git@github.com:akroviakov/daphne.git 415-densemat-strings-kernels\n</code></pre> </li> <li> <p>Once you have resolved all potential merge conflicts, you will have to do a merge commit. To get rid of this and ensure a linear history, start an interactive rebase from the last commit in main. In that process all non-relevant commits can be squashed and meaningful commit messages created if necessary.</p> <pre><code>git rebase -i &lt;commit hash of last commit in main&gt; \n</code></pre> </li> <li> <p>Once everything is cleaned up in the local PR branch, switch back to main and merge from the PR branch. This should yield clean commits on top of main because of the prior rebasing.</p> <pre><code>git checkout main\ngit merge akroviakov-415-densemat-strings-kernels\ngit push origin main\n</code></pre> </li> </ul> </li> </ul> </li> <li> <p>check if the code builds at all (should be checked automatically)</p> </li> <li>check if there are compiler warnings (should be fixed) (should be checked automatically)</li> <li>check if the test cases pass (should be checked automatically)</li> <li>whether these checks succeed or fail may be platform-specific<ul> <li>TODO: think about that aspect in more detail</li> </ul> </li> </ul> </li> </ul> <p>Reviewer fixes minor problems:</p> <ul> <li>things that are quicker to fix, than to communicate back and forth<ul> <li>typos and grammar mistakes (in variable names, status/error messages, comments, ...)</li> <li>obvious minor bugs</li> <li>wording/terminology (especially in comments)</li> </ul> </li> <li>add separate commit(s) on PR's branch<ul> <li>to clearly separate these amendments from original contribution</li> </ul> </li> <li>changes should be briefly mentioned/summarized in feedback<ul> <li>to document that something was changed</li> <li>to notify contributor (ideally, they look at the changes in detail, learn from them, and do it better the next time)</li> </ul> </li> <li>may be done at any point in time (before or after requested changes have been addressed by contributor)</li> </ul> <p>Reviewer provides feedback on the contribution:</p> <ul> <li>identify requests for concrete changes from contributor<ul> <li>things that the reviewer cannot fix within a few minutes<ul> <li>more general corrections, refactoring, ...</li> <li>more difficult bugs</li> </ul> </li> <li>suitable for requesting mandatory changes<ul> <li>in general, things that must to be done before the contribution can be merged, because there will be problems of some kind otherwise</li> <li>bugs (functional, non-functional/performance)</li> <li>things that could hinder others (e.g., unsolicited refactoring)</li> <li>simplifications that make the code dramatically shorter and/or easier to read/maintain, and are straightforward to achieve</li> <li>potentially also things that are in conflict with upcoming other PRs</li> </ul> </li> <li>not suitable for requesting mandatory changes<ul> <li>nice-to-have extensions of the feature: anything that could be done in a separate PR without leaving the code base in a bad state should not be a requirement for merging in at least a meaningful part of a feature</li> <li>the contribution of a PR does not need to be perfect, but it should bring us forward</li> <li>requests based on personal opinions which cannot be convincingly justified (e.g., implementing a feature in a different way as a matter of taste) (but might be okay for consistency)</li> <li>top efficiency</li> <li>such points can become follow-up issues and/or todos in the code (feel free to include issue number in todo)</li> </ul> </li> </ul> </li> <li>reviewer gives feedback by commenting on the PR<ul> <li>use the form on GitHub (\"Files changed\"-tab -&gt; \"Review changes\": select \"Approve\" or \"Request changes\")</li> <li>things to change should be enumerated clearly in the feedback on the PR (ideally numbered list or bullet points)<ul> <li>briefly explain why these requested changes are necessary</li> <li>ideally provide some rough hints on how they could be addressed (but contributor is responsible for figuring out the details)</li> </ul> </li> <li>optional extensions can be added as suggestions (some contributors are very eager), but clearly say that they are not required before merging</li> <li>feedback should be polite, actionable, concrete, and constructive</li> </ul> </li> </ul> <p>Contributor addresses reviewer comments:</p> <ul> <li>ideally, the contributor is willing to do this</li> <li>otherwise (and especially for new contributors, for whom we want to lower the barrier of entry), the reviewer or someone else should take charge of this, if possible</li> </ul>"},{"location":"development/HandlingPRs/#once-the-contribution-is-ready-a-collaborator-merges-the-pr","title":"Once the contribution is ready, a collaborator merges the PR","text":"<ul> <li>can be done by the reviewer or any collaborator</li> <li>we want to keep a clean history on the main branch (and remember never to force-push to main)<ul> <li>makes it easier for others to keep track of the changes that happen</li> <li>PR's branch might have untidy history with lots of commits for implementing the contribution and addressing reviewer comments; that should not end up on main</li> </ul> </li> <li>typically, we want to rebase the PR branch on main, which may require resolving conflicts</li> <li>an example of how to use git on the command line is given in try out the code in section 3.1 above</li> <li>case A) if PR is conceptually one contribution<ul> <li>on GitHub:<ul> <li>\"Conversation\"-tab: use \"Squash and merge\"-button (select this mode if necessary)</li> </ul> </li> <li>on the command line:<ul> <li>rebase and squash as required in the locally checked out PR branch</li> <li>force-push to the PR branch (but never force-push to main)</li> <li>locally switch to main and merge the PR branch</li> <li>push to main</li> <li>note: this procedure also ensures that the PR is shown as merged (not as closed) in GitHub later</li> </ul> </li> <li>this will place a single new commit onto main because you rebased/squashed in the PR branch first</li> </ul> </li> <li>case B) if PR consists of individual meaningful commits of a larger feature (with meaningful commit messages)<ul> <li>on GitHub:<ul> <li>\"Conversation\"-tab: use \"Rebase and merge\"-button (select this mode if necessary)</li> </ul> </li> <li>on the command line:<ul> <li>rebase as required in the locally checked out PR branch</li> <li>force-push to the PR branch (but never force-push to main)</li> <li>locally switch to main and merge the PR branch</li> <li>push to main</li> <li>note: this procedure also ensures that the PR is shown as merged (not as closed) in GitHub later</li> </ul> </li> <li>this will place the new commits onto main because you rebased in the PR branch first</li> </ul> </li> <li>in any case<ul> <li>enter a meaningful commit message (what and why, closed issues, ...)<ul> <li>ideally reuse the initial description of the PR</li> <li>in case of squashing (case A above): please remove the unnecessarily long generated commit message)</li> <li>TODO: commit messages should be a separate item in the developer documentation</li> </ul> </li> <li>authorship<ul> <li>if multiple authors edited the branch: choose one of them as the main author (after squashing in GitHub it should be the person who opened the PR); more authors can be added by adding <code>Co-authored-by:  NAME NAME@EXAMPLE.COM</code> after two blank lines at the end of the commit message (one for each co-author).</li> <li>very often, reviewers may have made minor fixes, but should refrain from adding themselves as co-authors (prefer to give full credit for the contribution to the initial contributor, unless the reviewer's contribution was significant)</li> </ul> </li> </ul> </li> </ul>"},{"location":"development/HandlingPRs/#creation-of-follow-up-issues-optional","title":"Creation of follow-up issues (optional)","text":"<ul> <li>things that were left out</li> <li>nice-to-haves</li> <li>functional, non-functional, documentation, tests</li> </ul>"},{"location":"development/HandlingPRs/#inviting-the-contributor-as-a-collaborator-conditional","title":"Inviting the contributor as a collaborator (conditional)","text":"<p>If this contributor has made enough non-trivial contributions of good quality (currently, we require three), he/she should be invited as a collaborator on GitHub.</p>"},{"location":"development/HandlingPRs/#more-hints","title":"More Hints","text":"<ul> <li>reviewer's time is precious, don't hesitate to request changes from the contributor (but keep in mind that we want to lower the barrier of entry for new contributors)</li> <li>avoid making a PR too large<ul> <li>makes it difficult to context-switch into it again and again</li> <li>makes overall changes hard to understand and diffs hard to read</li> </ul> </li> <li>whenever in doubt: use discussion features on GitHub to get others' opinions</li> </ul>"},{"location":"development/HandlingPRs/#communication","title":"Communication","text":"<p>We want to facilitate an open and inclusive atmosphere, which should be reflected in the way we communicate.</p> <ul> <li> <p>TODO: we should set up concrete guidelines for that, but that's actually a separate topic</p> </li> <li> <p>always be polite and respectful to others</p> </li> <li>keep the conversation constructive</li> <li>keep in mind the background of other persons<ul> <li>experienced DAPHNE collaborator or new contributor</li> <li>level of technical experience</li> <li>English language skills</li> </ul> </li> <li>...</li> </ul>"},{"location":"development/ImplementBuiltinKernel/","title":"Implementing a Built-in Kernel for a DaphneIR Operation","text":""},{"location":"development/ImplementBuiltinKernel/#background","title":"Background","text":"<p>(Almost) every DaphneIR operation will be backed by a kernel (= physical operator) at run-time. Extensibility w.r.t. kernels is one on the core goals of the DAPHNE system. It shall be easy for a user to add a custom kernel. However, the system will offer a full set of built-in kernels so that all DaphneIR operations can be used out-of-the-box.</p>"},{"location":"development/ImplementBuiltinKernel/#scope","title":"Scope","text":"<p>This document focuses on:</p> <ul> <li>default built-in kernels (not custom/external kernels)</li> <li>implementations for CPU (not HW accelerators)</li> <li>local execution (not distributed)</li> <li>pre-compiled kernels (not on-the-fly code generation and operator fusion)</li> </ul> <p>We will extend the system to those more advanced aspects step by step.</p>"},{"location":"development/ImplementBuiltinKernel/#guidelines","title":"Guidelines","text":"<p>The following are some guidelines and best practices (rather than strict rules) for implementing built-in kernels. As we proceed, we might adapt these guidelines step by step. The goal is to clarify how to implement a built-in kernel and what the thoughts behind it are. This is meant as a proposal, comments/suggestions are always welcome.</p> <p>Integration into the directory tree:</p> <p>The implementations of built-in kernels shall reside in <code>src/runtime/local/kernels</code>. By default, one C++ header-file should be used for all specializations of a kernel. Depending on the amount of code, the separation into multiple header files is also possible. At least, we should rather not mix kernels of different DaphneIR operations in one header file.</p> <p>Interfaces:</p> <p>Technically, a kernel is a C++ function taking one or more data objects (matrices, frames) and/or scalars as input and returning one or more data objects and/or scalars as output. As a central idea, (almost) all DaphneIR operations should be able to process (almost) all kinds of Daphne data structures, whereby these could have any Daphne value type. For example, elementwise binary operations (<code>+</code>, <code>*</code>, ...) should be applicable to <code>DenseMatrix</code> of <code>double</code>, <code>uint32_t</code>, etc. as well as to <code>CSRMatrix</code> of <code>double</code>, <code>uint32_t</code>, etc. and so on. This type flexibility motivates the use of C++ template metaprogramming.</p> <p>Thus, a kernel is a template function with:</p> <ul> <li>template parameters</li> <li>one for the type of each input/output data object (and scalar, if necessary)</li> <li>inputs</li> <li>of type <code>const DT *</code> for data objects (whereby <code>DT</code> is a particular C++ type such as <code>DenseMatrix&lt;double&gt;</code>)</li> <li>of type <code>VT</code> for scalars of some value type <code>VT</code></li> <li>outputs</li> <li>as a return value, in case of a single scalar</li> <li>as a parameter of type <code>DT *&amp;</code> in case of data objects (all output parameters before all input parameters)</li> </ul> <p>The reason for passing output data objects as parameters is that this mechanism could be used for efficient update-in-place operations.</p> <p>The declaration of a kernel function could look as follows:</p> <pre><code>template&lt;class DTRes, class DTArg, typename VT&gt;\nvoid someOp(DTRes *&amp; res, const DTArg * arg, VT otherArg);\n</code></pre> <p>For most DaphneIR operations, it will not be possible to define the template as a fully generic algorithm. For example, the algorithms for processing different data type implementations (like <code>DenseMatrix</code> and <code>CSRMatrix</code>) will usually differ significantly. Thus, we will usually want to specialize the template. At the same time, it will be possible to implement algorithms generically w.r.t. the value type. Thus, we will usually not need to specialize for that. Hence, we will often use partial template specialization. Unfortunately, this is not possible for functions in C++. Therefore, we use a very typical workaround: In addition to the kernel template function, we declare a template struct/class with the same template parameters as the kernel function. By convention, let us call this struct like the kernel function, but starting with upper case letter. This struct/class has a single static member function with the same results and parameters as the kernel function. By convention, let us always call this function <code>apply</code>. Then, the kernel function always forwards the processing to the <code>apply</code>-function of the correct template instantiation of the kernel struct. Instead of partially specializing the kernel function, we partially specialize the kernel struct and implement the respective algorithm in that specialization's <code>apply</code>-function. Finally, callers will call an instantiation of the kernel template function. C++ templates offer many ways to express such (partial) specializations, some examples are given below:</p> <pre><code>// Kernel struct to enable partial template specialization.\ntemplate&lt;class DTRes, class DTArg, typename VT&gt;\nstruct SomeOp {\n    static void apply(DTRes *&amp; res, const DTArg * arg, VT otherArg) = delete;\n};\n\n// Kernel function to be called from outside.\ntemplate&lt;class DTRes, class DTArg, typename VT&gt;\nvoid someOp(DTRes *&amp; res, const DTArg * arg, VT otherArg) {\n    SomeOp&lt;DTRes, DTArg, VT&gt;::apply(res, arg, otherArg);\n}\n\n// (Partial) specializations of the kernel struct (some examples).\n\n// E.g. for DenseMatrix of any value type.\ntemplate&lt;typename VT&gt;\nstruct SomeOp&lt;DenseMatrix&lt;VT&gt;, DenseMatrix&lt;VT&gt;, VT&gt; {\n    static void apply(DenseMatrix&lt;VT&gt; *&amp; res, const DenseMatrix&lt;VT&gt; * arg, VT otherArg) {\n        // do something\n    }\n};\n\n// E.g. for DenseMatrix and CSRMatrix of the same value type.\ntemplate&lt;typename VT&gt;\nstruct SomeOp&lt;DenseMatrix&lt;VT&gt;, CSRMatrix&lt;VT&gt;, VT&gt; {\n    static void apply(DenseMatrix&lt;VT&gt; *&amp; res, const CSRMatrix&lt;VT&gt; * arg, VT otherArg) {\n        // do something\n    }\n};\n\n// E.g. for DenseMatrix of independent value types.\ntemplate&lt;typename VTRes, typename VTArg, typename VTOtherArg&gt;\nstruct SomeOp&lt;DenseMatrix&lt;VTRes&gt;, DenseMatrix&lt;VTArg&gt;, VTOtherArg&gt; {\n    static void apply(DenseMatrix&lt;VTRes&gt; *&amp; res, const DenseMatrix&lt;VTArg&gt; * arg, VTOtherArg otherArg) {\n        // do something\n    }\n};\n\n// E.g. for super-class Matrix of the same value type.\ntemplate&lt;typename VT&gt;\nstruct SomeOp&lt;Matrix&lt;VT&gt;, Matrix&lt;VT&gt;, VT&gt; {\n    static void apply(Matrix&lt;VT&gt; *&amp; res, const Matrix&lt;VT&gt; * arg, VT otherArg) {\n        // do something\n    }\n};\n\n// E.g. for DenseMatrix&lt;double&gt;, CSRMatrix&lt;float&gt;, and double (full specialization).\ntemplate&lt;&gt;\nstruct SomeOp&lt;DenseMatrix&lt;double&gt;, CSRMatrix&lt;float&gt;, double&gt; {\n    static void apply(DenseMatrix&lt;double&gt; *&amp; res, const CSRMatrix&lt;float&gt; * arg, double otherArg) {\n        // do something\n    }\n};\n</code></pre> <p>Implementation of the <code>apply</code>-functions:</p> <p>As stated above, the <code>apply</code>-function contain the actual implementation of the kernel. Of course, that depends on what the kernel is supposed to do, but there some recurring actions.</p> <ul> <li>Obtaining an output data object</li> </ul> <p>Data objects like matrices and frames cannot be obtained using the <code>new</code>-operator, but must be obtained from the <code>DataObjectFactory</code>, e.g., as follows:</p> <pre><code>auto res = DataObjectFactory::create&lt;CSRMatrix&lt;double&gt;&gt;(3, 4, 6, false);\n</code></pre> <p>Internally, this <code>create</code>-function calls a private constructor of the specified data type implementation; so please have a look at these. - Accessing the input and output data objects</p> <p>For efficiency reasons, accessing the data in way specific to the data type implementation is preferred to generic access method of the super-classes.   That is, if possible, rather use <code>getValues()</code> (for <code>DenseMatrix</code>) or <code>getValues()</code>/<code>colColIdxs()</code>/<code>getRowOffsets()</code> (for <code>CSRMatrix</code>) rather than <code>get()</code>/<code>set()</code>/<code>append()</code> from the super-class <code>Matrix</code>.   The reason is that these generic access methods can incur a lot of unnecessary effort, depending on the data type implementation.   However, in the end it is always a trade-off between performance and code complexity.   For kernels that are rarely used or typically used on small data objects, a simple but inefficient implementation might be okay.   Nevertheless, since the DAPHNE system should be able to handle unexpected scripts efficiently, we should not get too much used to sacrificing efficiency.</p>"},{"location":"development/ImplementBuiltinKernel/#concrete-examples","title":"Concrete Examples","text":"<p>For concrete examples, please have a look at existing kernel implementations in src/runtime/local/kernels. For instance, the following kernels represent some interesting cases:</p> <ul> <li>ewBinarySca works only on scalars.</li> <li>ewBinaryMat works only on matrices.</li> <li>ewBinaryObjSca combines matrix/frame and scalar inputs.</li> <li>matMul delegates to an external library (OpenBLAS).</li> </ul>"},{"location":"development/ImplementBuiltinKernel/#test-cases","title":"Test Cases","text":"<p>Implementing test cases for each kernel is important to reduce the likelihood of bugs, now and after changes to the code base. Please have a look at test cases for existing kernel implementations in test/runtime/local/kernels (surely, these could still be improved).</p>"},{"location":"development/ImplementBuiltinKernel/#error-handling","title":"Error Handling","text":"<p>It is recommended to exceptions such as <code>throw std::runtime_error</code> in a kernel in case the code runs into an unresolvable issue. We catch these exceptions in our surrounding code to the kernel and provide, whenever possible, additional information about the source of the error in the DaphneDSL script.</p>"},{"location":"development/ImplementBuiltinKernel/#experimental-kernels","title":"Experimental Kernels","text":"<p>As an alternative to implementing a new kernel that is directly integrated into DAPHNE, one can also work on kernel implementations using the kernel catalog. These should reside in experimental/op/ where <code>op</code> is the mnemonic of the DaphneIR operation that the kernel is implementing.</p> <p>Experimental kernels are not directly integrated into DAPHNE and are neither compiled nor executed by default. They can be used to test new ideas and provide an easier way of prototyping kernel implementations. One can easily test multiple different implementations of the same DAPHNE kernel using a single DaphneDSL script which calls all the kernel implementations.</p> <p>There are less restrictions put on experimental kernels than on built-in kernels, e.g., they are not tested as part of the CI pipeline. You are also free to introduce new dependencies that are handled by the accompanying <code>Makefile</code> or build script. Testing and dependency management will have to be resolved before the experimental kernel is integrated into DAPHNE as a built-in kernel.</p> <p>Check out Extensions.md for more information on how to implement experimental kernels.</p>"},{"location":"development/Logging/","title":"Logging","text":""},{"location":"development/Logging/#general","title":"General","text":"<p>To write out messages of any kind from DAPHNE internals we use the spdlog library. E.g., not from a user's print() statement but when <code>std::cout &lt;&lt; \"my value: \" &lt;&lt; value &lt;&lt; std::endl;</code> is needed. With spdlog, the previous std::cout example would read like this: <code>spdlog::info(\"my value: {}\", value);</code>. The only difference being that we now need to choose a log level (which is arbitrarily chosen to be info in this case).</p>"},{"location":"development/Logging/#usage","title":"Usage","text":"<ol> <li>Before using the logging functionality, the loggers need to be created and registered. Due to the nature of how singletons work in C++, this has to be done once per binary (e.g., daphne, run_tests, libAllKernels.so, libCUDAKernels.so, etc). For the mentioned binaries this has already been taken care of (either somewhere near the main program entrypoint or via context creation in case of the libs). All setup is handled by the class DaphneLogger (with some extras in ConfigParser).</li> <li> <p>Log messages can be submitted in two forms:</p> <ul> <li><code>spdlog::warn(\"my warning\");</code></li> <li><code>spdlog::get(\"default\")-&gt;warn(\"my warning\");</code></li> </ul> <p>The two statements have the same effect. But while the former is a short form for using the default logger, the latter explicitly chooses the logger via the static <code>get()</code> method. This <code>get()</code> method is to be used with caution as  it involves acquiring a lock, which is to be avoided in performance critical sections of the code. In the initial  implementation there is a logger for runtime kernels provided by the context object to work around this limitation. See the matrix multiplication kernel in <code>src/runtime/local/kernels/MatMult.cpp</code> for example usage. </p> </li> <li> <p>We can have several loggers, which can be configured differently. For example, to control how messages are logged in the CUDA compiler pass <code>MarkCUDAOpsPass</code>, a logger named \"compiler::cuda\" is used. Additionally, avoiding the use of <code>spdlog::get()</code> is demonstrated there. For each used logger, an entry in <code>fallback_loggers</code> (see DaphneLogger.cpp) must exist to prevent crashing when using an unconfigured logger.</p> </li> <li>To configure log levels, formatting and output options, the DaphneUserConfig and ConfigParser have been extended. See an example of this in the <code>UserConfig.json</code> in the root directory of the DAPHNE code base.</li> <li>At the moment, the output options of our logging infrastructure are a bit limited (initial version). A logger currently always emits messages to the console's std-out and optionally to a file if a file name is given in the config.</li> <li>The format of log messages can be customized. See the examples in <code>UserConfig.json</code> and the spdlog documentation.</li> <li>If a logger is called while running unit tests (run_tests executable), make sure to <code>#include &lt;run_tests.h&gt;</code> and call <code>auto dctx = setupContextAndLogger();</code> somewhere before calling the kernel to be tested.</li> <li>Logging can be set to only work from a certain log level and above. This mechanism also serves as a global toggle. To set the log level limit, set <code>{ \"log-level-limit\": \"OFF\" },</code>. In this example, taken from <code>UserConfig.json</code>, all logging is switched off, regardless of configuration.</li> </ol>"},{"location":"development/Logging/#log-levels","title":"Log Levels","text":"<p>These are the available log levels (taken from <code>&lt;spdlog/common.h&gt;</code>). Since it's an enum, their numeric value start from 0 for TRACE to 6 for OFF.</p> <pre><code>namespace level {\nenum level_enum : int\n{\n    trace = SPDLOG_LEVEL_TRACE,\n    debug = SPDLOG_LEVEL_DEBUG,\n    info = SPDLOG_LEVEL_INFO,\n    warn = SPDLOG_LEVEL_WARN,\n    err = SPDLOG_LEVEL_ERROR,\n    critical = SPDLOG_LEVEL_CRITICAL,\n    off = SPDLOG_LEVEL_OFF,\n    n_levels\n};\n</code></pre>"},{"location":"development/Logging/#todo","title":"ToDo","text":"<ul> <li>Guideline when which log level is recommended</li> <li>Toggle console output</li> <li>Other log sinks</li> </ul>"},{"location":"development/Profiling/","title":"Profiling in DAPHNE","text":"<p>For a general overview of the profiling support in DAPHNE see the user profiling documentation.</p> <p>Profiling is implemented via an instrumentation pass that injects calls to the <code>StartProfiling</code> and <code>StopProfiling</code> kernels at the start and end of each block. In turn, the kernels call the PAPI-HL API start and stop functions.</p>"},{"location":"development/Profiling/#known-issues-todo","title":"Known Issues / TODO","text":"<ul> <li>For scripts with multiple blocks (e.g. UDFs), the compiler will generate   profiling passes for each block separately instead of a single script-wide   profiling pass.</li> <li>The profiling kernels should be exposed at the DSL / IR level, so that users   can instrument / profile specific parts of their script. This will also need   compiler cooperation, to make sure that the profiled bock is not rearranged /   fused with other operations.</li> <li>To aid with the development and regression tracking of the runtime, the   profiling kernels could also be extended to support profiling specific   kernels or parts of kernels.</li> </ul>"},{"location":"development/Testing/","title":"Testing","text":"<p>Automatic testing at various levels is an important aspect of any software project. This document describes how we do testing in DAPHNE.</p> <p>DAPHNE's testing philosophy is to mainly focus on end-to-end tests of the entire DAPHNE system, since that is what users care about and what, thus, ultimately matters. However, as it can be hard to trigger all possible uses of DAPHNE's internal C++ components from DaphneDSL or DaphneLib scripts, we also test some of those components in isolation.</p> <p>Thus, there are two kinds of test cases in DAPHNE:</p> <ol> <li>Script-level test cases, which invoke the entire DAPHNE system with some input script file.</li> <li>Unit test cases, which use individual C++ functions/classes of the DAPHNE source code.</li> </ol> <p>Both kinds of test cases are expressed using catch2, a widely used C++ testing framework. We use catch2 such that we don't need to reinvent the wheel for C++ testing (plus, learning about catch2 can help DAPHNE developers also in other software projects). Reading the catch2 documentation is very worthwhile to better understand testing in DAPHNE. We use the same way to define tests for both script-level and unit tests to ensure a consistent test format and a unified test summary.</p> <p>The test suite should be run by developers before committing any changes. To ensure that the code base passes the tests, the test suite is run as part of DAPHNE's CI workflows on GitHub for every commit on the main branch and every pull request to the main branch.</p>"},{"location":"development/Testing/#running-the-test-suite","title":"Running the Test Suite","text":"<p>To run the test suite, execute DAPHNE's test script from the DAPHNE root directory as follows:</p> <pre><code># If you built the dependencies yourself (typical in a native environment):\n./test.sh\n\n# If you want to use pre-built dependencies (typical in a DAPHNE container):\n./build.sh --no-deps --target run_tests &amp;&amp; ./test.sh -nb\n</code></pre> <p>Ideally, the output looks as follows (the concrete numbers may have changed):</p> <pre><code>===============================================================================\nAll tests passed (416199 assertions in 1580 test cases)\n</code></pre>"},{"location":"development/Testing/#background","title":"Background","text":"<p>We try to make running the test suite as simple as possible in the standard case. For that purpose, we provide a test script <code>test.sh</code>, which (1) builds virtually all DAPHNE targets, including the test executable <code>run_tests</code> (a catch2 executable), (2) sets up some environment variables, (3) handles a few DAPHNE-specific options, and (4) invokes the test executable. Due to this sequence of actions, invoking the test executable <code>run_tests</code> directly is discouraged.</p>"},{"location":"development/Testing/#customizing-the-test-execution","title":"Customizing the Test Execution","text":"<p>The test execution can be controlled in several ways. On the one hand, all catch2 arguments are supported, which already yields quite some flexibility (below, we only comment on the most useful ones in DAPHNE). On the other hand, there a few DAPHNE-specific options. Both kinds of options can simply be added after <code>test.sh</code> and can be freely mixed.</p>"},{"location":"development/Testing/#controlling-which-tests-to-run","title":"Controlling which Tests to Run","text":"<p>The set of test cases to run can be tailored in a fine-grained way, and we completely rely on catch2's features here. Each test case has a name and one or multiple tags (think of groups/categories of test cases).</p> <ul> <li> <p>Run all test cases:</p> <pre><code>./test.sh\n</code></pre> <p>If nothing special is specified, all test cases are run.</p> </li> <li> <p>Run a specific test case:</p> <pre><code>./test.sh kmeans\n</code></pre> <p>As our test case names often have slightly technical-looking suffixes (due to <code>TEMPLATE_TEST_CASE</code> and <code>TEMPLATE_PRODUCT_TEST_CASE</code>, see below), it is quite handy to use wildcards:</p> <pre><code>./test.sh matmul*\n</code></pre> </li> <li> <p>Run all test cases with a specific tag:</p> <pre><code>./test.sh [algorithms]\n</code></pre> <p>A list of all test tags can be found in <code>test/tags.h</code>.</p> </li> <li> <p>catch2 also supports complex combinations and set operations on test names and tags.     See the catch2 documentation for details.</p> </li> </ul>"},{"location":"development/Testing/#additional-useful-flags","title":"Additional Useful Flags","text":"<p>See the catch2 documentation for a full reference of all command-line arguments.</p> <ul> <li> <p>Show the progress of the test execution:</p> <pre><code>./test.sh -d yes\n</code></pre> <p>By this catch2 flag, the duration of each test is displayed, which also means some progress indication.</p> </li> </ul>"},{"location":"development/Testing/#daphne-specific-flags","title":"DAPHNE-specific Flags","text":"<p>The test script has several flags that control how the DAPHNE system and the test executable are built through the build script <code>build.sh</code>.</p> <ul> <li><code>--cuda</code>, <code>--fpgaopencl</code>, and <code>--mpi</code> switch on the respective feature of DAPHNE.</li> <li><code>--no-papi</code> switch off the respective feature of DAPHNE.</li> <li><code>--debug</code> builds DAPHNE with debug information.</li> <li><code>-nb</code> or <code>--no-build</code> builds DAPHNE with pre-compiled dependencies (available in the DAPHNE containers).</li> </ul> <p>These flags are merely forwarded to <code>build.sh</code>. Further information can be found there.</p>"},{"location":"development/Testing/#reacting-to-test-failures","title":"Reacting to Test Failures","text":""},{"location":"development/Testing/#tests-fail-in-your-local-development-environment","title":"Tests Fail in Your Local Development Environment","text":"<p>Finding out what's going wrong and how to fix it requires the typical debugging skills, where most developers have their own ways and preferences.</p> <p>Some suggestions in the context of DAPHNE:</p> <ul> <li>Look at the error messages produced by catch2, which indicate the name of the failing test case as well as the assertions that failed (including detailed information like the expected and found values). This output is often sufficient to narrow down the problem.</li> <li>Try to change the code and/or the test case and execute only the affected test case separately (see above) to save time. For script-level test cases, it can be helpful to invoke the script outside the test suite (the scripts typically reside in <code>test/api/cli/</code>, make sure to use the same arguments as in the test suite).</li> <li>Use DAPHNE's logger or custom print-outs to generate more debug output.</li> <li>Use a debugger.</li> </ul>"},{"location":"development/Testing/#tests-fail-in-the-ci-workflow-even-though-they-pass-in-your-local-development-environment","title":"Tests Fail in the CI Workflow (even though they pass in your local development environment)","text":"<p>Unfortunately, this can happen. Possible reasons include:</p> <ul> <li> <p>Differences in the software setups of your system and the CI system.     The CI machine may have a different OS (version), C++ compiler (version), installed software packages (versions), etc.     You can try to debug the failing tests in the CI's OS/software environment by using the CI container image as follows:</p> <p>On the host (from the DAPHNE root directory):</p> <pre><code>docker pull daphneeu/github-action\ndocker run -it --rm -w /daphne -v \"$(pwd):/daphne\" daphneeu/github-action:latest bash\n</code></pre> <p>In the container:</p> <pre><code># To avoid cmake complaints.\n./build.sh --clean -y\n# Build the test cases (effectively all targets) using pre-built dependencies.\n./build.sh --no-deps --target run_tests\n# Run the tests and print which test is running.\n./test.sh -nb -d yes\n</code></pre> <p>If the test failures cannot be reproduced that way, they are most likely not due to OS/software environment reasons.</p> </li> <li> <p>Differences in the hardware setups of your system and the CI system.     The CI machine may have a different processor model, a different number of physical/virtual cores, a different memory/disk capacity etc.     These differences could potentially have an impact on race conditions, the numerical stability of results, etc.     Ideally, DAPHNE should behave well in a reasonable range of hardware setups.</p> </li> </ul>"},{"location":"development/Testing/#writing-test-cases","title":"Writing Test Cases","text":""},{"location":"development/Testing/#overview-of-the-testing-code","title":"Overview of the Testing Code","text":"<p>All testing code resides in the directory <code>test/</code>. The subdirectories of <code>test/</code> are (almost) the same as those of the source code directory <code>src/</code> and contain the test cases for the respective parts of the code base.</p> <ul> <li><code>test/</code><ul> <li><code>api/</code>: all script-level test cases<ul> <li><code>cli/</code>: command-line use of DAPHNE (DaphneDSL scripts)</li> <li><code>python</code>: Python API use of DAPHNE (DaphneLib Python scripts)</li> </ul> </li> <li><code>data/</code>: data sets to be used in script-level test cases</li> <li><code>codegen/</code>, <code>ir/</code>, <code>parser/</code>, <code>runtime/</code> (all remaining subdirectories): all unit test cases</li> <li><code>CMakeLists.txt</code>: list of all <code>cpp</code>-files containing test cases</li> <li><code>run_tests.h</code>/<code>.cpp</code>: source code of the test executable <code>run_tests</code> (including the catch2 main function), rarely needs editing</li> <li><code>tags.h</code>: list of all test case tags</li> </ul> </li> </ul> <p>In the following, we give an overview on writing test cases in DAPHNE. The main goal is to enable developers to understand the way we write test cases in DAPHNE, rather than providing a detailed tutorial. There is already a multitude of existing test cases that can serve as concrete examples to learn from.</p> <p>To add a new test case, first identify the right spot in the directory hierarchy. Then, either create a new <code>*Test.cpp</code>-file (and add it to <code>test/CMakeLists.txt</code>) or extend an existing one. Within this <code>*Test.cpp</code>-file, you can add any logic required to perform your test, including helper functions etc. Create the individual catch2 test cases with macros like <code>TEST_CASE</code>, <code>TEMPLATE_TEST_CASE</code> and <code>TEMPLATE_PRODUCT_TEST_CASE</code>. Inside a test case, you can implement whatever you need to (a) set up the test fixture, (b) invoke the components to test, and (c) free any allocated resources. While test cases can be totally custom, we comment on frequent cases below.</p>"},{"location":"development/Testing/#writing-script-level-test-cases","title":"Writing Script-level Test Cases","text":"<p>Script-level test cases test the end-to-end use of DAPHNE by invoking DaphneDSL or DaphneLib scripts like users do. That way, we can involve the entire DAPHNE parser, compiler, and runtime stack into the test case. To that end, we invoke the <code>daphne</code> executable (DaphneDSL scripts) or <code>python3</code> (DaphneLib scripts) as separate processes from our C++ test cases, pass them all required arguments, and capture their status code as well as their output to <code>stdout</code> and <code>stderr</code>. We typically check if a given DaphneDSL/DaphneLib script:</p> <ol> <li>parses, compiles, and executes successfully (or fails in the expected way) (status code)</li> <li>yields the expected output (<code>stdout</code> and <code>stderr</code>)</li> </ol> <p>We offer a hierarchy of utilities for common tasks in <code>test/api/cli/Utils.h</code>/<code>.cpp</code>. These utilities work at slightly different abstraction levels. In case there is no utility function that exactly solves your case, you can still compose the lower-level utlities. In the following, we list just the most important ones, see the mentioned file for a detailed reference.</p> <ul> <li> <p><code>runProgram</code><code>(std::stringstream &amp; out, std::stringstream &amp; err, const char * execPath, Args ... args)</code></p> <p>Executes the specified program with the given arguments and captures <code>stdout</code>, <code>stderr</code>, and the status code.</p> <p>This is the basis for the other utility functions.</p> </li> <li> <p><code>runDaphne</code><code>(std::stringstream &amp; out, std::stringstream &amp; err, Args ... args)</code></p> <p>Executes DAPHNE's command line interface with the given arguments and captures <code>stdout</code>, <code>stderr</code>, and the status code.</p> </li> <li> <p><code>runDaphneLib</code><code>(std::stringstream &amp; out, std::stringstream &amp; err, const char * scriptPath, Args ... args)</code></p> <p>Executes the given Python script with the <code>python3</code> interpreter and captures <code>stdout</code>, <code>stderr</code>, and the status code.</p> </li> <li> <p><code>runLIT</code><code>(std::stringstream &amp;out, std::stringstream &amp;err, std::string dirPath,            Args... args)</code></p> <p>Executes the \"run-lit.py\" Python script in a directory and captures <code>stdout</code>, <code>stderr</code>, and the status code.</p> <p>Used for tests of <code>mlir</code>-files.</p> </li> <li> <p><code>checkDaphneStatusCode</code><code>(StatusCode exp, const std::string &amp; scriptFilePath, Args ... args)</code></p> <p>Checks whether executing the given DaphneDSL script with the command line interface of DAPHNE returns the given status code.</p> </li> <li> <p><code>checkDaphneFails</code><code>(const std::string &amp; scriptFilePath, Args ... args)</code></p> <p>Checks whether executing the given DaphneDSL script with the command line interface of DAPHNE fails.</p> </li> <li> <p><code>compareDaphneToStr</code><code>(const std::string &amp; exp, const std::string &amp; scriptFilePath, Args ... args)</code></p> <p>Compares the standard output of executing the given DaphneDSL script with the command line interface of DAPHNE to a reference text.</p> </li> <li> <p><code>compareDaphneToRef</code><code>(const std::string &amp; refFilePath, const std::string &amp; scriptFilePath, Args ... args)</code></p> <p>Compares the standard output of executing the given DaphneDSL script with the command line interface of DAPHNE to a reference text file.</p> </li> <li> <p><code>compareDaphneToDaphneLib</code><code>(const std::string &amp; pythonScriptFilePath, const std::string &amp; daphneDSLScriptFilePath, Args ... args)</code></p> <p>Compares the standard output of the given DaphneDSL script with that of the given Python/DaphneLib script.</p> </li> <li> <p><code>compareDaphneToSelfRef</code><code>(const std::string &amp;expScriptFilePath, const std::string &amp;actScriptFilePath, Args ... args)</code></p> <p>Compares the standard output of executing a given DaphneDSL script with the command line interface of DAPHNE to a (simpler) DaphneDSL script defining the expected behavior.</p> </li> </ul> <p>Some more hints:</p> <ul> <li>The <code>.cpp</code>-files of the DaphneDSL script-level test cases often define macros like <code>MAKE_TEST_CASE</code> and <code>MAKE_FAILURE_TEST_CASE</code>. These wrap the test case creation such that we can easily create families of similar test cases.</li> <li>Many of the script-level test cases have no highly descriptive names as that would be cumbersome. Instead, they are typically numbered. In that context, several of the utility functions mentioned above have a variant suffixed with <code>Simple</code> (e.g., <code>compareDaphneToRefSimple()</code>), which assumes a certain naming scheme of the test cases.</li> <li>Please put a comment line at the top of each DaphneDSL/DaphneLib script used in a script-level test case that briefly explains what is being tested. Note that the purpose of the test is otherwise not always obvious. Putting a comment helps to keep the test case up-to-date when the DAPHNE system changes.</li> <li>When using hand-written reference text files (e.g., with <code>compareDaphneToRef()</code>), note that there usually needs to be a newline at the end of the text file, as the DAPHNE output typically also ends with a newline. Otherwise, the test case fails even though the actual and expected outputs look quite similar at first glance.</li> </ul>"},{"location":"development/Testing/#writing-unit-test-cases","title":"Writing Unit Test Cases","text":"<p>Unit test cases invoke individual C++ components of DAPHNE in isolation to test their behavior in various situations. We use unit tests mainly for internal data structures, kernels, file readers/writers, as well as some pieces of the DAPHNE compiler.</p>"},{"location":"development/Testing/#unit-tests-for-kernels","title":"Unit Tests for Kernels","text":"<p>Unit tests for kernels are the most frequent kind of unit test case in DAPHNE. These tests reside in <code>test/runtime/local/kernels/</code>. For each DaphneIR operation <code>Xyz</code> (i.e., per <code>Xyz.h</code> in <code>src/runtime/local/kernels/</code>), there is one <code>XyzTest.cpp</code>-file. DAPHNE's kernels make extensive use of C++ template metaprogramming. Thus, the test cases are usually defined by catch2's <code>TEMPLATE_TEST_CASE</code> or <code>TEMPLATE_PRODUCT_TEST_CASE</code>, but some also use the plain <code>TEST_CASE</code>.</p> <p>The typical structure of a unit test case for a kernel is as follows:</p> <ol> <li>Define multiple pairs of inputs and expected outputs. These should include various regular and corner cases as well as cases with invalid arguments. catch2 SECTIONs can be used to define such cases very concisely, with minimal code duplication.</li> <li>Call the kernel under test as a C++ function with the given inputs and obtain the outputs.</li> <li>Compare the actual and expected outputs. To this end, catch2 offers various assertion macros, such as <code>CHECK</code>, <code>REQUIRE</code>, <code>CHECK_FALSE</code>, <code>CHECK_THROWS</code> etc. In DAPHNE, the <code>==</code> operator can be used to compare entire DAPHNE matrices and frames, e.g., <code>*exp == *res</code>. The <code>checkEqApprox</code>-kernel can be used to take floating-point round-off errors into account when comparing DAPHNE data objects.</li> <li>Destroy all created data objects exactly once using <code>DataObjectFactory::destroy()</code>.</li> </ol> <p>For steps 2 and 3, we often employ a separate <code>checkXyz()</code> helper function.</p> <p>Some more hints:</p> <ul> <li>Don't encode the types provided by <code>TEMPLATE_TEST_CASE</code> and <code>TEMPLATE_PRODUCT_TEST_CASE</code> in the test name manually; catch2 appends them automatically.</li> <li>The type of a particular instantiation of a <code>TEMPLATE_TEST_CASE</code> or <code>TEMPLATE_PRODUCT_TEST_CASE</code> is available as <code>TestType</code> within the test case. DAPHNE matrices further expose their value type as a member type <code>VT</code> (e.g., <code>TestType::VT</code>) and offer <code>withValueType&lt;VT&gt;</code> as a utility for getting the same data type with a different value type (e.g., <code>TestType::WithValueType&lt;double&gt;</code>). Furthermore, utilities from the STL header <code>&lt;type_traits&gt;</code> can be very helpful to manipulate C++ types.</li> <li>Small test matrices with hardcoded elements can easily be created by <code>genGivenVals()</code> (see <code>src/runtime/datagen/GenGivenVals.h</code>).</li> <li>All kernels (except for <code>createDaphneContext</code>) expect a <code>DaphneContext</code> as their last parameter. In the context of an invocation of the <code>daphne</code> executable, the <code>DaphneContext</code> is normally provided by the DAPHNE compiler/runtime. In unit test cases, it is typically fine to simply pass a <code>nullptr</code> as the context.</li> <li>Try to write the checks in a way such that catch2 produces helpful outputs in case of a failure. For instance, to check if a string <code>s</code> is empty, don't use <code>CHECK(s.empty())</code>, but rather do <code>CHECK(s == \"\")</code>, since the latter will include the contents of <code>s</code> in the failure indication, which is usually quite helpful. Furthermore, be aware that <code>REQUIRE</code> stops the test case execution on a failure, which also means that the following checks (which might produce helpful error indications) are not performed. Thus, consider using <code>CHECK</code> instead and use <code>REQUIRE</code> only when the following checks would not even be well-defined (e.g., you could require that something is not a <code>nullptr</code>).</li> </ul>"},{"location":"development/Testing/#limitations-and-outlook","title":"Limitations and Outlook","text":"<p>DAPHNE's test suite is continuously under development and contributions are always welcome. Open topics include, but are not limited to:</p> <ul> <li>Better test coverage<ul> <li>Unit tests for kernels:<ul> <li>Systematic tests with view as inputs</li> <li>Systematic tests with zero-row/column inputs</li> <li>Systematic tests with invalid inputs</li> </ul> </li> <li>Script-level tests<ul> <li>Systematic tests with all combinations of arguments</li> <li>DSL fuzzing for testing a multitude of valid DaphneDSL/DaphneLib scripts</li> </ul> </li> </ul> </li> <li>Comparison to baseline systems to check correctness for complex scripts beyond hand-written expected results</li> <li>Performance regression tests (see #208)</li> <li>Specification of test cases: more concise ways, especially for many small script-level test cases</li> </ul>"},{"location":"development/WriteDocs/","title":"Writing Documentation","text":"<p>At the moment the collection of markdown files in the <code>doc</code> directory is rendered to HTML and deployed via GitHub Pages.</p> <p>If you insert a new markdown file, you have to add it into the html docs tree in mkdocs.yml at a suitable position under the <code>nav</code> section.</p>"},{"location":"development/WriteDocs/#markdown-guideline","title":"Markdown Guideline","text":"<p>Please write clean markdown code to ensure a proper parsing by the tools used to render HTML. It is very recommended to use an IDE like VS Code. Code offers the feature to directly render markdown pages while you work on them. The extension markdownlint directly highlights syntax violations / problems.</p>"},{"location":"development/WriteDocs/#links","title":"Links","text":"<ul> <li>With <code>[&lt;link-name&gt;](&lt;link-url/path&gt;)</code> you can link to other files in the repo</li> <li>Write links to other markdown files or source code files/directories so that they work locally / in the github repository</li> <li>Do not use relative links like <code>../BuildingDaphne.md</code></li> <li>Always use absolute paths relative to the repo root like <code>/doc/development/BuildingDaphne.md</code></li> <li>The Links/URLs will be altered in order to work on the rendered HTML page as well</li> <li>Reference to issues with <code>[&lt;description&gt;](https://github.com/daphne-eu/daphne/tree/main/issues/123)</code>. This won't work on github itself but will be rendered in the html page then</li> </ul>"},{"location":"development/WriteDocs/#additional-syntax","title":"Additional Syntax","text":"<p>While some markdown renderers are much more relaxed and render as wished, some points have to be considered so that mkdocs renders correctly as well.</p> <ul> <li>4 spaces indentation for nested lists (ordered/unordered) and code blocks within lists to ensure proper rendering for HTML</li> <li>Using &lt;&gt;: To use angle brackets use <code>&lt;\\&gt;</code> notation outside an codeblock<ul> <li>Example: <code>&lt;nicer dicer\\&gt;</code> renders to &lt;nicer dicer&gt;</li> </ul> </li> </ul>"},{"location":"development/WriteDocs/#toolstack","title":"Toolstack","text":"<ul> <li>MkDocs to build html from markdown files</li> <li>Material for MkDocs as HTML theme</li> </ul>"},{"location":"tutorial/sqlTutorial/","title":"Using SQL in DaphneDSL","text":"<p>DAPHNE supports a rudimentary version of SQL. At any point in a DaphneDSL script, we can execute a SQL query on frames. We need two operations to achieve this: <code>registerView(...)</code> and <code>sql(...)</code></p> <p>For the following examples we assume we already have a DaphneDSL script which includes calculations on a frame \"x\" that has the columns \"a\", \"b\" and \"c\".</p>"},{"location":"tutorial/sqlTutorial/#general-procedure","title":"General Procedure","text":""},{"location":"tutorial/sqlTutorial/#registerview","title":"registerView(...)","text":"<p>RegisterView registers a frame for the sql operation. If we want to execute a SQL query on a frame, we need to register it before that. The operation has two inputs: the name of the table, as a string, and the frame which shall be associated with the given name.</p> <p>For example, we can register the frame \"x\", from previous calculations, under the name \"Table1\". The DaphneDSL script for this would look like this:</p> <pre><code>registerView(\"Table1\", x);\n</code></pre>"},{"location":"tutorial/sqlTutorial/#sql","title":"sql(...)","text":"<p>Now that we have registered the tables, that we need for our SQL query, we can go ahead and execute our query. The SQL operation takes one input: the SQL query, as a string. In it, we will reference the table names we previously have registered via registerView(...). As a result of this operation, we get back a frame. The columns of the frame are named after the projection arguments inside the SQL query.</p> <p>For example, we want to return all the rows of the frame x, which we have previously registered under the name \"Table1\", where the column \"a\" is greater than 5 and save it in a new frame named \"y\". The DaphneDSL script for this would look like this:</p> <pre><code>y = sql(\"SELECT t.a as a, t.b as b, t.c as c FROM Table1 as t WHERE t.a &gt; 5;\");\n</code></pre> <p>This results in a frame \"y\" that has three columns \"a\", \"b\" and \"c\". On the frame y we can continue to build our DaphneDSL script.</p>"},{"location":"tutorial/sqlTutorial/#features","title":"Features","text":"<p>We don't support the complete SQL standard at the moment. For instance, we need to fully specify on which columns we want to operate. In the example above, we see \"t.a\" instead of simply \"a\". Also, not supported are DDL and DCL Queries. Our goal for DML queries is to only support SELECT-statements. Other features we do and don't support right now can be found below.</p>"},{"location":"tutorial/sqlTutorial/#supported-features","title":"Supported Features","text":"<ul> <li>Cross Product</li> <li>Complex Where Clauses</li> <li>Inner Join with single and multiple join conditions separated by an \"AND\" Operator</li> <li>Group By Clauses</li> <li>Having Clauses</li> <li>Order By Clauses</li> <li>As</li> <li>Distinct</li> </ul>"},{"location":"tutorial/sqlTutorial/#not-yet-supported-features","title":"Not Yet Supported Features","text":"<ul> <li>The Star Operator *</li> <li>Nested SQL Queries like: <code>SELECT a FROM x WHERE a IN SELECT a FROM y</code></li> <li>All Set Operations (Union, Except, Intersect)</li> <li>Recursive SQL Queries</li> <li>Limit</li> </ul>"},{"location":"tutorial/sqlTutorial/#examples","title":"Examples","text":"<p>In the following, we show two simple examples of SQL in DaphneDSL. The DaphneDSL scripts can be found in <code>doc/tutorial/sqlExample1.daph</code> and <code>doc/tutorial/sqlExample2.daph</code>.</p>"},{"location":"tutorial/sqlTutorial/#example-1","title":"Example 1","text":"<pre><code>//Creation of different matrices for a Frame\n    //seq(a, b, c) generates a sequences of the form [a, b] and step size c\n    employee_id = seq(1, 20, 1);\n    //rand(a, b, c, d, e, f) generates a matrix with a rows and b columns in a value range of [c, d]\n    salary = rand(20, 1, 250.0, 500.0, 1.0, -1);\n    //with [a, b, ..] we can create a matrix with the given values.\n    age = [20, 30, 23, 65, 70, 42, 34, 55, 76, 32, 53, 40, 42, 69, 63, 26, 70, 36, 21, 23];\n\n    //createFrame() creates a Frame with the given matrices. The column names (strings) are optional.\n    employee_frame = createFrame(employee_id, salary, age, \"employee_id\", \"salary\", \"age\");\n\n//We register the employee_frame we created previously. note the name for the registration and the \n//name of the frame don't have to be the same.\n    registerView(\"employee\", employee_frame);\n\n//We run a SQL Query on the registered Frame. Note here we have to reference the name we choose\n//during registration.\n    res = sql(\n        \"SELECT e.employee_id as employee_id, e.salary as salary, e.age as age\n        FROM employee as e\n        WHERE e.salary &gt; 450.0;\");\n\n//We can Print both employee and the query result to the console with print().\n    print(employee_frame);\n    print(res);\n</code></pre>"},{"location":"tutorial/sqlTutorial/#example-2","title":"Example 2","text":"<pre><code>employee_id = seq(1, 20, 1);\nsalary = rand(20, 1, 250.0, 500.0, 1.0, -1);\nage = [20, 30, 23, 65, 70, 42, 34, 55, 76, 32, 53, 40, 42, 69, 63, 26, 70, 36, 21, 23];\n\nemployee_frame = createFrame(employee_id, salary, age, \"employee_id\", \"salary\", \"age\");\n\nregisterView(\"employee\", employee_frame);\n\nres = sql(\n    \"SELECT  e.age as age, avg(e.salary) as salary\n    FROM employee as e\n    GROUP BY e.age\n    ORDER BY e.age\");\n\nprint(employee_frame);\nprint(res);\n</code></pre>"}]}