/*
 * Copyright 2021 The DAPHNE Consortium
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include "MTWrapper.h"
#include <runtime/local/vectorized/Tasks.h>

#ifdef USE_CUDA
#include <runtime/local/vectorized/TasksCUDA.h>
#endif

template <typename VT>
[[maybe_unused]] void MTWrapper<DenseMatrix<VT>>::executeSingleQueue(
    std::vector<
        std::function<typename MTWrapper<DenseMatrix<VT>>::PipelineFunc>>
        funcs,
    DenseMatrix<VT> ***res, const bool *isScalar, Structure **inputs,
    size_t numInputs, size_t numOutputs, int64_t *outRows, int64_t *outCols,
    VectorSplit *splits, VectorCombine *combines, DCTX(ctx),
    const bool verbose) {
    auto inputProps = this->getInputProperties(inputs, numInputs, splits);
    auto len = inputProps.first;
    auto mem_required = inputProps.second;
    mem_required +=
        this->allocateOutput(res, numOutputs, outRows, outCols, combines);
    auto row_mem = mem_required / len;

    // create task queue (w/o size-based blocking)
    std::unique_ptr<TaskQueue> q = std::make_unique<BlockingTaskQueue>(len);

    std::vector<TaskQueue *> tmp_q{q.get()};
    auto batchSize8M =
        std::max(100ul, static_cast<size_t>(std::ceil(8388608 / row_mem)));
    this->initCPPWorkers(tmp_q, batchSize8M, verbose, 1, 0, false);

#ifdef USE_CUDA
    if (this->_numCUDAThreads) {
        this->initCUDAWorkers(q.get(), batchSize8M * 4, verbose);
        this->cudaPrefetchInputs(inputs, numInputs, mem_required, splits);
        ctx->logger->info("MTWrapper_dense: \nRequired memory (ins/outs): {} "
                          "Required mem/row: {}\n batchsizeCPU={} "
                          "batchsizeGPU={}",
                          mem_required, row_mem, batchSize8M, batchSize8M * 4);
    }
#endif

    // lock for aggregation combine
    // TODO: multiple locks per output
    std::mutex resLock;

    // create tasks and close input
    uint64_t startChunk = 0;
    uint64_t endChunk = 0;
    int method = ctx->config.taskPartitioningScheme;
    int chunkParam = ctx->config.minimumTaskSize;
    if (chunkParam <= 0)
        chunkParam = 1;
    bool autoChunk = false;
    if (method == AUTO)
        autoChunk = true;
    LoadPartitioning lp(method, len, chunkParam, this->_numThreads, autoChunk);
    while (lp.hasNextChunk()) {
        endChunk += lp.getNextChunk();
        q->enqueueTask(new CompiledPipelineTask<DenseMatrix<VT>>(
            CompiledPipelineTaskData<DenseMatrix<VT>>{
                funcs, isScalar, inputs, numInputs, numOutputs, outRows,
                outCols, splits, combines, startChunk, endChunk, outRows,
                outCols, 0, ctx},
            resLock, res));
        startChunk = endChunk;
    }
    q->closeInput();

    this->joinAll();
}

template <typename VT>
[[maybe_unused]] void MTWrapper<DenseMatrix<VT>>::executeCpuQueues(
    std::vector<
        std::function<typename MTWrapper<DenseMatrix<VT>>::PipelineFunc>>
        funcs,
    DenseMatrix<VT> ***res, const bool *isScalar, Structure **inputs,
    size_t numInputs, size_t numOutputs, int64_t *outRows, int64_t *outCols,
    VectorSplit *splits, VectorCombine *combines, DCTX(ctx), bool verbose) {
    auto inputProps = this->getInputProperties(inputs, numInputs, splits);
    auto len = inputProps.first;
    auto mem_required = inputProps.second;
    mem_required +=
        this->allocateOutput(res, numOutputs, outRows, outCols, combines);
    auto row_mem = mem_required / len;

    std::vector<std::unique_ptr<TaskQueue>> q;
    std::vector<TaskQueue *> qvector;
    if (ctx->getUserConfig().pinWorkers) {
        for (int i = 0; i < this->_numQueues; i++) {
            cpu_set_t cpuset;
            CPU_ZERO(&cpuset);
            CPU_SET(i, &cpuset);
            sched_setaffinity(0, sizeof(cpu_set_t), &cpuset);
            std::unique_ptr<TaskQueue> tmp =
                std::make_unique<BlockingTaskQueue>(len);
            q.push_back(std::move(tmp));
            qvector.push_back(q[i].get());
        }
    } else {
        for (int i = 0; i < this->_numQueues; i++) {
            std::unique_ptr<TaskQueue> tmp =
                std::make_unique<BlockingTaskQueue>(len);
            q.push_back(std::move(tmp));
            qvector.push_back(q[i].get());
        }
    }

    auto batchSize8M =
        std::max(100ul, static_cast<size_t>(std::ceil(8388608 / row_mem)));
    this->initCPPWorkers(qvector, batchSize8M, verbose, this->_numQueues,
                         this->_queueMode, ctx->getUserConfig().pinWorkers);

    // lock for aggregation combine
    // TODO: multiple locks per output
    std::mutex resLock;

    // create tasks and close input
    uint64_t startChunk = 0;
    uint64_t endChunk = 0;
    uint64_t currentItr = 0;
    uint64_t target;
    int method = ctx->config.taskPartitioningScheme;
    int chunkParam = ctx->config.minimumTaskSize;
    if (chunkParam <= 0)
        chunkParam = 1;
    if (ctx->getUserConfig().prePartitionRows) {
        uint64_t oneChunk = len / this->_numQueues;
        int remainder = len - (oneChunk * this->_numQueues);
        std::vector<LoadPartitioning> lps;
        lps.emplace_back(method, oneChunk + remainder, chunkParam,
                         this->_numThreads, false);
        for (int i = 1; i < this->_numQueues; i++) {
            lps.emplace_back(method, oneChunk, chunkParam, this->_numThreads,
                             false);
        }
        if (ctx->getUserConfig().pinWorkers) {
            for (int i = 0; i < this->_numQueues; i++) {
                while (lps[i].hasNextChunk()) {
                    endChunk += lps[i].getNextChunk();
                    qvector[i]->enqueueTaskPinned(
                        new CompiledPipelineTask<DenseMatrix<VT>>(
                            CompiledPipelineTaskData<DenseMatrix<VT>>{
                                funcs, isScalar, inputs, numInputs, numOutputs,
                                outRows, outCols, splits, combines, startChunk,
                                endChunk, outRows, outCols, 0, ctx},
                            resLock, res),
                        this->topologyResponsibleThreads[i]);
                    startChunk = endChunk;
                }
            }
        } else {
            for (int i = 0; i < this->_numQueues; i++) {
                while (lps[i].hasNextChunk()) {
                    endChunk += lps[i].getNextChunk();
                    qvector[i]->enqueueTask(
                        new CompiledPipelineTask<DenseMatrix<VT>>(
                            CompiledPipelineTaskData<DenseMatrix<VT>>{
                                funcs, isScalar, inputs, numInputs, numOutputs,
                                outRows, outCols, splits, combines, startChunk,
                                endChunk, outRows, outCols, 0, ctx},
                            resLock, res));
                    startChunk = endChunk;
                }
            }
        }
    } else {
        bool autoChunk = false;
        if (method == AUTO)
            autoChunk = true;
        LoadPartitioning lp(method, len, chunkParam, this->_numThreads,
                            autoChunk);
        if (ctx->getUserConfig().pinWorkers) {
            while (lp.hasNextChunk()) {
                endChunk += lp.getNextChunk();
                target = currentItr % this->_numQueues;
                qvector[target]->enqueueTaskPinned(
                    new CompiledPipelineTask<DenseMatrix<VT>>(
                        CompiledPipelineTaskData<DenseMatrix<VT>>{
                            funcs, isScalar, inputs, numInputs, numOutputs,
                            outRows, outCols, splits, combines, startChunk,
                            endChunk, outRows, outCols, 0, ctx},
                        resLock, res),
                    this->topologyUniqueThreads[target]);
                startChunk = endChunk;
                currentItr++;
            }
        } else {
            while (lp.hasNextChunk()) {
                endChunk += lp.getNextChunk();
                target = currentItr % this->_numQueues;
                qvector[target]->enqueueTask(
                    new CompiledPipelineTask<DenseMatrix<VT>>(
                        CompiledPipelineTaskData<DenseMatrix<VT>>{
                            funcs, isScalar, inputs, numInputs, numOutputs,
                            outRows, outCols, splits, combines, startChunk,
                            endChunk, outRows, outCols, 0, ctx},
                        resLock, res));
                startChunk = endChunk;
                currentItr++;
            }
        }
    }
    for (int i = 0; i < this->_numQueues; i++) {
        qvector[i]->closeInput();
    }

    this->joinAll();
}

template <typename VT>
[[maybe_unused]] void MTWrapper<DenseMatrix<VT>>::executeQueuePerDeviceType(
    std::vector<
        std::function<typename MTWrapper<DenseMatrix<VT>>::PipelineFunc>>
        funcs,
    DenseMatrix<VT> ***res, const bool *isScalar, Structure **inputs,
    size_t numInputs, size_t numOutputs, int64_t *outRows, int64_t *outCols,
    VectorSplit *splits, VectorCombine *combines, DCTX(ctx), bool verbose) {
    size_t device_task_len = 0ul;
    auto inputProps = this->getInputProperties(inputs, numInputs, splits);
    auto len = inputProps.first;
    auto mem_required = inputProps.second;
    mem_required +=
        this->allocateOutput(res, numOutputs, outRows, outCols, combines);
    auto row_mem = mem_required / len;
    auto batchSize8M =
        std::max(100ul, static_cast<size_t>(std::ceil(8388608 / row_mem)));
    // lock for aggregation combine
    // TODO: multiple locks per output
    std::mutex resLock;

#ifdef USE_CUDA
    // ToDo: multi-device support :-P
    float taskRatioCUDA = 0.25f;
    auto gpu_task_len =
        static_cast<size_t>(std::ceil(static_cast<float>(len) * taskRatioCUDA));
    device_task_len += gpu_task_len;
    std::unique_ptr<TaskQueue> q_cuda =
        std::make_unique<BlockingTaskQueue>(gpu_task_len);
    this->initCUDAWorkers(q_cuda.get(), batchSize8M * 4, verbose);

    auto ***res_cuda = new DenseMatrix<VT> **[numOutputs];
    auto blksize = gpu_task_len / ctx->cuda_contexts.size();
    ctx->logger->debug("gpu_task_len: {}\ntaskRatioCUDA: {}\nBlock size: {}",
                       gpu_task_len, taskRatioCUDA, blksize);
    for (size_t i = 0; i < numOutputs; ++i) {
        res_cuda[i] = new DenseMatrix<VT> *;
        if (combines[i] == mlir::daphne::VectorCombine::ROWS) {
            auto rc2 = static_cast<DenseMatrix<VT> *>((*res[i]))->sliceRow(
                0, gpu_task_len);
            (*res_cuda[i]) = rc2;
        } else if (combines[i] == mlir::daphne::VectorCombine::COLS) {
            (*res_cuda[i]) = static_cast<DenseMatrix<VT> *>((*res[i]))->slice(
                0, outRows[i], 0, gpu_task_len);
        } else {
            (*res_cuda[i]) = (*res[i]);
        }
    }

    for (uint32_t k = 0; k < gpu_task_len; k += blksize) {
        q_cuda->enqueueTask(new CompiledPipelineTaskCUDA<DenseMatrix<VT>>(
            CompiledPipelineTaskData<DenseMatrix<VT>>{
                funcs, isScalar, inputs, numInputs, numOutputs, outRows,
                outCols, splits, combines, k, std::min(k + blksize, len),
                outRows, outCols, 0, ctx},
            resLock, res_cuda));
    }
    q_cuda->closeInput();
#endif

    auto cpu_task_len = len - device_task_len;
    DenseMatrix<VT> ***res_cpp{};
    std::unique_ptr<TaskQueue> q_cpp;

    std::vector<std::unique_ptr<TaskQueue>> q;
    std::vector<TaskQueue *> qvector;
    if (cpu_task_len > 0) {
        // Multiple Queues addition
        if (ctx->getUserConfig().pinWorkers) {
            for (int i = 0; i < this->_numQueues; i++) {
                cpu_set_t cpuset;
                CPU_ZERO(&cpuset);
                CPU_SET(i, &cpuset);
                sched_setaffinity(0, sizeof(cpu_set_t), &cpuset);
                std::unique_ptr<TaskQueue> tmp =
                    std::make_unique<BlockingTaskQueue>(cpu_task_len);
                q.push_back(std::move(tmp));
                qvector.push_back(q[i].get());
            }
        } else {
            for (int i = 0; i < this->_numQueues; i++) {
                std::unique_ptr<TaskQueue> tmp =
                    std::make_unique<BlockingTaskQueue>(cpu_task_len);
                q.push_back(std::move(tmp));
                qvector.push_back(q[i].get());
            }
        }
        this->initCPPWorkers(qvector, batchSize8M, verbose, this->_numQueues,
                             this->_queueMode, ctx->getUserConfig().pinWorkers);
        // End Multiple Queues

        res_cpp = new DenseMatrix<VT> **[numOutputs];
        auto offset = device_task_len;

        for (size_t i = 0; i < numOutputs; ++i) {
            res_cpp[i] = new DenseMatrix<VT> *;
            if (combines[i] == mlir::daphne::VectorCombine::ROWS) {
                (*res_cpp[i]) =
                    static_cast<DenseMatrix<VT> *>((*res[i]))->sliceRow(
                        device_task_len, len);
            } else if (combines[i] == mlir::daphne::VectorCombine::COLS) {
                (*res_cpp[i]) =
                    static_cast<DenseMatrix<VT> *>((*res[i]))->sliceCol(
                        device_task_len, len);
            } else {
                (*res_cpp[i]) = (*res[i]);
            }
        }

        uint64_t startChunk = device_task_len;
        uint64_t endChunk = device_task_len;
        uint64_t currentItr = 0;
        uint64_t target;
        int method = ctx->config.taskPartitioningScheme;
        int chunkParam = ctx->config.minimumTaskSize;
        if (chunkParam <= 0)
            chunkParam = 1;
        bool autoChunk = false;
        if (method == AUTO)
            autoChunk = true;

        LoadPartitioning lp(method, cpu_task_len, chunkParam,
                            this->_numCPPThreads, autoChunk);
        while (lp.hasNextChunk()) {
            endChunk += lp.getNextChunk();
            target = currentItr % this->_numQueues;
            qvector[target]->enqueueTask(
                new CompiledPipelineTask<DenseMatrix<VT>>(
                    CompiledPipelineTaskData<DenseMatrix<VT>>{
                        funcs, isScalar, inputs, numInputs, numOutputs, outRows,
                        outCols, splits, combines, startChunk, endChunk,
                        outRows, outCols, offset, ctx},
                    resLock, res_cpp));
            startChunk = endChunk;
            currentItr++;
        }
        for (int i = 0; i < this->_numQueues; i++) {
            qvector[i]->closeInput();
        }
    }
    this->joinAll();

#ifdef USE_CUDA
    this->combineOutputs(res, res_cuda, numOutputs, combines, ctx);
#endif

    if (cpu_task_len > 0) {
        for (size_t i = 0; i < numOutputs; ++i) {
            if (combines[i] == mlir::daphne::VectorCombine::ROWS ||
                combines[i] == mlir::daphne::VectorCombine::COLS)
                DataObjectFactory::destroy((*res_cpp[i]));
        }
    }
}

#ifdef USE_CUDA
template <typename VT>
void MTWrapper<DenseMatrix<VT>>::combineOutputs(
    DenseMatrix<VT> ***&res_, DenseMatrix<VT> ***&res_cuda_, size_t numOutputs,
    mlir::daphne::VectorCombine *combines, DCTX(ctx)) {
    const size_t deviceID = 0; // ToDo: multi device support
    AllocationDescriptorCUDA alloc_desc(ctx, deviceID);
    for (size_t i = 0; i < numOutputs; ++i) {
        auto *res = static_cast<DenseMatrix<VT> *>((*res_[i]));
        auto *res_cuda = static_cast<DenseMatrix<VT> *>((*res_cuda_[i]));
        if (combines[i] == mlir::daphne::VectorCombine::ROWS) {
            const auto &const_res_cuda = *res_cuda;
            auto data_dest = res->getValues();
            CHECK_CUDART(cudaMemcpy(
                data_dest, const_res_cuda.getValues(&alloc_desc),
                const_res_cuda.getBufferSize(), cudaMemcpyDeviceToHost));
            //            debugPrintCUDABuffer("MTWrapperDense: combine
            //            outputs", const_res_cuda.getValues(&alloc_desc),
            //            const_res_cuda.getNumItems());
            DataObjectFactory::destroy(res_cuda);
        } else if (combines[i] == mlir::daphne::VectorCombine::COLS) {
            const auto &const_res_cuda = *res_cuda;
            auto dst_base_ptr = res->getValues();
            auto src_base_ptr = const_res_cuda.getValues(&alloc_desc);
            for (auto j = 0u; j < res_cuda->getNumRows(); ++j) {
                // ToDo: rowSkip would be correct here if res_cuda wasn't a
                // shallow copy
                //                auto data_src = src_base_ptr +
                //                res_cuda->getRowSkip() * j;
                auto data_src = src_base_ptr + res_cuda->getNumCols() * j;
                auto data_dst = dst_base_ptr + res->getRowSkip() * j;
                CHECK_CUDART(cudaMemcpy(data_dst, data_src,
                                        res_cuda->getNumCols() * sizeof(VT),
                                        cudaMemcpyDeviceToHost));
            }
            DataObjectFactory::destroy(res_cuda);
        }
    }
}
#else
template <typename VT>
void MTWrapper<DenseMatrix<VT>>::combineOutputs(
    DenseMatrix<VT> ***&res_, DenseMatrix<VT> ***&res_cuda_, size_t numOutputs,
    mlir::daphne::VectorCombine *combines, DCTX(ctx)) {}
#endif

template class MTWrapper<DenseMatrix<double>>;
template class MTWrapper<DenseMatrix<float>>;
template class MTWrapper<DenseMatrix<int64_t>>;
